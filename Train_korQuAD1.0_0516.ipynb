{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMCj6fapQLVsEI97mT0iWIo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e0a234238a20442188bffeb50b8a82e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_508da04576294734b60f3fa90d40b2f5",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_31dda3106cc647fda699a60bf34a320c",
              "IPY_MODEL_7269bb94559540d8a2ad15890118c9a0"
            ]
          }
        },
        "508da04576294734b60f3fa90d40b2f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "31dda3106cc647fda699a60bf34a320c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4bbddc49854648ee809cab2744f19f03",
            "_dom_classes": [],
            "description": "Dl Completed...: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a6b91a068a2445da88b086a99123cca1"
          }
        },
        "7269bb94559540d8a2ad15890118c9a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a0c5df4e61074ded90aadad9d9123604",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 5/5 [01:57&lt;00:00, 23.44s/ url]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ffaa2c46811545579645138631dfcfb7"
          }
        },
        "4bbddc49854648ee809cab2744f19f03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a6b91a068a2445da88b086a99123cca1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a0c5df4e61074ded90aadad9d9123604": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ffaa2c46811545579645138631dfcfb7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b07bff8f8c0242cb9d14c4e9200fe065": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c138a58915804d3a82cdab0711e6ee13",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b2f387970f2b453db3beaedc6aaea0a0",
              "IPY_MODEL_10d6cdeac7824a1da0e7ee23a9857162"
            ]
          }
        },
        "c138a58915804d3a82cdab0711e6ee13": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b2f387970f2b453db3beaedc6aaea0a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8cde191a1d694cac973a9ff53c680c6f",
            "_dom_classes": [],
            "description": "Dl Size...: ",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f715a113aec4444f91d26969ea994d30"
          }
        },
        "10d6cdeac7824a1da0e7ee23a9857162": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8fad692c0da84f67b2981b9dc5085798",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 557/? [01:57&lt;00:00,  4.75 MiB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d94643c7f32d4c679260bb0abf305de3"
          }
        },
        "8cde191a1d694cac973a9ff53c680c6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f715a113aec4444f91d26969ea994d30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8fad692c0da84f67b2981b9dc5085798": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d94643c7f32d4c679260bb0abf305de3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a94f18110c324dc780e714e50491e9fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_be9e86e7e0b54aa1a8cceb4a7c44fbd9",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d6c60b10cea943e2b0fdba4f546ec849",
              "IPY_MODEL_d8100564205c4989b0cc9512ffdef942"
            ]
          }
        },
        "be9e86e7e0b54aa1a8cceb4a7c44fbd9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d6c60b10cea943e2b0fdba4f546ec849": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_83c6a479d4594efdaa7c8fdf758f86ef",
            "_dom_classes": [],
            "description": "Extraction completed...: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_edb7645bd9cf4ebcb0472ab8413f15ec"
          }
        },
        "d8100564205c4989b0cc9512ffdef942": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6dd9062c334a4755ba7b0a1bd86468ca",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2/2 [01:57&lt;00:00, 58.56s/ file]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0ecf99f564724d79b28843833d2e9c6b"
          }
        },
        "83c6a479d4594efdaa7c8fdf758f86ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "edb7645bd9cf4ebcb0472ab8413f15ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6dd9062c334a4755ba7b0a1bd86468ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0ecf99f564724d79b28843833d2e9c6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "141567d835724105af5415594e9a7f9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2293ab6f8444436d826a597626e581f1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e7df796c569f4432b158b7d360cea766",
              "IPY_MODEL_dc0e96d7d05e4ed0ac616431ffc17104"
            ]
          }
        },
        "2293ab6f8444436d826a597626e581f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e7df796c569f4432b158b7d360cea766": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_845e048e05db4a1ba2fbe08d84aefa40",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_15172d9909fd47dc98f1666eb3ab3102"
          }
        },
        "dc0e96d7d05e4ed0ac616431ffc17104": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_058be0a299de4e408294b8551c55bc63",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 222894/0 [15:10&lt;00:00, 231.40 examples/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6ecd6166ed4b41b9b09da5d04d26ada4"
          }
        },
        "845e048e05db4a1ba2fbe08d84aefa40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "15172d9909fd47dc98f1666eb3ab3102": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "058be0a299de4e408294b8551c55bc63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6ecd6166ed4b41b9b09da5d04d26ada4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "44757a3a7afe4e4eb95e95f11e94baed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_fee20f84e6204e7bad616f22de2b85ba",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_673b5f7827d7437f92e1f3240161aa5a",
              "IPY_MODEL_9170fc91ba304ad8b6a4c2d2ccb31355"
            ]
          }
        },
        "fee20f84e6204e7bad616f22de2b85ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "673b5f7827d7437f92e1f3240161aa5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e561f9e07090434cb77090aeddc4789a",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 344259,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 344259,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_de0381976d7c4fc68b06665cbae12834"
          }
        },
        "9170fc91ba304ad8b6a4c2d2ccb31355": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6d81c8e6bbaa47139a9161791c49e526",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 344k/344k [00:01&lt;00:00, 303kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_db5a8e04fa0949de9cfc76eb7494f9ca"
          }
        },
        "e561f9e07090434cb77090aeddc4789a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "de0381976d7c4fc68b06665cbae12834": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6d81c8e6bbaa47139a9161791c49e526": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "db5a8e04fa0949de9cfc76eb7494f9ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f6fb86e6f6914670968ad70d09ab5852": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_bd03d7a79e02416ebcdb5f6193414fe3",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b656b044f5964b18a69be602cf391abf",
              "IPY_MODEL_3bb580a2181641c48ca8a14da6e7c2a0"
            ]
          }
        },
        "bd03d7a79e02416ebcdb5f6193414fe3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b656b044f5964b18a69be602cf391abf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4ceff003d619437d8ecae1bf080db110",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 684,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 684,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c807b711d5e84e2fb60db9716f57a004"
          }
        },
        "3bb580a2181641c48ca8a14da6e7c2a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6c478a8fd1214967a49ba63b7a5f5c2d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 684/684 [00:00&lt;00:00, 802B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2a15c90485f04cdab63dcd9704bc68d7"
          }
        },
        "4ceff003d619437d8ecae1bf080db110": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c807b711d5e84e2fb60db9716f57a004": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6c478a8fd1214967a49ba63b7a5f5c2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2a15c90485f04cdab63dcd9704bc68d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cc2d8144b76b471a824dddd49b6c5889": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_51dca6c50af5412994758f1c854a78de",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2448a43b9a3645cebcabe6a5c21b0d6f",
              "IPY_MODEL_0c511bccc20b4815a1ff1a54d40f94da"
            ]
          }
        },
        "51dca6c50af5412994758f1c854a78de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2448a43b9a3645cebcabe6a5c21b0d6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_508cd332718045e290ac9a43cf950af3",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 53325832,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 53325832,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_42119f7a708b411b93a877db42feae49"
          }
        },
        "0c511bccc20b4815a1ff1a54d40f94da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5dad0c302f7d4533ada1482eebc97809",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 53.3M/53.3M [00:52&lt;00:00, 1.02MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_73e88e9c09f44e86a9ae023ec7f6ed21"
          }
        },
        "508cd332718045e290ac9a43cf950af3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "42119f7a708b411b93a877db42feae49": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5dad0c302f7d4533ada1482eebc97809": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "73e88e9c09f44e86a9ae023ec7f6ed21": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "900992c9731b494a836b090acd817255": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4a609fde633448e4abedf96a3ca46514",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_aee229bbb2cb4014a2aaaa7de42ecadc",
              "IPY_MODEL_804ff19fd0c34b33a64d3db138eee97b"
            ]
          }
        },
        "4a609fde633448e4abedf96a3ca46514": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "aee229bbb2cb4014a2aaaa7de42ecadc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_628451d485bb41b687e0388a3a0733d9",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 760289,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 760289,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_04f37ec1ff4a455b84732cde635a7a32"
          }
        },
        "804ff19fd0c34b33a64d3db138eee97b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1210ff77115442c8adea202c2384ad7a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 760k/760k [00:00&lt;00:00, 1.09MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_daa84cb7428547dda7903cbeb1648392"
          }
        },
        "628451d485bb41b687e0388a3a0733d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "04f37ec1ff4a455b84732cde635a7a32": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1210ff77115442c8adea202c2384ad7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "daa84cb7428547dda7903cbeb1648392": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "20e74878ee6d4169838f5ebcd9bf9105": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2b8569172ab848749ef1d1fbaaf12666",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4922360857c941f8ba39fa6b714f368d",
              "IPY_MODEL_b38b917be9b24e0088cc29f43318e8e5"
            ]
          }
        },
        "2b8569172ab848749ef1d1fbaaf12666": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4922360857c941f8ba39fa6b714f368d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_34b650cda7e6401281b3d38a515a9bbc",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1312669,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1312669,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0c0903beadef4ee9980a15bc0fac1125"
          }
        },
        "b38b917be9b24e0088cc29f43318e8e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2ff724f00db740828f6f3167b7586b6b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.31M/1.31M [00:22&lt;00:00, 57.6kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d558b31da7c84edc86e1b08c0f9c44dd"
          }
        },
        "34b650cda7e6401281b3d38a515a9bbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0c0903beadef4ee9980a15bc0fac1125": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2ff724f00db740828f6f3167b7586b6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d558b31da7c84edc86e1b08c0f9c44dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "733866c92eec4255b38e995e9b7ba0b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_13052ef80a744def9c3537a2ac0750c3",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5d3816317c6843efbd58e22c6ffea1e2",
              "IPY_MODEL_b7f8684692d740c2a684aba285114e1b"
            ]
          }
        },
        "13052ef80a744def9c3537a2ac0750c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5d3816317c6843efbd58e22c6ffea1e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_89cf8cf5c0554bb886a5749336ca8ee7",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 685,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 685,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_dc2ea1c1d9a448cd84cd1af0c731e4fe"
          }
        },
        "b7f8684692d740c2a684aba285114e1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c66d419b769649b18f80dba829755316",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 685/685 [00:00&lt;00:00, 1.28kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_aa3bc3bdaf5d48d5b678ca4cf01930cb"
          }
        },
        "89cf8cf5c0554bb886a5749336ca8ee7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "dc2ea1c1d9a448cd84cd1af0c731e4fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c66d419b769649b18f80dba829755316": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "aa3bc3bdaf5d48d5b678ca4cf01930cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1e942b3ef56f449299c8d22c98b5443d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c0282cbb9c9c42ec910ecfe70e6bb49c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_978efca384e942b4aec18babdf2e9b6a",
              "IPY_MODEL_6502ffd64d1940509266fa7d5890b669"
            ]
          }
        },
        "c0282cbb9c9c42ec910ecfe70e6bb49c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "978efca384e942b4aec18babdf2e9b6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_86da887a18b44fa1a450129f2c8986d7",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 236197176,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 236197176,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_132e520423e94ef39e88cb0b872a006e"
          }
        },
        "6502ffd64d1940509266fa7d5890b669": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3a5049dee38641f08f01c4c58e4a09c1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 236M/236M [00:21&lt;00:00, 10.8MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8f07e69a62694d099a85e21b39328ecf"
          }
        },
        "86da887a18b44fa1a450129f2c8986d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "132e520423e94ef39e88cb0b872a006e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3a5049dee38641f08f01c4c58e4a09c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8f07e69a62694d099a85e21b39328ecf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7a1ea1419ff94665a1679ebf1ef6c2c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5fe568b1040646d3a2f792b785d90d96",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0ecfea61dacd4d049417134627dcffbe",
              "IPY_MODEL_ccb6c5752492462aa46991e3c8f29018"
            ]
          }
        },
        "5fe568b1040646d3a2f792b785d90d96": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0ecfea61dacd4d049417134627dcffbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_81bb36671bfc4b34b8c5608eb8ae3641",
            "_dom_classes": [],
            "description": "Downloading: ",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1710,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1710,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_47d7c7b046c44f43803e2adb86b20a4f"
          }
        },
        "ccb6c5752492462aa46991e3c8f29018": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a109e9b9abf84b3d84c504dcd5521535",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 4.50k/? [00:02&lt;00:00, 1.52kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0798de252e4747d683310debeffc81fc"
          }
        },
        "81bb36671bfc4b34b8c5608eb8ae3641": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "47d7c7b046c44f43803e2adb86b20a4f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a109e9b9abf84b3d84c504dcd5521535": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0798de252e4747d683310debeffc81fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5deb1925808640979a566c5fa32eb6f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9f7eeb0e11504da4b2e23ef6744f6b17",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_012d08bda7274ea3a5730d16ba66d829",
              "IPY_MODEL_a1dfff15cf764d78a76d8888dccf3fc0"
            ]
          }
        },
        "9f7eeb0e11504da4b2e23ef6744f6b17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "012d08bda7274ea3a5730d16ba66d829": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a09b66730f654338b72598f897c75b2b",
            "_dom_classes": [],
            "description": "Downloading: ",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 962,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 962,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cb1d69197f7d416b906735615f37388c"
          }
        },
        "a1dfff15cf764d78a76d8888dccf3fc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7bcb67d12c87446d8889831d65ed6584",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2.24k/? [00:00&lt;00:00, 35.8kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7fdaa1bd04354070942b989cd999111a"
          }
        },
        "a09b66730f654338b72598f897c75b2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cb1d69197f7d416b906735615f37388c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7bcb67d12c87446d8889831d65ed6584": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7fdaa1bd04354070942b989cd999111a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "36f10e2a42d4499aab746161965d745e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_02c40ae99caf4187ac0e65579c9a14f1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1febe86e12724dd3bf54924a9b1a9df9",
              "IPY_MODEL_5803670df0b54a9b93270e4a28ae41e6"
            ]
          }
        },
        "02c40ae99caf4187ac0e65579c9a14f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1febe86e12724dd3bf54924a9b1a9df9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_efe766d9df5d4b559dee67bcf1f50c8c",
            "_dom_classes": [],
            "description": "Downloading: ",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 7568316,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 7568316,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1010972e9ac348aba93396e550239712"
          }
        },
        "5803670df0b54a9b93270e4a28ae41e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_632141fc749b4320a5be37dccb2e64b0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 38.5M/? [00:01&lt;00:00, 32.9MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_84b9036778044484bb19f272c7be3704"
          }
        },
        "efe766d9df5d4b559dee67bcf1f50c8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1010972e9ac348aba93396e550239712": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "632141fc749b4320a5be37dccb2e64b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "84b9036778044484bb19f272c7be3704": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4097582128074b04b685b0e0dcf81571": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4394c7a212e848d6b43fae8e794e5489",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b92831f0f6cf4b81bb5aaf07709e76c2",
              "IPY_MODEL_95a39ea5baa6433a8a7945bc1321c0c2"
            ]
          }
        },
        "4394c7a212e848d6b43fae8e794e5489": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b92831f0f6cf4b81bb5aaf07709e76c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_71a0a34e432c43e69408a0a58bb94d7f",
            "_dom_classes": [],
            "description": "Downloading: ",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 770480,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 770480,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7676240a37dc451fae13593c0a14dd04"
          }
        },
        "95a39ea5baa6433a8a7945bc1321c0c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_cc04180e950142bfbbac22e9b3f50631",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 3.88M/? [00:00&lt;00:00, 12.0MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8dbdf60713994ee487b866aef3e9d8fb"
          }
        },
        "71a0a34e432c43e69408a0a58bb94d7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7676240a37dc451fae13593c0a14dd04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cc04180e950142bfbbac22e9b3f50631": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8dbdf60713994ee487b866aef3e9d8fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cd13a20683d1434e81ff24c62c61062f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_468839e9821046dca2522a9f70d08a15",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d527560cc672479da23545843b5ce3d8",
              "IPY_MODEL_ad489bcca0f543a3b4b7a129cd2b9ee6"
            ]
          }
        },
        "468839e9821046dca2522a9f70d08a15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d527560cc672479da23545843b5ce3d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_77056c1516c442d585d44ca811f483e3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4e1749c0a95b49fc94b47f7b47f9bd4f"
          }
        },
        "ad489bcca0f543a3b4b7a129cd2b9ee6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2362bb41af734dd1a177b181db97d4f0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 60407/0 [00:04&lt;00:00, 14382.85 examples/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_041daf84b043406f9556ccdb781a9395"
          }
        },
        "77056c1516c442d585d44ca811f483e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4e1749c0a95b49fc94b47f7b47f9bd4f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2362bb41af734dd1a177b181db97d4f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "041daf84b043406f9556ccdb781a9395": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "868c74a8732d4bc4913a7f0cfbcf84af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_242debe30c79434493b1fa31743c038b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_cd0b327520c149519b65b39079428241",
              "IPY_MODEL_500641e407e24282801352ff47fb1419"
            ]
          }
        },
        "242debe30c79434493b1fa31743c038b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cd0b327520c149519b65b39079428241": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a2c94d8efe9547a386ab4d394b5a0685",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d2a428c6fc674c0dace3089c2ecf8ba4"
          }
        },
        "500641e407e24282801352ff47fb1419": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_febd425758d3491e8afaf7a59b0aaec1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 5774/0 [00:00&lt;00:00, 19.34 examples/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_80904efc1d3d4c88915189bbf9e423d7"
          }
        },
        "a2c94d8efe9547a386ab4d394b5a0685": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d2a428c6fc674c0dace3089c2ecf8ba4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "febd425758d3491e8afaf7a59b0aaec1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "80904efc1d3d4c88915189bbf9e423d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dolmani38/QA/blob/main/Train_korQuAD1.0_0516.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21tlIhtCjJia"
      },
      "source": [
        "# 한국어 Albert Tokenizer 학습!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zc1weyK_jPwQ",
        "outputId": "6ab3ac9e-d34e-47c0-e375-eb467886435a"
      },
      "source": [
        "\n",
        "if True:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5DRavVNRey3"
      },
      "source": [
        "## 참조\n",
        "\n",
        "https://colab.research.google.com/github/parmarsuraj99/suraj-parmar/blob/master/_notebooks/2020-05-02-SanskritALBERT.ipynb#scrollTo=VNAOMXjpMHZD\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "038IYm33Muol"
      },
      "source": [
        "import os\n",
        "import gc\n",
        "import glob\n",
        "import torch\n",
        "import pickle\n",
        "import joblib\n",
        "from tqdm.auto import tqdm"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVCIQF4UMmyF",
        "outputId": "64988b92-45c3-4d42-f640-9b4beb8ba376"
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers\n",
        "!pip install transformers/.\n",
        "!pip install sentencepiece==0.1.95\n",
        "!pip install datasets==1.6.2"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 72664, done.\u001b[K\n",
            "remote: Counting objects: 100% (702/702), done.\u001b[K\n",
            "remote: Compressing objects: 100% (375/375), done.\u001b[K\n",
            "remote: Total 72664 (delta 402), reused 496 (delta 287), pack-reused 71962\u001b[K\n",
            "Receiving objects: 100% (72664/72664), 56.17 MiB | 26.85 MiB/s, done.\n",
            "Resolving deltas: 100% (51530/51530), done.\n",
            "Processing ./transformers\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (20.9)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 4.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (4.0.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 32.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (3.0.12)\n",
            "Collecting huggingface-hub==0.0.8\n",
            "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.7.0.dev0) (2.4.7)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.7.0.dev0) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.7.0.dev0) (3.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.7.0.dev0) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.7.0.dev0) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.7.0.dev0) (8.0.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.7.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.7.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.7.0.dev0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.7.0.dev0) (2020.12.5)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.7.0.dev0-cp37-none-any.whl size=2259809 sha256=b3393a1f7dfd6214f6b775a1ad292efa6a4b2da94954ea168efbdf64e7bda812\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-2fk6hb2y/wheels/23/19/dd/2561a4e47240cf6b307729d58e56f8077dd0c698f5992216cf\n",
            "Successfully built transformers\n",
            "Installing collected packages: tokenizers, sacremoses, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.0.8 sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.7.0.dev0\n",
            "Collecting sentencepiece==0.1.95\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 4.3MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.95\n",
            "Collecting datasets==1.6.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/1a/b9f9b3bfef624686ae81c070f0a6bb635047b17cdb3698c7ad01281e6f9a/datasets-1.6.2-py3-none-any.whl (221kB)\n",
            "\u001b[K     |████████████████████████████████| 225kB 4.3MB/s \n",
            "\u001b[?25hCollecting xxhash\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/4f/0a862cad26aa2ed7a7cd87178cbbfa824fc1383e472d63596a0d018374e7/xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 31.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets==1.6.2) (4.0.1)\n",
            "Requirement already satisfied: huggingface-hub<0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets==1.6.2) (0.0.8)\n",
            "Requirement already satisfied: pyarrow>=1.0.0<4.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets==1.6.2) (3.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets==1.6.2) (20.9)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets==1.6.2) (2.23.0)\n",
            "Collecting fsspec\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/52/816d1a3a599176057bf29dfacb1f8fadb61d35fbd96cb1bab4aaa7df83c0/fsspec-2021.5.0-py3-none-any.whl (111kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 29.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets==1.6.2) (0.70.11.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets==1.6.2) (1.19.5)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets==1.6.2) (4.41.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets==1.6.2) (1.1.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets==1.6.2) (0.3.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets==1.6.2) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets==1.6.2) (3.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets==1.6.2) (3.0.12)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets==1.6.2) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.6.2) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.6.2) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.6.2) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.6.2) (2.10)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.6.2) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.6.2) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets==1.6.2) (1.15.0)\n",
            "Installing collected packages: xxhash, fsspec, datasets\n",
            "Successfully installed datasets-1.6.2 fsspec-2021.5.0 xxhash-2.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQt6ukg-M5p9"
      },
      "source": [
        "## mecab 설치 참조\n",
        "\n",
        "https://somjang.tistory.com/entry/Google-Colab%EC%97%90%EC%84%9C-Mecab-koMecab-ko-dic-%EC%89%BD%EA%B2%8C-%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7E1YDPFdMslI",
        "outputId": "02b58627-275a-4a5c-8979-f06ff6627975"
      },
      "source": [
        "!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Mecab-ko-for-Google-Colab'...\n",
            "remote: Enumerating objects: 91, done.\u001b[K\n",
            "remote: Counting objects: 100% (91/91), done.\u001b[K\n",
            "remote: Compressing objects: 100% (85/85), done.\u001b[K\n",
            "remote: Total 91 (delta 43), reused 22 (delta 6), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (91/91), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1XdrdGGM9ky",
        "outputId": "a62569f9-26fb-40a5-c7de-af343053b72d"
      },
      "source": [
        "!bash ./Mecab-ko-for-Google-Colab/install_mecab-ko_on_colab190912.sh"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Installing konlpy.....\n",
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 1.3MB/s \n",
            "\u001b[?25hCollecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Collecting JPype1>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/a5/9781e2ef4ca92d09912c4794642c1653aea7607f473e156cf4d423a881a1/JPype1-1.2.1-cp37-cp37m-manylinux2010_x86_64.whl (457kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 38.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 10.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.12.5)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Installing collected packages: colorama, JPype1, beautifulsoup4, konlpy\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-1.2.1 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2\n",
            "Done\n",
            "Installing mecab-0.996-ko-0.9.2.tar.gz.....\n",
            "Downloading mecab-0.996-ko-0.9.2.tar.gz.......\n",
            "from https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz\n",
            "--2021-05-16 00:27:56--  https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz\n",
            "Resolving bitbucket.org (bitbucket.org)... 104.192.141.1, 2406:da00:ff00::6b17:d1f5, 2406:da00:ff00::22cd:e0db, ...\n",
            "Connecting to bitbucket.org (bitbucket.org)|104.192.141.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://bbuseruploads.s3.amazonaws.com/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz?Signature=67OlFPehmTqro5oAkWE3QxK%2FEUs%3D&Expires=1621126677&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=null&response-content-disposition=attachment%3B%20filename%3D%22mecab-0.996-ko-0.9.2.tar.gz%22&response-content-encoding=None [following]\n",
            "--2021-05-16 00:27:57--  https://bbuseruploads.s3.amazonaws.com/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz?Signature=67OlFPehmTqro5oAkWE3QxK%2FEUs%3D&Expires=1621126677&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=null&response-content-disposition=attachment%3B%20filename%3D%22mecab-0.996-ko-0.9.2.tar.gz%22&response-content-encoding=None\n",
            "Resolving bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)... 52.217.14.12\n",
            "Connecting to bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)|52.217.14.12|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1414979 (1.3M) [application/x-tar]\n",
            "Saving to: ‘mecab-0.996-ko-0.9.2.tar.gz’\n",
            "\n",
            "mecab-0.996-ko-0.9. 100%[===================>]   1.35M  2.76MB/s    in 0.5s    \n",
            "\n",
            "2021-05-16 00:27:58 (2.76 MB/s) - ‘mecab-0.996-ko-0.9.2.tar.gz’ saved [1414979/1414979]\n",
            "\n",
            "Done\n",
            "Unpacking mecab-0.996-ko-0.9.2.tar.gz.......\n",
            "Done\n",
            "Change Directory to mecab-0.996-ko-0.9.2.......\n",
            "installing mecab-0.996-ko-0.9.2.tar.gz........\n",
            "configure\n",
            "make\n",
            "make check\n",
            "make install\n",
            "ldconfig\n",
            "Done\n",
            "Change Directory to /content\n",
            "Downloading mecab-ko-dic-2.1.1-20180720.tar.gz.......\n",
            "from https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz\n",
            "--2021-05-16 00:29:28--  https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz\n",
            "Resolving bitbucket.org (bitbucket.org)... 104.192.141.1, 2406:da00:ff00::22cd:e0db, 2406:da00:ff00::6b17:d1f5, ...\n",
            "Connecting to bitbucket.org (bitbucket.org)|104.192.141.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://bbuseruploads.s3.amazonaws.com/a4fcd83e-34f1-454e-a6ac-c242c7d434d3/downloads/b5a0c703-7b64-45ed-a2d7-180e962710b6/mecab-ko-dic-2.1.1-20180720.tar.gz?Signature=AFp1DDrdBcn9tGABJKqq%2BkVxBHM%3D&Expires=1621126768&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=tzyxc1TtnZU_zEuaaQDGN4F76hPDpyFq&response-content-disposition=attachment%3B%20filename%3D%22mecab-ko-dic-2.1.1-20180720.tar.gz%22&response-content-encoding=None [following]\n",
            "--2021-05-16 00:29:28--  https://bbuseruploads.s3.amazonaws.com/a4fcd83e-34f1-454e-a6ac-c242c7d434d3/downloads/b5a0c703-7b64-45ed-a2d7-180e962710b6/mecab-ko-dic-2.1.1-20180720.tar.gz?Signature=AFp1DDrdBcn9tGABJKqq%2BkVxBHM%3D&Expires=1621126768&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=tzyxc1TtnZU_zEuaaQDGN4F76hPDpyFq&response-content-disposition=attachment%3B%20filename%3D%22mecab-ko-dic-2.1.1-20180720.tar.gz%22&response-content-encoding=None\n",
            "Resolving bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)... 52.217.0.164\n",
            "Connecting to bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)|52.217.0.164|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 49775061 (47M) [application/x-tar]\n",
            "Saving to: ‘mecab-ko-dic-2.1.1-20180720.tar.gz’\n",
            "\n",
            "mecab-ko-dic-2.1.1- 100%[===================>]  47.47M  24.3MB/s    in 2.0s    \n",
            "\n",
            "2021-05-16 00:29:31 (24.3 MB/s) - ‘mecab-ko-dic-2.1.1-20180720.tar.gz’ saved [49775061/49775061]\n",
            "\n",
            "Done\n",
            "Unpacking  mecab-ko-dic-2.1.1-20180720.tar.gz.......\n",
            "Done\n",
            "Change Directory to mecab-ko-dic-2.1.1-20180720\n",
            "Done\n",
            "installing........\n",
            "configure\n",
            "make\n",
            "make install\n",
            "apt-get update\n",
            "apt-get upgrade\n",
            "apt install curl\n",
            "apt install git\n",
            "bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\n",
            "Done\n",
            "Successfully Installed\n",
            "Now you can use Mecab\n",
            "from konlpy.tag import Mecab\n",
            "mecab = Mecab()\n",
            "사용자 사전 추가 방법 : https://bit.ly/3k0ZH53\n",
            "NameError: name 'Tagger' is not defined 오류 발생 시 런타임을 재실행 해주세요\n",
            "블로그에 해결 방법을 남겨주신 tana님 감사합니다.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecJdu9LKxh2z"
      },
      "source": [
        "# 한국어 Tokenizer 학습"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaP5MmCejnq_"
      },
      "source": [
        "## dataset 만들기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqRjnbbJlpBl"
      },
      "source": [
        "### 네이버 뉴스.... 영화 평가 댓글... 에서 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUJMmmyZmpqX",
        "outputId": "994eb67e-8960-47ed-c6d1-85097cf422ec"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ea31IIPRm2cy"
      },
      "source": [
        "\n",
        "# 간단한 전처리\n",
        "def clean_text(txt):\n",
        "    txt = txt.replace('\\n',' ')\n",
        "    txt = txt.replace('\\r',' ')    \n",
        "    txt = txt.replace('=','')\n",
        "    txt = txt.replace('\\\"','')   \n",
        "    txt = txt.replace('\\'','')\n",
        "    #txt = txt.replace(',','')\n",
        "    txt = txt.replace('..','')\n",
        "    txt = txt.replace('...','')\n",
        "    #txt = txt.replace('.','. ')\n",
        "    txt = txt.replace('.','. ')\n",
        "    txt = txt.replace('  ',' ')\n",
        "    txt = txt.replace('  ',' ')    \n",
        "    txt = txt.replace('  ',' ')   \n",
        "    txt = txt.replace('  ',' ')           \n",
        "    txt = txt.replace('  ',' ')\n",
        "    txt = txt.replace('  ',' ')    \n",
        "    txt = txt.replace('  ',' ')   \n",
        "    txt = txt.replace('  ',' ')             \n",
        "    return txt.strip()\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RijcRl1ukG3Z",
        "outputId": "0967639c-2c8d-4d60-ba25-878b7e67afa6"
      },
      "source": [
        "%%time\n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "import sys\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/summary/korean_news_corpus.csv')\n",
        "# 검사...\n",
        "pattens = [\"[34569][0-9]{3}[\\;.\\;-\\; ][0-9]{4}[\\;.\\;-\\; ][0-9]{4}[\\;.\\;-\\; ][0-9]{4}\",\n",
        "           \"[0-9]{2,3}[\\:\\s\\;.\\;,\\;-;)][0-9]{3,4}[\\:\\s\\;.\\;,\\;-][0-9]{4}\",\n",
        "           \"[0-9]{1}[0-9]{1}[\\W]?[0-1]{1}[0-9]{1}[\\W]?[0-3]{1}[\\W]?[0-9]{1}[\\W]?[1-4]{1}[\\W]?[0-9]{1}[\\W]?[0-9]{1}[\\W]?[0-9]{1}[\\W]?[0-9]{1}[\\W]?[0-9]{1}[\\W]?[0-9]{1}\",\n",
        "           \"[0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{6}|[0-9]{3}[\\:\\s\\;.\\;,\\;-]([0-9]{5,6}[\\:\\s\\;.\\;,\\;-][0-9]{3}|[0-9]{6}[\\:\\s\\;.\\;,\\;-][0-9]{5}|[0-9]{2,3}[\\:\\s\\;.\\;,\\;-][0-9]{6}|[0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{7}|[0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{4,6}[\\:\\s\\;.\\;,\\;-][0-9]|[0-9]{5}[\\:\\s\\;.\\;,\\;-][0-9]{3}[\\:\\s\\;.\\;,\\;-][0-9]{2}|[0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{5}[\\:\\s\\;.\\;,\\;-][0-9]{3}|[0-9]{4}[\\:\\s\\;.\\;,\\;-][0-9]{4}[\\:\\s\\;.\\;,\\;-][0-9]{3}|[0-9]{6}[\\:\\s\\;.\\;,\\;-][0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{3}|[0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{7})|[0-9]{4}[\\:\\s\\;.\\;,\\;-]([0-9]{3}[\\:\\s\\;.\\;,\\;-][0-9]{6}|[0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{6}[\\:\\s\\;.\\;,\\;-][0-9])|[0-9]{5}[\\:\\s\\;.\\;,\\;-][0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{6}|[0-9]{6}[\\:\\s\\;.\\;,\\;-][0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{5,6}\"\n",
        "           ]\n",
        "\n",
        "filters = []\n",
        "for p in pattens:\n",
        "    filters.append(re.compile(p))\n",
        "\n",
        "sentences = []\n",
        "df = df.dropna(axis=0)\n",
        "cnt = df['contents'].count()\n",
        "print('Total row count:',cnt)\n",
        "i=0\n",
        "for raw_text in df['contents']:\n",
        "    i=i+1\n",
        "    try:\n",
        "        if i%100 == 0:\n",
        "            percent = (\"{0:.2f}\").format(100 * (i / float(cnt)))\n",
        "            print(f'\\r {percent}% {i}/{str(cnt)}', end=\"\", flush=True)\n",
        "\n",
        "        docs = nltk.sent_tokenize(clean_text(raw_text))\n",
        "        for txt in docs:\n",
        "            if txt.find('▶') > -1 or txt.find('@') > -1 or txt.find('ⓒ') > -1: \n",
        "                pass\n",
        "            else:\n",
        "                txt = txt.strip()\n",
        "                if any(chr.isdigit() for chr in txt) :\n",
        "                    pass\n",
        "                else:\n",
        "                    sentences.append(txt)\n",
        "    except KeyboardInterrupt as ki:\n",
        "        raise ki        \n",
        "    except:\n",
        "        pass #print(\"Unexpected error:\", sys.exc_info()[0])\n",
        "\n",
        "print('')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total row count: 140536\n",
            " 99.97% 140500/140536\n",
            "CPU times: user 3min 19s, sys: 4.44 s, total: 3min 24s\n",
            "Wall time: 3min 25s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-U-syo92kQis",
        "outputId": "f980d0f8-66cd-4884-9a28-d066da55a4f5"
      },
      "source": [
        "len(sentences)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2967202"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FcekjFG6nvX9",
        "outputId": "a8a969de-2175-4e42-f455-e77b56349506"
      },
      "source": [
        "sentences[:10]"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['연합뉴스 문재인 대통령이 변창흠 국토교통부 장관의 사의 표명을 사실상 수용했다.',\n",
              " '문 대통령은 변 장관의 사의 표명에 책임지는 모습을 보일 수밖에 없다고 밝혔다.',\n",
              " '이는 문 대통령이 사실상 변 장관의 사의를 수용한 것으로 해석된다.',\n",
              " '이에 앞서 변창흠 국토교통부 장관은 LH 땅 투기 의혹 사건과 관련한 책임론에 대해 “자리에 연연하지 않는다”면서 “(청와대의) 결정에 따르겠다”고 말했다.',\n",
              " '변 장관은 이날 국회 국토교통위원회 전체회의에 참석해 “LH 사태로 국민들이 걱정하는 부분을 해소할 수 있게 최대한 대안을 만들고 LH가 근본적으로 다시 태어날 수 있도록 책임지고 추진하겠다”고 언급하고 “그 역할이 충분하다고 평가되지 못했을 때 언제든지 자리에 연연하지 않고 결정에 따르겠다”고 말했다.',\n",
              " 'kr ) 무단 전재 및 재배포 금지',\n",
              " 'kr ) 무단 전재 및 재배포 금지',\n",
              " 'kr ) 무단 전재 및 재배포 금지',\n",
              " '인재 양성·소외 계층 지원 등 계획 “부친 질병·가난 악순환 끊기 원해 국내 최고 넘어 세계적 병원 되길 정몽구 현대차그룹 명예회장.',\n",
              " '그는 “질병과 가난이 악순환되는 고리를 끊기 위해 아산재단과 서울아산병원을 설립했던 아버님의 뜻을 이어 우리 사회의 어려운 이웃을 돕는 데 보탬이 되기를 바란다”고 기부 취지를 밝혔다.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGheAmmIkVQ4",
        "outputId": "cd480125-d385-433e-8822-7cb5d2398f61"
      },
      "source": [
        "%%time\n",
        "\n",
        "import re\n",
        "import sys\n",
        "import io\n",
        "\n",
        "#텍스트 정제(전처리)\n",
        "def cleanText(readData):\n",
        "    #텍스트에 포함되어 있는 특수 문자 제거\n",
        "    text = re.sub('[-=+#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》◆◇●🎧○▲\\t―△━▷]', '', readData)\n",
        "    return text\n",
        "\n",
        "c_sentences = []\n",
        "for sentence in sentences:\n",
        "    s = cleanText(sentence)\n",
        "    c = len(s.split())\n",
        "    if c >= 3 and c < 10 and s.find('재배포') < 0 and s.find('기자') < 0  and s.find('유투브') < 0 and s.find('www') < 0 and s.find('com') < 0 and s.find('접속하기') < 0 and s.find('http') < 0 and s.find('뉴스') < 0 and s.find('일보') < 0 :\n",
        "        if s.endswith(('다','요')):\n",
        "            c_sentences.append(s.strip())    "
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 12.8 s, sys: 194 ms, total: 13 s\n",
            "Wall time: 13 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb6oBQFrkV_3",
        "outputId": "73faa22b-c98a-4795-b050-1ced40a7238e"
      },
      "source": [
        "len(c_sentences)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "867766"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChgB1u3fmQzL"
      },
      "source": [
        "# 한국어 영화 리뷰를 Load.\n",
        "ds = pd.read_csv(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\",sep='\\t')\n",
        "for row in ds.iterrows():\n",
        "    doc_id = row[1][0]\n",
        "    doc_cont = str(row[1][1])\n",
        "    c_sentences.append(doc_cont)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8W3HKirnUG1",
        "outputId": "61ba768e-5664-4127-cf24-72129b4f5fab"
      },
      "source": [
        "len(c_sentences)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1017766"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDANhUJ9qDel"
      },
      "source": [
        "import random\n",
        "\n",
        "random.shuffle(c_sentences)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugNi7PH5qtw2"
      },
      "source": [
        "### 한국어... 교착어 처리..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMRu1dDHqre-",
        "outputId": "ca4c1a6e-82f9-4f82-e14d-cf082d44c55a"
      },
      "source": [
        "%%time\n",
        "# mecab for window는 아래 코드 사용\n",
        "from konlpy.tag import Mecab  # install mecab for window: https://hong-yp-ml-records.tistory.com/91\n",
        "mecab_tokenizer = Mecab().morphs\n",
        "print('mecab check :', mecab_tokenizer('어릴때보고 지금다시봐도 재밌어요ㅋㅋ'))\n",
        "\n",
        "ko_sentences = []\n",
        "\n",
        "for sentence in c_sentences:\n",
        "    # 문장단위 mecab 적용\n",
        "    morph_sentence= mecab_tokenizer(sentence)\n",
        "    # 문장단위 저장\n",
        "    ko_sentences.append( (' '.join(morph_sentence)).strip())\n",
        "        \n",
        "num_word_list = [len(sentence.split()) for sentence in ko_sentences]\n",
        "print('\\n힌국어 코퍼스 평균/총 단어 갯수 : %.1f / %d' % (sum(num_word_list)/len(num_word_list), sum(num_word_list)))       "
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mecab check : ['어릴', '때', '보', '고', '지금', '다시', '봐도', '재밌', '어요', 'ㅋㅋ']\n",
            "\n",
            "힌국어 코퍼스 평균/총 단어 갯수 : 13.7 / 13929565\n",
            "CPU times: user 1min 23s, sys: 849 ms, total: 1min 24s\n",
            "Wall time: 1min 24s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2ZFt4Xrnpak",
        "outputId": "0995fdf5-c176-43c9-8696-3766967f0e8b"
      },
      "source": [
        "%%time\n",
        "# subword 학습을 위해 문장만 따로 저장\n",
        "with open('/content/drive/MyDrive/Tokenizer_train/data/train_tokenizer.txt', 'w', encoding='utf-8') as f:\n",
        "    for line in ko_sentences:\n",
        "        f.write(line+'\\n')\n",
        "\n",
        "f.close()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 575 ms, sys: 115 ms, total: 691 ms\n",
            "Wall time: 2.14 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sZ4yn-toGXw"
      },
      "source": [
        "### CNN/Daily main Dataset에서 영어... 문장 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 647,
          "referenced_widgets": [
            "e0a234238a20442188bffeb50b8a82e0",
            "508da04576294734b60f3fa90d40b2f5",
            "31dda3106cc647fda699a60bf34a320c",
            "7269bb94559540d8a2ad15890118c9a0",
            "4bbddc49854648ee809cab2744f19f03",
            "a6b91a068a2445da88b086a99123cca1",
            "a0c5df4e61074ded90aadad9d9123604",
            "ffaa2c46811545579645138631dfcfb7",
            "b07bff8f8c0242cb9d14c4e9200fe065",
            "c138a58915804d3a82cdab0711e6ee13",
            "b2f387970f2b453db3beaedc6aaea0a0",
            "10d6cdeac7824a1da0e7ee23a9857162",
            "8cde191a1d694cac973a9ff53c680c6f",
            "f715a113aec4444f91d26969ea994d30",
            "8fad692c0da84f67b2981b9dc5085798",
            "d94643c7f32d4c679260bb0abf305de3",
            "a94f18110c324dc780e714e50491e9fa",
            "be9e86e7e0b54aa1a8cceb4a7c44fbd9",
            "d6c60b10cea943e2b0fdba4f546ec849",
            "d8100564205c4989b0cc9512ffdef942",
            "83c6a479d4594efdaa7c8fdf758f86ef",
            "edb7645bd9cf4ebcb0472ab8413f15ec",
            "6dd9062c334a4755ba7b0a1bd86468ca",
            "0ecf99f564724d79b28843833d2e9c6b",
            "141567d835724105af5415594e9a7f9a",
            "2293ab6f8444436d826a597626e581f1",
            "e7df796c569f4432b158b7d360cea766",
            "dc0e96d7d05e4ed0ac616431ffc17104",
            "845e048e05db4a1ba2fbe08d84aefa40",
            "15172d9909fd47dc98f1666eb3ab3102",
            "058be0a299de4e408294b8551c55bc63",
            "6ecd6166ed4b41b9b09da5d04d26ada4"
          ]
        },
        "id": "gpv2vn9Bom28",
        "outputId": "c0512baf-6051-4174-90f4-ec6cbecbca8a"
      },
      "source": [
        "\n",
        "import tensorflow_datasets as tfds\n",
        "train_data, test_data = tfds.load(name=\"cnn_dailymail\",split=(tfds.Split.TRAIN,tfds.Split.TEST),with_info=True)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mDownloading and preparing dataset cnn_dailymail/plain_text/3.0.0 (download: 558.32 MiB, generated: 1.27 GiB, total: 1.82 GiB) to /root/tensorflow_datasets/cnn_dailymail/plain_text/3.0.0...\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e0a234238a20442188bffeb50b8a82e0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Completed...', max=1.0, style=Progre…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b07bff8f8c0242cb9d14c4e9200fe065",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Size...', max=1.0, style=ProgressSty…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a94f18110c324dc780e714e50491e9fa",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Extraction completed...', max=1.0, styl…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "141567d835724105af5415594e9a7f9a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-7ea54526221a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_datasets\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cnn_dailymail\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSplit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtfds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSplit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTEST\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwith_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/load.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, split, data_dir, batch_size, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)\u001b[0m\n\u001b[1;32m    342\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[0mdownload_and_prepare_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_and_prepare_kwargs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m     \u001b[0mdbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_and_prepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdownload_and_prepare_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mas_dataset_kwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/dataset_builder.py\u001b[0m in \u001b[0;36mdownload_and_prepare\u001b[0;34m(self, download_dir, download_config)\u001b[0m\n\u001b[1;32m    385\u001b[0m           self._download_and_prepare(\n\u001b[1;32m    386\u001b[0m               \u001b[0mdl_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdl_manager\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m               download_config=download_config)\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m           \u001b[0;31m# NOTE: If modifying the lines below to put additional information in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/dataset_builder.py\u001b[0m in \u001b[0;36m_download_and_prepare\u001b[0;34m(self, dl_manager, download_config)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     super(GeneratorBasedBuilder, self)._download_and_prepare(\n\u001b[1;32m   1023\u001b[0m         \u001b[0mdl_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdl_manager\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m         \u001b[0mmax_examples_per_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_examples_per_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m     )\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/dataset_builder.py\u001b[0m in \u001b[0;36m_download_and_prepare\u001b[0;34m(self, dl_manager, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m    972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m       \u001b[0;31m# Prepare split will record examples associated to the split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 974\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mprepare_split_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    975\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m     \u001b[0;31m# Update the info object with the splits.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/dataset_builder.py\u001b[0m in \u001b[0;36m_prepare_split\u001b[0;34m(self, split_generator, max_examples_per_split)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                      hash_salt=split_generator.name)\n\u001b[1;32m   1038\u001b[0m     for key, record in utils.tqdm(generator, unit=\" examples\",\n\u001b[0;32m-> 1039\u001b[0;31m                                   total=split_info.num_examples, leave=False):\n\u001b[0m\u001b[1;32m   1040\u001b[0m       \u001b[0mexample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[1;32m   1103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1104\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1105\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m             \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/summarization/cnn_dailymail.py\u001b[0m in \u001b[0;36m_generate_examples\u001b[0;34m(self, files)\u001b[0m\n\u001b[1;32m    306\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_generate_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m       \u001b[0marticle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhighlights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_art_abs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0marticle\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhighlights\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/summarization/cnn_dailymail.py\u001b[0m in \u001b[0;36m_get_art_abs\u001b[0;34m(story_file, tfds_version)\u001b[0m\n\u001b[1;32m    187\u001b[0m   \u001b[0;31m#     make_datafiles.py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m   \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_text_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstory_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m   \u001b[0;31m# The github code lowercase the text and we removed it in 3.0.0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/summarization/cnn_dailymail.py\u001b[0m in \u001b[0;36m_read_text_file\u001b[0;34m(text_file)\u001b[0m\n\u001b[1;32m    177\u001b[0m   \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m       \u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m     \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mreadline\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;34mr\"\"\"Reads the next line, keeping \\n. At EOF, returns ''.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preread_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_buf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xc7bauWlov2n"
      },
      "source": [
        "%%time\n",
        "en_sentences = []\n",
        "iterator = iter(test_data[0])\n",
        "for data in iterator:\n",
        "    article = nltk.sent_tokenize(data['article'].numpy().decode('UTF-8'))\n",
        "    for sent in article:    \n",
        "        en_sentences.append(sent)\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VhwqYk_o32m"
      },
      "source": [
        "num_word_list = [len(sentence.split()) for sentence in en_sentences]\n",
        "print('\\n영어 코퍼스 평균/총 단어 갯수 : %.1f / %d' % (sum(num_word_list)/len(num_word_list), sum(num_word_list)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3V3QEInEkhEl",
        "outputId": "899d9a85-fd37-466d-b87c-2999f9f7d720"
      },
      "source": [
        "%%time\n",
        "# subword 학습을 위해 문장만 따로 저장\n",
        "with open('/content/drive/MyDrive/Tokenizer_train/data/train_tokenizer.txt', 'w', encoding='utf-8') as f:\n",
        "    for line in ko_sentences:\n",
        "        f.write(line+'\\n')\n",
        "\n",
        "f.close()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 288 ms, sys: 40.3 ms, total: 328 ms\n",
            "Wall time: 1.35 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNXWYH5FRyu1"
      },
      "source": [
        "# 여기서부터 다시"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-ircJrdZKh6",
        "outputId": "f974e1d8-fe7d-474e-de39-cd5334904669"
      },
      "source": [
        "%%time\n",
        "import sentencepiece as spm\n",
        "import os\n",
        "# spm_train --input=data/train_tokenizer.txt  --model_prefix=sentencepiece/sp --vocab_size=32000 character_coverage=1.0 --model_type=\"unigram\"\n",
        "\n",
        "input_file = '/content/drive/MyDrive/Tokenizer_train/data/train_tokenizer.txt'\n",
        "vocab_size = 30015\n",
        "\n",
        "sp_model_root='/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model'\n",
        "if not os.path.isdir(sp_model_root):\n",
        "    os.mkdir(sp_model_root)\n",
        "sp_model_name = 'spiece'\n",
        "sp_model_path = os.path.join(sp_model_root, sp_model_name)\n",
        "model_type = 'unigram'  # 학습할 모델 선택, unigram이 더 성능이 좋음'bpe'\n",
        "character_coverage  = 1.0  # 전체를 cover 하기 위해, default=0.9995\n",
        "user_defined_symbols = '[PAD],[UNK],[CLS],[SEP],[MASK],[BOS],[EOS],[UNK0],[UNK1],[UNK2],[UNK3],[UNK4],[UNK5],[UNK6],[UNK7],[UNK8],[UNK9],[unused0],[unused1],[unused2],[unused3],[unused4],[unused5],[unused6],[unused7],[unused8],[unused9],[unused10],[unused11],[unused12],[unused13],[unused14],[unused15],[unused16],[unused17],[unused18],[unused19],[unused20],[unused21],[unused22],[unused23],[unused24],[unused25],[unused26],[unused27],[unused28],[unused29],[unused30],[unused31],[unused32],[unused33],[unused34],[unused35],[unused36],[unused37],[unused38],[unused39],[unused40],[unused41],[unused42],[unused43],[unused44],[unused45],[unused46],[unused47],[unused48],[unused49],[unused50],[unused51],[unused52],[unused53],[unused54],[unused55],[unused56],[unused57],[unused58],[unused59],[unused60],[unused61],[unused62],[unused63],[unused64],[unused65],[unused66],[unused67],[unused68],[unused69],[unused70],[unused71],[unused72],[unused73],[unused74],[unused75],[unused76],[unused77],[unused78],[unused79],[unused80],[unused81],[unused82],[unused83],[unused84],[unused85],[unused86],[unused87],[unused88],[unused89],[unused90],[unused91],[unused92],[unused93],[unused94],[unused95],[unused96],[unused97],[unused98],[unused99]'\n",
        "\n",
        "input_argument = '--input=%s --model_prefix=%s --vocab_size=%s --user_defined_symbols=%s --model_type=%s --character_coverage=%s'\n",
        "cmd = input_argument%(input_file, sp_model_path, vocab_size,user_defined_symbols, model_type, character_coverage)\n",
        "\n",
        "spm.SentencePieceTrainer.Train(cmd)\n",
        "print('train done')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train done\n",
            "CPU times: user 33.2 s, sys: 667 ms, total: 33.9 s\n",
            "Wall time: 28.6 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55T0J0MDFtrm",
        "outputId": "99638ebd-6ef5-4187-e2c4-e55dda183044"
      },
      "source": [
        "## 1) define special tokens\n",
        "user_defined_symbols = ['[BOS]','[EOS]','[UNK0]','[UNK1]','[UNK2]','[UNK3]','[UNK4]','[UNK5]','[UNK6]','[UNK7]','[UNK8]','[UNK9]']\n",
        "unused_token_num = 200\n",
        "unused_list = ['[unused{}]'.format(n) for n in range(unused_token_num)]\n",
        "user_defined_symbols = user_defined_symbols + unused_list\n",
        "\n",
        "print(user_defined_symbols)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[BOS]', '[EOS]', '[UNK0]', '[UNK1]', '[UNK2]', '[UNK3]', '[UNK4]', '[UNK5]', '[UNK6]', '[UNK7]', '[UNK8]', '[UNK9]', '[unused0]', '[unused1]', '[unused2]', '[unused3]', '[unused4]', '[unused5]', '[unused6]', '[unused7]', '[unused8]', '[unused9]', '[unused10]', '[unused11]', '[unused12]', '[unused13]', '[unused14]', '[unused15]', '[unused16]', '[unused17]', '[unused18]', '[unused19]', '[unused20]', '[unused21]', '[unused22]', '[unused23]', '[unused24]', '[unused25]', '[unused26]', '[unused27]', '[unused28]', '[unused29]', '[unused30]', '[unused31]', '[unused32]', '[unused33]', '[unused34]', '[unused35]', '[unused36]', '[unused37]', '[unused38]', '[unused39]', '[unused40]', '[unused41]', '[unused42]', '[unused43]', '[unused44]', '[unused45]', '[unused46]', '[unused47]', '[unused48]', '[unused49]', '[unused50]', '[unused51]', '[unused52]', '[unused53]', '[unused54]', '[unused55]', '[unused56]', '[unused57]', '[unused58]', '[unused59]', '[unused60]', '[unused61]', '[unused62]', '[unused63]', '[unused64]', '[unused65]', '[unused66]', '[unused67]', '[unused68]', '[unused69]', '[unused70]', '[unused71]', '[unused72]', '[unused73]', '[unused74]', '[unused75]', '[unused76]', '[unused77]', '[unused78]', '[unused79]', '[unused80]', '[unused81]', '[unused82]', '[unused83]', '[unused84]', '[unused85]', '[unused86]', '[unused87]', '[unused88]', '[unused89]', '[unused90]', '[unused91]', '[unused92]', '[unused93]', '[unused94]', '[unused95]', '[unused96]', '[unused97]', '[unused98]', '[unused99]', '[unused100]', '[unused101]', '[unused102]', '[unused103]', '[unused104]', '[unused105]', '[unused106]', '[unused107]', '[unused108]', '[unused109]', '[unused110]', '[unused111]', '[unused112]', '[unused113]', '[unused114]', '[unused115]', '[unused116]', '[unused117]', '[unused118]', '[unused119]', '[unused120]', '[unused121]', '[unused122]', '[unused123]', '[unused124]', '[unused125]', '[unused126]', '[unused127]', '[unused128]', '[unused129]', '[unused130]', '[unused131]', '[unused132]', '[unused133]', '[unused134]', '[unused135]', '[unused136]', '[unused137]', '[unused138]', '[unused139]', '[unused140]', '[unused141]', '[unused142]', '[unused143]', '[unused144]', '[unused145]', '[unused146]', '[unused147]', '[unused148]', '[unused149]', '[unused150]', '[unused151]', '[unused152]', '[unused153]', '[unused154]', '[unused155]', '[unused156]', '[unused157]', '[unused158]', '[unused159]', '[unused160]', '[unused161]', '[unused162]', '[unused163]', '[unused164]', '[unused165]', '[unused166]', '[unused167]', '[unused168]', '[unused169]', '[unused170]', '[unused171]', '[unused172]', '[unused173]', '[unused174]', '[unused175]', '[unused176]', '[unused177]', '[unused178]', '[unused179]', '[unused180]', '[unused181]', '[unused182]', '[unused183]', '[unused184]', '[unused185]', '[unused186]', '[unused187]', '[unused188]', '[unused189]', '[unused190]', '[unused191]', '[unused192]', '[unused193]', '[unused194]', '[unused195]', '[unused196]', '[unused197]', '[unused198]', '[unused199]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XF_Q_0VTFzPv",
        "outputId": "3cdc7f8d-264a-4e8d-9698-66952422600a"
      },
      "source": [
        "#'/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model'\n",
        "\n",
        "from transformers import BertTokenizer, AlbertTokenizer\n",
        "\n",
        "albet_tokenizer_model = '/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model'\n",
        "\n",
        "tokenizer = AlbertTokenizer.from_pretrained(albet_tokenizer_model)\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "QB0SlhA4GMQQ",
        "outputId": "0d1ecff0-fc3f-4426-a8e8-208382164e24"
      },
      "source": [
        "op = tokenizer.encode(\"나는 오늘 학교에 간다.\")\n",
        "print(op)\n",
        "tokenizer.decode(op)\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[5, 157, 491, 395, 740, 1614, 1723, 11640, 6]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[CLS] 나는 오늘 학교에 간다.[SEP]'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMDfH8GZF_mK",
        "outputId": "2cde659c-5c7e-4af8-e240-8930cf1746bf"
      },
      "source": [
        "# tokenizer에 special token 추가\n",
        "special_tokens_dict = {'additional_special_tokens': user_defined_symbols}\n",
        "tokenizer.add_special_tokens(special_tokens_dict)\n",
        "\n",
        "# check tokenizer vocab with special tokens\n",
        "print('check special tokens : %s'%tokenizer.all_special_tokens[:20])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "check special tokens : ['[CLS]', '[SEP]', '<unk>', '<pad>', '[MASK]', '[BOS]', '[EOS]', '[UNK0]', '[UNK1]', '[UNK2]', '[UNK3]', '[UNK4]', '[UNK5]', '[UNK6]', '[UNK7]', '[UNK8]', '[UNK9]', '[unused0]', '[unused1]', '[unused2]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Inr3cDwQG6Fs",
        "outputId": "5daeb04b-e77e-48ca-ecec-fcbdcd9b38b3"
      },
      "source": [
        "# save tokenizer model with special tokens\n",
        "tokenizer.save_pretrained(albet_tokenizer_model+'_special2')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/spiece.model',\n",
              " '/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/added_tokens.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwhlWrXXHr6L"
      },
      "source": [
        "tokenizer = AlbertTokenizer.from_pretrained(albet_tokenizer_model+'_special2')"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgHhYsZbH3zr",
        "outputId": "42e9e39a-7484-4988-bd25-81db4c90b42a"
      },
      "source": [
        "op = tokenizer(\"나는 오늘 학교에 간다.\", return_tensors=\"pt\")\n",
        "print(op)\n",
        "print(\"Tokens (str)      : {}\".format([tokenizer.convert_ids_to_tokens(s) for s in op['input_ids'].tolist()[0]]))\n",
        "print(\"Tokens (int)      : {}\".format(op['input_ids'].tolist()[0]))\n",
        "print(\"Tokens (attn_mask): {}\\n\".format(op['attention_mask'].tolist()[0]))\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'input_ids': tensor([[    5,   157,   491,   395,   740,  1614,  1723, 11640,     6]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
            "Tokens (str)      : ['[CLS]', '▁나', '는', '▁오늘', '▁학교', '에', '▁간다', '.', '[SEP]']\n",
            "Tokens (int)      : [5, 157, 491, 395, 740, 1614, 1723, 11640, 6]\n",
            "Tokens (attn_mask): [1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "paElX_hhJAoh",
        "outputId": "15c94fa8-f098-47e8-b3ea-69022323773b"
      },
      "source": [
        "#Checking vocabulary size\n",
        "vocab_size=tokenizer.vocab_size ; vocab_size"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30015"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "9RPI1UerOSCB",
        "outputId": "be3cd8b1-cb77-4de4-9979-40c906ebc89b"
      },
      "source": [
        "import json\n",
        "\n",
        "config = {\n",
        "    \"_name_or_path\": \"dolmani38/albert-xlarge-kor-v1\",\n",
        "    \"architectures\": [\n",
        "        \"AlbertModel\"\n",
        "    ],\n",
        "    \"attention_probs_dropout_prob\": 0,\n",
        "    \"bos_token_id\": 2,\n",
        "    \"classifier_dropout_prob\": 0.1,\n",
        "    \"down_scale_factor\": 1,\n",
        "    \"embedding_size\": 128,\n",
        "    \"eos_token_id\": 3,\n",
        "    \"gap_size\": 0,\n",
        "    \"hidden_act\": \"gelu_new\",\n",
        "    \"hidden_dropout_prob\": 0,\n",
        "    \"hidden_size\": 2048,\n",
        "    \"initializer_range\": 0.02,\n",
        "    \"inner_group_num\": 1,\n",
        "    \"intermediate_size\": 8192,\n",
        "    \"layer_norm_eps\": 1e-12,\n",
        "    \"max_position_embeddings\": 512,\n",
        "    \"model_type\": \"albert\",\n",
        "    \"net_structure_type\": 0,\n",
        "    \"num_attention_heads\": 16,\n",
        "    \"num_hidden_groups\": 1,\n",
        "    \"num_hidden_layers\": 24,\n",
        "    \"num_memory_blocks\": 0,\n",
        "    \"pad_token_id\": 0,\n",
        "    \"position_embedding_type\": \"absolute\",\n",
        "    \"type_vocab_size\": 2,\n",
        "\t\"vocab_size\": vocab_size\n",
        "}\n",
        "with open(albet_tokenizer_model + \"_special2/config.json\", 'w') as fp:\n",
        "    json.dump(config, fp)\n",
        "\n",
        "\n",
        "#Configuration for tokenizer.\n",
        "#Note: I set do_lower_case: False, and keep_accents:True\n",
        "# Opening JSON file\n",
        "f = open(albet_tokenizer_model+ \"_special2/tokenizer_config.json\")\n",
        "   \n",
        "# returns JSON object as \n",
        "# a dictionary\n",
        "tokenizer_config = json.load(f)\n",
        "\n",
        "tokenizer_config['max_len'] = 512\n",
        "tokenizer_config['model_type'] = 'albert'\n",
        "tokenizer_config['do_lower_case'] = False\n",
        "tokenizer_config['keep_accents'] = True\n",
        "\n",
        "with open(albet_tokenizer_model+ \"_special2/tokenizer_config.json\", 'w') as outfile:\n",
        "    json.dump(tokenizer_config, outfile)\n",
        "'''\n",
        "tokenizer_config = {\n",
        "\t\"max_len\": 512,\n",
        "\t\"model_type\": \"albert\",\n",
        "\t\"do_lower_case\":False, \n",
        "\t\"keep_accents\":True\n",
        "}\n",
        "with open(albet_tokenizer_model+ \"_special/tokenizer_config.json\", 'w') as fp:\n",
        "    json.dump(tokenizer_config, fp)\n",
        "'''"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ntokenizer_config = {\\n\\t\"max_len\": 512,\\n\\t\"model_type\": \"albert\",\\n\\t\"do_lower_case\":False, \\n\\t\"keep_accents\":True\\n}\\nwith open(albet_tokenizer_model+ \"_special/tokenizer_config.json\", \\'w\\') as fp:\\n    json.dump(tokenizer_config, fp)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdjOOoG6QX2p",
        "outputId": "8750bb2d-df26-46bd-bf75-27c9c10e7b3f"
      },
      "source": [
        "\n",
        "#To train from scratch\n",
        "!python /content/transformers/examples/pytorch/language-modeling/run_mlm.py \\\n",
        "        --model_type albert-xlarge \\\n",
        "        --config_name /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/ \\\n",
        "        --tokenizer_name /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/ \\\n",
        "        --train_file /content/drive/MyDrive/Tokenizer_train/data/train_tokenizer.txt \\\n",
        "        --output_dir /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model \\\n",
        "        --do_train \\\n",
        "        --line_by_line \\\n",
        "        --save_steps 500 \\\n",
        "        --logging_steps 500 \\\n",
        "        --save_total_limit 2 \\\n",
        "        --num_train_epochs 1 \\\n",
        "        --seed 108 \\\n",
        "        --logging_dir /content/drive/MyDrive/Tokenizer_train/logs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-16 08:19:16.230075: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "05/16/2021 08:19:19 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "05/16/2021 08:19:19 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model, overwrite_output_dir=False, do_train=True, do_eval=False, do_predict=False, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=/content/drive/MyDrive/Tokenizer_train/logs, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.STEPS, save_steps=500, save_total_limit=2, no_cuda=False, seed=108, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, _n_gpu=1, mp_parameters=)\n",
            "05/16/2021 08:19:20 - WARNING - datasets.builder -   Using custom data configuration default-f65fc3c826c08b2e\n",
            "Downloading and preparing dataset text/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/text/default-f65fc3c826c08b2e/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5...\n",
            "Dataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-f65fc3c826c08b2e/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5. Subsequent calls will reuse this data.\n",
            "[INFO|configuration_utils.py:515] 2021-05-16 08:19:24,985 >> loading configuration file /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/config.json\n",
            "[INFO|configuration_utils.py:553] 2021-05-16 08:19:24,986 >> Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"dolmani38/albert-xlarge-kor-v1\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu_new\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.7.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30015\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:515] 2021-05-16 08:19:24,987 >> loading configuration file /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/config.json\n",
            "[INFO|configuration_utils.py:553] 2021-05-16 08:19:24,988 >> Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"dolmani38/albert-xlarge-kor-v1\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu_new\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.7.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30015\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1651] 2021-05-16 08:19:24,989 >> Didn't find file /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/tokenizer.json. We won't load it.\n",
            "[INFO|tokenization_utils_base.py:1715] 2021-05-16 08:19:24,991 >> loading file /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/spiece.model\n",
            "[INFO|tokenization_utils_base.py:1715] 2021-05-16 08:19:24,991 >> loading file None\n",
            "[INFO|tokenization_utils_base.py:1715] 2021-05-16 08:19:24,991 >> loading file /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:1715] 2021-05-16 08:19:24,991 >> loading file /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:1715] 2021-05-16 08:19:24,992 >> loading file /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/tokenizer_config.json\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,651 >> Adding <pad> to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,652 >> Adding [unused100] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,652 >> Adding [unused101] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,652 >> Adding [unused102] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,652 >> Adding [unused103] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,652 >> Adding [unused104] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,652 >> Adding [unused105] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,652 >> Adding [unused106] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,652 >> Adding [unused107] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,652 >> Adding [unused108] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,652 >> Adding [unused109] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,652 >> Adding [unused110] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,652 >> Adding [unused111] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,653 >> Adding [unused112] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,653 >> Adding [unused113] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,653 >> Adding [unused114] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,653 >> Adding [unused115] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,653 >> Adding [unused116] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,653 >> Adding [unused117] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,653 >> Adding [unused118] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,653 >> Adding [unused119] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,653 >> Adding [unused120] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,653 >> Adding [unused121] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,653 >> Adding [unused122] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,653 >> Adding [unused123] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,653 >> Adding [unused124] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,654 >> Adding [unused125] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,654 >> Adding [unused126] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,654 >> Adding [unused127] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,654 >> Adding [unused128] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,654 >> Adding [unused129] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,654 >> Adding [unused130] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,654 >> Adding [unused131] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,654 >> Adding [unused132] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,654 >> Adding [unused133] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,654 >> Adding [unused134] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,654 >> Adding [unused135] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,654 >> Adding [unused136] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,654 >> Adding [unused137] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,654 >> Adding [unused138] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,654 >> Adding [unused139] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,655 >> Adding [unused140] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,655 >> Adding [unused141] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,655 >> Adding [unused142] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,655 >> Adding [unused143] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,655 >> Adding [unused144] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,655 >> Adding [unused145] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,655 >> Adding [unused146] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,655 >> Adding [unused147] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,655 >> Adding [unused148] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,655 >> Adding [unused149] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,655 >> Adding [unused150] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,655 >> Adding [unused151] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,655 >> Adding [unused152] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,655 >> Adding [unused153] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,656 >> Adding [unused154] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,656 >> Adding [unused155] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,656 >> Adding [unused156] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,656 >> Adding [unused157] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,656 >> Adding [unused158] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,656 >> Adding [unused159] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,656 >> Adding [unused160] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,656 >> Adding [unused161] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,656 >> Adding [unused162] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,656 >> Adding [unused163] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,656 >> Adding [unused164] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,656 >> Adding [unused165] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,656 >> Adding [unused166] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,656 >> Adding [unused167] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,657 >> Adding [unused168] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,657 >> Adding [unused169] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,657 >> Adding [unused170] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,657 >> Adding [unused171] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,657 >> Adding [unused172] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,657 >> Adding [unused173] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,657 >> Adding [unused174] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,657 >> Adding [unused175] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,657 >> Adding [unused176] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,657 >> Adding [unused177] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,657 >> Adding [unused178] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,657 >> Adding [unused179] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,657 >> Adding [unused180] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,658 >> Adding [unused181] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,658 >> Adding [unused182] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,658 >> Adding [unused183] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,658 >> Adding [unused184] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,658 >> Adding [unused185] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,658 >> Adding [unused186] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,658 >> Adding [unused187] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,658 >> Adding [unused188] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,658 >> Adding [unused189] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,658 >> Adding [unused190] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,658 >> Adding [unused191] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,658 >> Adding [unused192] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,658 >> Adding [unused193] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,658 >> Adding [unused194] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,658 >> Adding [unused195] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,659 >> Adding [unused196] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,659 >> Adding [unused197] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,659 >> Adding [unused198] to the vocabulary\n",
            "[INFO|tokenization_utils.py:207] 2021-05-16 08:19:28,659 >> Adding [unused199] to the vocabulary\n",
            "05/16/2021 08:19:28 - INFO - __main__ -   Training new model from scratch\n",
            "100% 1018/1018 [00:44<00:00, 22.73ba/s]\n",
            "[INFO|trainer.py:1043] 2021-05-16 08:20:21,936 >> Loading model from /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-500).\n",
            "[INFO|trainer.py:516] 2021-05-16 08:20:26,077 >> The following columns in the training set  don't have a corresponding argument in `AlbertForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[INFO|trainer.py:1145] 2021-05-16 08:20:37,277 >> ***** Running training *****\n",
            "[INFO|trainer.py:1146] 2021-05-16 08:20:37,277 >>   Num examples = 1017766\n",
            "[INFO|trainer.py:1147] 2021-05-16 08:20:37,277 >>   Num Epochs = 1\n",
            "[INFO|trainer.py:1148] 2021-05-16 08:20:37,277 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1149] 2021-05-16 08:20:37,277 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:1150] 2021-05-16 08:20:37,277 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1151] 2021-05-16 08:20:37,277 >>   Total optimization steps = 127221\n",
            "[INFO|trainer.py:1171] 2021-05-16 08:20:37,994 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1172] 2021-05-16 08:20:37,994 >>   Continuing training from epoch 0\n",
            "[INFO|trainer.py:1173] 2021-05-16 08:20:37,994 >>   Continuing training from global step 500\n",
            "[INFO|trainer.py:1176] 2021-05-16 08:20:37,994 >>   Will skip the first 0 epochs then the first 500 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.\n",
            "Skipping the first batches:   0% 0/500 [00:00<?, ?it/s]\n",
            "Skipping the first batches:  98% 488/500 [00:03<00:00, 203.00it/s]\n",
            "Skipping the first batches: 100% 500/500 [00:03<00:00, 136.86it/s]\n",
            "{'loss': 7.5582, 'learning_rate': 4.960698312385534e-05, 'epoch': 0.01}\n",
            "  1% 1000/127221 [03:08<11:51:37,  2.96it/s][INFO|trainer.py:1885] 2021-05-16 08:23:47,243 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-1000\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 08:23:47,249 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-1000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 08:23:48,022 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-1000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 08:23:48,050 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 08:23:48,055 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-1000/special_tokens_map.json\n",
            "{'loss': 7.3002, 'learning_rate': 4.941047468578301e-05, 'epoch': 0.01}\n",
            "  1% 1500/127221 [06:15<11:29:38,  3.04it/s][INFO|trainer.py:1885] 2021-05-16 08:26:53,647 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-1500\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 08:26:53,654 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-1500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 08:26:54,433 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-1500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 08:26:54,439 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-1500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 08:26:54,461 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-1500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 08:26:56,458 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-500] due to args.save_total_limit\n",
            "{'loss': 7.2772, 'learning_rate': 4.921396624771068e-05, 'epoch': 0.02}\n",
            "  2% 2000/127221 [09:17<10:39:58,  3.26it/s][INFO|trainer.py:1885] 2021-05-16 08:29:56,326 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-2000\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 08:29:56,333 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-2000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 08:29:57,061 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-2000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 08:29:57,067 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-2000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 08:29:57,072 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-2000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 08:29:59,045 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-1000] due to args.save_total_limit\n",
            "{'loss': 7.1557, 'learning_rate': 4.901745780963835e-05, 'epoch': 0.02}\n",
            "  2% 2500/127221 [12:22<14:49:51,  2.34it/s][INFO|trainer.py:1885] 2021-05-16 08:33:01,446 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-2500\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 08:33:01,453 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-2500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 08:33:02,179 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-2500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 08:33:02,185 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-2500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 08:33:02,189 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-2500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 08:33:04,204 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-1500] due to args.save_total_limit\n",
            "{'loss': 7.1746, 'learning_rate': 4.882094937156602e-05, 'epoch': 0.02}\n",
            "  2% 3000/127221 [15:33<12:09:29,  2.84it/s][INFO|trainer.py:1885] 2021-05-16 08:36:12,506 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-3000\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 08:36:12,512 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-3000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 08:36:13,249 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-3000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 08:36:13,255 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-3000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 08:36:13,260 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-3000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 08:36:15,272 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-2000] due to args.save_total_limit\n",
            "{'loss': 7.1885, 'learning_rate': 4.8624440933493684e-05, 'epoch': 0.03}\n",
            "  3% 3500/127221 [18:39<12:20:47,  2.78it/s][INFO|trainer.py:1885] 2021-05-16 08:39:18,124 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-3500\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 08:39:18,130 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-3500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 08:39:18,867 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-3500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 08:39:18,874 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-3500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 08:39:18,880 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-3500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 08:39:21,995 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-2500] due to args.save_total_limit\n",
            "{'loss': 7.1239, 'learning_rate': 4.8427932495421356e-05, 'epoch': 0.03}\n",
            "  3% 4000/127221 [21:45<11:23:28,  3.00it/s][INFO|trainer.py:1885] 2021-05-16 08:42:24,067 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-4000\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 08:42:24,073 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-4000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 08:42:24,842 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-4000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 08:42:24,853 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-4000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 08:42:24,862 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-4000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 08:42:26,862 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-3000] due to args.save_total_limit\n",
            "{'loss': 7.1577, 'learning_rate': 4.823142405734902e-05, 'epoch': 0.04}\n",
            "  4% 4500/127221 [24:51<12:31:27,  2.72it/s][INFO|trainer.py:1885] 2021-05-16 08:45:29,910 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-4500\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 08:45:29,915 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-4500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 08:45:30,660 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-4500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 08:45:30,666 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-4500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 08:45:30,671 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-4500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 08:45:32,661 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-3500] due to args.save_total_limit\n",
            "{'loss': 7.1103, 'learning_rate': 4.8034915619276695e-05, 'epoch': 0.04}\n",
            "  4% 5000/127221 [27:59<14:10:16,  2.40it/s][INFO|trainer.py:1885] 2021-05-16 08:48:37,903 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-5000\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 08:48:37,909 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-5000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 08:48:38,640 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-5000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 08:48:38,646 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-5000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 08:48:38,650 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-5000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 08:48:40,493 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-4000] due to args.save_total_limit\n",
            "{'loss': 7.134, 'learning_rate': 4.783840718120437e-05, 'epoch': 0.04}\n",
            "  4% 5500/127221 [31:11<10:40:31,  3.17it/s][INFO|trainer.py:1885] 2021-05-16 08:51:49,627 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-5500\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 08:51:49,634 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-5500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 08:51:50,361 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-5500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 08:51:50,366 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-5500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 08:51:50,370 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-5500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 08:51:52,166 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-4500] due to args.save_total_limit\n",
            "{'loss': 7.1304, 'learning_rate': 4.764189874313203e-05, 'epoch': 0.05}\n",
            "  5% 6000/127221 [34:20<11:04:27,  3.04it/s][INFO|trainer.py:1885] 2021-05-16 08:54:59,348 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-6000\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 08:54:59,355 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-6000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 08:55:00,164 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-6000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 08:55:00,170 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-6000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 08:55:00,175 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-6000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 08:55:02,308 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-5000] due to args.save_total_limit\n",
            "{'loss': 7.1232, 'learning_rate': 4.74453903050597e-05, 'epoch': 0.05}\n",
            "  5% 6500/127221 [37:23<12:20:23,  2.72it/s][INFO|trainer.py:1885] 2021-05-16 08:58:02,501 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-6500\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 08:58:02,511 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-6500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 08:58:03,234 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-6500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 08:58:03,240 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-6500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 08:58:03,244 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-6500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 08:58:05,148 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-5500] due to args.save_total_limit\n",
            "{'loss': 7.1369, 'learning_rate': 4.724888186698737e-05, 'epoch': 0.06}\n",
            "  6% 7000/127221 [40:30<14:04:14,  2.37it/s][INFO|trainer.py:1885] 2021-05-16 09:01:09,187 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-7000\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 09:01:09,194 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-7000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 09:01:09,956 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-7000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 09:01:09,963 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-7000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 09:01:09,967 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-7000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 09:01:12,018 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-6000] due to args.save_total_limit\n",
            "{'loss': 7.1357, 'learning_rate': 4.7052373428915045e-05, 'epoch': 0.06}\n",
            "  6% 7500/127221 [43:43<12:02:51,  2.76it/s][INFO|trainer.py:1885] 2021-05-16 09:04:22,358 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-7500\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 09:04:22,365 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-7500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 09:04:23,096 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-7500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 09:04:23,101 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-7500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 09:04:23,124 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-7500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 09:04:25,236 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-6500] due to args.save_total_limit\n",
            "{'loss': 7.0512, 'learning_rate': 4.685586499084271e-05, 'epoch': 0.06}\n",
            "  6% 8000/127221 [46:58<13:24:29,  2.47it/s][INFO|trainer.py:1885] 2021-05-16 09:07:37,145 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-8000\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 09:07:37,151 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-8000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 09:07:37,908 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-8000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 09:07:37,914 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-8000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 09:07:37,918 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-8000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 09:07:40,195 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-7000] due to args.save_total_limit\n",
            "{'loss': 7.0952, 'learning_rate': 4.6659356552770376e-05, 'epoch': 0.07}\n",
            "  7% 8500/127221 [50:09<11:38:02,  2.83it/s][INFO|trainer.py:1885] 2021-05-16 09:10:47,590 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-8500\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 09:10:47,596 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-8500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 09:10:48,368 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-8500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 09:10:48,373 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-8500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 09:10:48,377 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-8500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 09:10:50,578 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-7500] due to args.save_total_limit\n",
            "{'loss': 7.0987, 'learning_rate': 4.646284811469805e-05, 'epoch': 0.07}\n",
            "  7% 9000/127221 [53:17<12:35:18,  2.61it/s][INFO|trainer.py:1885] 2021-05-16 09:13:56,516 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-9000\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 09:13:56,523 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-9000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 09:13:57,240 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-9000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 09:13:57,246 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-9000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 09:13:57,251 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-9000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 09:13:59,347 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-8000] due to args.save_total_limit\n",
            "{'loss': 7.1568, 'learning_rate': 4.6266339676625715e-05, 'epoch': 0.07}\n",
            "  7% 9500/127221 [56:23<12:13:34,  2.67it/s][INFO|trainer.py:1885] 2021-05-16 09:17:02,452 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-9500\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 09:17:02,458 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-9500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 09:17:03,214 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-9500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 09:17:03,219 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-9500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 09:17:03,223 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-9500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 09:17:05,455 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-8500] due to args.save_total_limit\n",
            "{'loss': 7.0721, 'learning_rate': 4.606983123855339e-05, 'epoch': 0.08}\n",
            "  8% 10000/127221 [59:35<10:57:09,  2.97it/s][INFO|trainer.py:1885] 2021-05-16 09:20:13,530 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-10000\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 09:20:13,536 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-10000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 09:20:14,233 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-10000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 09:20:14,239 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-10000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 09:20:14,244 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-10000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 09:20:17,329 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-9000] due to args.save_total_limit\n",
            "{'loss': 7.0999, 'learning_rate': 4.5873322800481053e-05, 'epoch': 0.08}\n",
            "  8% 10500/127221 [1:02:40<10:49:31,  3.00it/s][INFO|trainer.py:1885] 2021-05-16 09:23:18,713 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-10500\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 09:23:18,720 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-10500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 09:23:19,472 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-10500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 09:23:19,477 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-10500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 09:23:19,482 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-10500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 09:23:22,684 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-9500] due to args.save_total_limit\n",
            "{'loss': 7.1242, 'learning_rate': 4.5676814362408726e-05, 'epoch': 0.09}\n",
            "  9% 11000/127221 [1:05:51<10:56:48,  2.95it/s][INFO|trainer.py:1885] 2021-05-16 09:26:30,520 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-11000\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 09:26:30,527 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-11000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 09:26:31,275 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-11000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 09:26:31,281 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-11000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 09:26:31,286 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-11000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 09:26:33,362 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-10000] due to args.save_total_limit\n",
            "{'loss': 7.1048, 'learning_rate': 4.548030592433639e-05, 'epoch': 0.09}\n",
            "  9% 11500/127221 [1:08:56<14:54:54,  2.16it/s][INFO|trainer.py:1885] 2021-05-16 09:29:34,720 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-11500\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 09:29:34,728 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-11500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 09:29:35,459 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-11500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 09:29:35,464 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-11500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 09:29:35,469 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-11500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 09:29:37,504 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-10500] due to args.save_total_limit\n",
            "{'loss': 7.0685, 'learning_rate': 4.5283797486264065e-05, 'epoch': 0.09}\n",
            "  9% 12000/127221 [1:12:09<11:51:28,  2.70it/s][INFO|trainer.py:1885] 2021-05-16 09:32:47,770 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-12000\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 09:32:47,777 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-12000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 09:32:48,488 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-12000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 09:32:48,494 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-12000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 09:32:48,498 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-12000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 09:32:50,550 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-11000] due to args.save_total_limit\n",
            "{'loss': 7.0783, 'learning_rate': 4.508728904819173e-05, 'epoch': 0.1}\n",
            " 10% 12500/127221 [1:15:20<10:52:39,  2.93it/s][INFO|trainer.py:1885] 2021-05-16 09:35:58,812 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-12500\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 09:35:58,818 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-12500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 09:35:59,570 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-12500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 09:35:59,636 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-12500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 09:35:59,640 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-12500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 09:36:01,643 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-11500] due to args.save_total_limit\n",
            "{'loss': 7.1352, 'learning_rate': 4.4890780610119396e-05, 'epoch': 0.1}\n",
            " 10% 13000/127221 [1:18:31<11:38:04,  2.73it/s][INFO|trainer.py:1885] 2021-05-16 09:39:09,712 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-13000\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 09:39:09,718 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-13000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 09:39:10,440 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-13000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 09:39:10,447 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-13000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 09:39:10,452 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-13000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 09:39:12,543 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-12000] due to args.save_total_limit\n",
            "{'loss': 7.0947, 'learning_rate': 4.469427217204707e-05, 'epoch': 0.11}\n",
            " 11% 13500/127221 [1:21:37<11:04:57,  2.85it/s][INFO|trainer.py:1885] 2021-05-16 09:42:16,138 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-13500\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 09:42:16,144 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-13500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 09:42:16,878 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-13500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 09:42:16,884 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-13500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 09:42:16,889 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-13500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 09:42:18,947 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-12500] due to args.save_total_limit\n",
            "{'loss': 7.1112, 'learning_rate': 4.449776373397474e-05, 'epoch': 0.11}\n",
            " 11% 14000/127221 [1:24:44<9:47:08,  3.21it/s][INFO|trainer.py:1885] 2021-05-16 09:45:23,151 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-14000\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 09:45:23,156 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-14000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 09:45:23,875 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-14000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 09:45:23,880 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-14000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 09:45:23,885 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-14000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 09:45:25,918 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-13000] due to args.save_total_limit\n",
            "{'loss': 7.0709, 'learning_rate': 4.430125529590241e-05, 'epoch': 0.11}\n",
            " 11% 14500/127221 [1:27:51<12:49:17,  2.44it/s][INFO|trainer.py:1885] 2021-05-16 09:48:29,554 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-14500\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 09:48:29,575 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-14500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 09:48:30,313 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-14500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 09:48:30,319 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-14500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 09:48:30,324 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-14500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 09:48:32,446 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-13500] due to args.save_total_limit\n",
            "{'loss': 7.1292, 'learning_rate': 4.4104746857830073e-05, 'epoch': 0.12}\n",
            " 12% 15000/127221 [1:30:57<11:37:00,  2.68it/s][INFO|trainer.py:1885] 2021-05-16 09:51:35,687 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-15000\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 09:51:35,693 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-15000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 09:51:36,421 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-15000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 09:51:36,442 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-15000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 09:51:36,447 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-15000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 09:51:38,562 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-14000] due to args.save_total_limit\n",
            "{'loss': 7.0717, 'learning_rate': 4.3908238419757746e-05, 'epoch': 0.12}\n",
            " 12% 15500/127221 [1:34:01<10:58:57,  2.83it/s][INFO|trainer.py:1885] 2021-05-16 09:54:40,125 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-15500\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 09:54:40,138 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-15500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 09:54:40,871 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-15500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 09:54:40,893 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-15500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 09:54:40,897 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-15500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 09:54:42,815 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-14500] due to args.save_total_limit\n",
            "{'loss': 7.1452, 'learning_rate': 4.371172998168542e-05, 'epoch': 0.13}\n",
            " 13% 16000/127221 [1:37:10<12:00:28,  2.57it/s][INFO|trainer.py:1885] 2021-05-16 09:57:49,189 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-16000\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 09:57:49,197 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-16000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 09:57:49,914 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-16000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 09:57:49,933 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-16000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 09:57:49,937 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-16000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 09:57:51,942 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-15000] due to args.save_total_limit\n",
            "{'loss': 7.0793, 'learning_rate': 4.3515221543613085e-05, 'epoch': 0.13}\n",
            " 13% 16500/127221 [1:40:18<13:17:00,  2.32it/s][INFO|trainer.py:1885] 2021-05-16 10:00:56,866 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-16500\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 10:00:56,872 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-16500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 10:00:57,581 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-16500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 10:00:57,590 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-16500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 10:00:57,614 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-16500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 10:00:59,529 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-15500] due to args.save_total_limit\n",
            "{'loss': 7.1303, 'learning_rate': 4.331871310554075e-05, 'epoch': 0.13}\n",
            " 13% 17000/127221 [1:43:29<11:34:05,  2.65it/s][INFO|trainer.py:1885] 2021-05-16 10:04:08,199 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-17000\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 10:04:08,205 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-17000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 10:04:08,899 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-17000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 10:04:08,905 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-17000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 10:04:08,910 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-17000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 10:04:10,767 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-16000] due to args.save_total_limit\n",
            "{'loss': 7.0957, 'learning_rate': 4.312220466746842e-05, 'epoch': 0.14}\n",
            " 14% 17500/127221 [1:46:34<10:09:07,  3.00it/s][INFO|trainer.py:1885] 2021-05-16 10:07:13,310 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-17500\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 10:07:13,317 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-17500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 10:07:14,007 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-17500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 10:07:14,014 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-17500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 10:07:14,019 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-17500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 10:07:15,975 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-16500] due to args.save_total_limit\n",
            "{'loss': 7.1425, 'learning_rate': 4.292569622939609e-05, 'epoch': 0.14}\n",
            " 14% 18000/127221 [1:49:40<12:43:34,  2.38it/s][INFO|trainer.py:1885] 2021-05-16 10:10:19,246 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-18000\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 10:10:19,253 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-18000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 10:10:19,963 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-18000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 10:10:19,969 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-18000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 10:10:19,974 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-18000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 10:10:21,881 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-17000] due to args.save_total_limit\n",
            "{'loss': 7.1171, 'learning_rate': 4.272918779132376e-05, 'epoch': 0.15}\n",
            " 15% 18500/127221 [1:52:48<13:39:09,  2.21it/s][INFO|trainer.py:1885] 2021-05-16 10:13:27,140 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-18500\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 10:13:27,147 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-18500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 10:13:27,853 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-18500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 10:13:27,859 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-18500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 10:13:27,863 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-18500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 10:13:29,782 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-17500] due to args.save_total_limit\n",
            "{'loss': 7.0715, 'learning_rate': 4.2532679353251434e-05, 'epoch': 0.15}\n",
            " 15% 19000/127221 [1:55:55<16:07:28,  1.86it/s][INFO|trainer.py:1885] 2021-05-16 10:16:34,007 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-19000\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 10:16:34,014 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-19000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 10:16:34,762 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-19000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 10:16:34,768 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-19000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 10:16:34,773 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-19000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 10:16:36,713 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-18000] due to args.save_total_limit\n",
            "{'loss': 7.08, 'learning_rate': 4.23361709151791e-05, 'epoch': 0.15}\n",
            " 15% 19500/127221 [1:59:08<9:30:41,  3.15it/s][INFO|trainer.py:1885] 2021-05-16 10:19:47,248 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-19500\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 10:19:47,254 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-19500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 10:19:47,965 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-19500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 10:19:47,970 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-19500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 10:19:47,974 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-19500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 10:19:51,024 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-18500] due to args.save_total_limit\n",
            "{'loss': 7.163, 'learning_rate': 4.2139662477106766e-05, 'epoch': 0.16}\n",
            " 16% 20000/127221 [2:02:16<9:46:35,  3.05it/s][INFO|trainer.py:1885] 2021-05-16 10:22:54,968 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-20000\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 10:22:54,976 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-20000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 10:22:55,796 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-20000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 10:22:55,802 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-20000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 10:22:55,806 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-20000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 10:22:58,826 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-19000] due to args.save_total_limit\n",
            "{'loss': 7.1714, 'learning_rate': 4.194315403903444e-05, 'epoch': 0.16}\n",
            " 16% 20500/127221 [2:05:24<10:02:14,  2.95it/s][INFO|trainer.py:1885] 2021-05-16 10:26:02,913 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-20500\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 10:26:02,919 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-20500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 10:26:03,635 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-20500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 10:26:03,640 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-20500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 10:26:03,644 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-20500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 10:26:06,566 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-19500] due to args.save_total_limit\n",
            "{'loss': 7.11, 'learning_rate': 4.174664560096211e-05, 'epoch': 0.17}\n",
            " 17% 21000/127221 [2:08:29<9:57:43,  2.96it/s][INFO|trainer.py:1885] 2021-05-16 10:29:08,162 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-21000\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 10:29:08,168 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-21000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 10:29:08,998 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-21000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 10:29:09,004 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-21000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 10:29:09,010 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-21000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 10:29:11,017 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-20000] due to args.save_total_limit\n",
            "{'loss': 7.1453, 'learning_rate': 4.155013716288978e-05, 'epoch': 0.17}\n",
            " 17% 21500/127221 [2:11:38<10:09:10,  2.89it/s][INFO|trainer.py:1885] 2021-05-16 10:32:17,273 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-21500\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 10:32:17,280 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-21500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 10:32:18,025 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-21500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 10:32:18,032 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-21500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 10:32:18,036 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-21500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 10:32:20,328 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-20500] due to args.save_total_limit\n",
            "{'loss': 6.9898, 'learning_rate': 4.135362872481744e-05, 'epoch': 0.17}\n",
            " 17% 22000/127221 [2:14:50<11:07:37,  2.63it/s][INFO|trainer.py:1885] 2021-05-16 10:35:28,780 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-22000\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 10:35:28,786 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-22000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 10:35:29,553 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-22000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 10:35:29,559 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-22000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 10:35:29,563 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-22000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 10:35:31,713 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-21000] due to args.save_total_limit\n",
            "{'loss': 7.1435, 'learning_rate': 4.1157120286745116e-05, 'epoch': 0.18}\n",
            " 18% 22500/127221 [2:18:00<9:48:25,  2.97it/s][INFO|trainer.py:1885] 2021-05-16 10:38:38,849 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-22500\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 10:38:38,856 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-22500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 10:38:39,593 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-22500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 10:38:39,599 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-22500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 10:38:39,603 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-22500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 10:38:41,655 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-21500] due to args.save_total_limit\n",
            "{'loss': 7.1384, 'learning_rate': 4.096061184867279e-05, 'epoch': 0.18}\n",
            " 18% 23000/127221 [2:21:08<9:36:07,  3.02it/s][INFO|trainer.py:1885] 2021-05-16 10:41:47,322 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-23000\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 10:41:47,328 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-23000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 10:41:48,072 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-23000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 10:41:48,078 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-23000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 10:41:48,082 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-23000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 10:41:50,133 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-22000] due to args.save_total_limit\n",
            "{'loss': 7.1437, 'learning_rate': 4.0764103410600454e-05, 'epoch': 0.18}\n",
            " 18% 23500/127221 [2:24:19<12:33:33,  2.29it/s][INFO|trainer.py:1885] 2021-05-16 10:44:57,551 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-23500\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 10:44:57,560 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-23500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 10:44:58,278 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-23500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 10:44:58,283 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-23500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 10:44:58,288 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-23500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 10:45:00,332 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-22500] due to args.save_total_limit\n",
            "{'loss': 7.0879, 'learning_rate': 4.056759497252812e-05, 'epoch': 0.19}\n",
            " 19% 24000/127221 [2:27:27<9:51:02,  2.91it/s][INFO|trainer.py:1885] 2021-05-16 10:48:06,205 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-24000\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 10:48:06,212 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-24000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 10:48:07,059 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-24000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 10:48:07,065 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-24000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 10:48:07,069 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-24000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 10:48:09,165 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-23000] due to args.save_total_limit\n",
            "{'loss': 7.0535, 'learning_rate': 4.037108653445579e-05, 'epoch': 0.19}\n",
            " 19% 24500/127221 [2:30:34<8:47:07,  3.25it/s][INFO|trainer.py:1885] 2021-05-16 10:51:13,009 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-24500\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 10:51:13,015 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-24500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 10:51:13,751 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-24500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 10:51:13,756 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-24500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 10:51:13,760 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-24500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 10:51:15,686 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-23500] due to args.save_total_limit\n",
            "{'loss': 7.1602, 'learning_rate': 4.017457809638346e-05, 'epoch': 0.2}\n",
            " 20% 25000/127221 [2:33:43<9:37:24,  2.95it/s][INFO|trainer.py:1885] 2021-05-16 10:54:21,539 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-25000\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 10:54:21,545 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-25000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 10:54:22,355 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-25000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 10:54:22,361 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-25000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 10:54:22,365 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-25000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 10:54:24,276 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-24000] due to args.save_total_limit\n",
            "{'loss': 7.128, 'learning_rate': 3.997806965831113e-05, 'epoch': 0.2}\n",
            " 20% 25500/127221 [2:36:53<13:30:29,  2.09it/s][INFO|trainer.py:1885] 2021-05-16 10:57:31,820 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-25500\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 10:57:31,827 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-25500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 10:57:32,666 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-25500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 10:57:32,672 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-25500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 10:57:32,676 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-25500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 10:57:34,589 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-24500] due to args.save_total_limit\n",
            "{'loss': 7.1246, 'learning_rate': 3.97815612202388e-05, 'epoch': 0.2}\n",
            " 20% 26000/127221 [2:39:57<9:28:12,  2.97it/s][INFO|trainer.py:1885] 2021-05-16 11:00:36,341 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-26000\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 11:00:36,364 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-26000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 11:00:37,206 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-26000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 11:00:37,211 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-26000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 11:00:37,216 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-26000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 11:00:39,125 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-25000] due to args.save_total_limit\n",
            "{'loss': 7.0865, 'learning_rate': 3.958505278216647e-05, 'epoch': 0.21}\n",
            " 21% 26500/127221 [2:43:05<8:47:32,  3.18it/s][INFO|trainer.py:1885] 2021-05-16 11:03:44,251 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-26500\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 11:03:44,259 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-26500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 11:03:45,047 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-26500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 11:03:45,054 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-26500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 11:03:45,059 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-26500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 11:03:47,017 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-25500] due to args.save_total_limit\n",
            "{'loss': 7.108, 'learning_rate': 3.9388544344094136e-05, 'epoch': 0.21}\n",
            " 21% 27000/127221 [2:46:13<10:52:58,  2.56it/s][INFO|trainer.py:1885] 2021-05-16 11:06:51,914 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-27000\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 11:06:51,920 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-27000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 11:06:52,657 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-27000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 11:06:52,663 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-27000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 11:06:52,668 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-27000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 11:06:54,758 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-26000] due to args.save_total_limit\n",
            "{'loss': 7.1916, 'learning_rate': 3.919203590602181e-05, 'epoch': 0.22}\n",
            " 22% 27500/127221 [2:49:25<9:13:43,  3.00it/s][INFO|trainer.py:1885] 2021-05-16 11:10:03,868 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-27500\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 11:10:03,878 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-27500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 11:10:04,710 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-27500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 11:10:04,716 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-27500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 11:10:04,721 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-27500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 11:10:06,736 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-26500] due to args.save_total_limit\n",
            "{'loss': 7.0771, 'learning_rate': 3.8995527467949474e-05, 'epoch': 0.22}\n",
            " 22% 28000/127221 [2:52:33<9:46:28,  2.82it/s][INFO|trainer.py:1885] 2021-05-16 11:13:11,673 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-28000\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 11:13:11,679 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-28000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 11:13:12,416 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-28000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 11:13:12,440 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-28000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 11:13:12,444 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-28000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 11:13:14,493 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-27000] due to args.save_total_limit\n",
            "{'loss': 7.1583, 'learning_rate': 3.879901902987714e-05, 'epoch': 0.22}\n",
            " 22% 28500/127221 [2:55:42<10:37:52,  2.58it/s][INFO|trainer.py:1885] 2021-05-16 11:16:20,603 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-28500\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 11:16:20,610 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-28500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 11:16:21,356 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-28500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 11:16:21,363 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-28500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 11:16:21,384 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-28500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 11:16:23,487 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-27500] due to args.save_total_limit\n",
            "{'loss': 7.1224, 'learning_rate': 3.860251059180481e-05, 'epoch': 0.23}\n",
            " 23% 29000/127221 [2:58:46<9:26:38,  2.89it/s][INFO|trainer.py:1885] 2021-05-16 11:19:24,877 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-29000\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 11:19:24,885 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-29000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 11:19:25,774 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-29000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 11:19:25,779 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-29000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 11:19:25,783 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-29000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 11:19:27,959 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-28000] due to args.save_total_limit\n",
            "{'loss': 7.0915, 'learning_rate': 3.8406002153732485e-05, 'epoch': 0.23}\n",
            " 23% 29500/127221 [3:01:55<8:24:17,  3.23it/s][INFO|trainer.py:1885] 2021-05-16 11:22:34,503 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-29500\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 11:22:34,509 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-29500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 11:22:35,243 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-29500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 11:22:35,249 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-29500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 11:22:35,253 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-29500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 11:22:37,404 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-28500] due to args.save_total_limit\n",
            "{'loss': 7.0906, 'learning_rate': 3.820949371566015e-05, 'epoch': 0.24}\n",
            " 24% 30000/127221 [3:05:05<9:33:54,  2.82it/s][INFO|trainer.py:1885] 2021-05-16 11:25:43,593 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-30000\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 11:25:43,599 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-30000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 11:25:44,326 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-30000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 11:25:44,331 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-30000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 11:25:44,336 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-30000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 11:25:46,401 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-29000] due to args.save_total_limit\n",
            "{'loss': 7.0545, 'learning_rate': 3.801298527758782e-05, 'epoch': 0.24}\n",
            " 24% 30500/127221 [3:08:11<8:27:16,  3.18it/s][INFO|trainer.py:1885] 2021-05-16 11:28:49,974 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-30500\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 11:28:49,980 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-30500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 11:28:50,714 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-30500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 11:28:50,719 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-30500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 11:28:50,724 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-30500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 11:28:53,153 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-29500] due to args.save_total_limit\n",
            "{'loss': 7.1102, 'learning_rate': 3.781647683951549e-05, 'epoch': 0.24}\n",
            " 24% 31000/127221 [3:11:19<9:10:43,  2.91it/s][INFO|trainer.py:1885] 2021-05-16 11:31:57,860 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-31000\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 11:31:57,867 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-31000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 11:31:58,722 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-31000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 11:31:58,727 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-31000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 11:31:58,731 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-31000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 11:32:01,152 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-30000] due to args.save_total_limit\n",
            "{'loss': 7.1375, 'learning_rate': 3.761996840144316e-05, 'epoch': 0.25}\n",
            " 25% 31500/127221 [3:14:28<12:10:36,  2.18it/s][INFO|trainer.py:1885] 2021-05-16 11:35:06,521 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-31500\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 11:35:06,526 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-31500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 11:35:07,249 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-31500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 11:35:07,254 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-31500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 11:35:07,258 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-31500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 11:35:10,500 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-30500] due to args.save_total_limit\n",
            "{'loss': 7.1286, 'learning_rate': 3.742345996337083e-05, 'epoch': 0.25}\n",
            " 25% 32000/127221 [3:17:41<9:58:54,  2.65it/s][INFO|trainer.py:1885] 2021-05-16 11:38:19,996 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-32000\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 11:38:20,004 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-32000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 11:38:20,745 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-32000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 11:38:20,753 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-32000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 11:38:20,762 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-32000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 11:38:22,928 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-31000] due to args.save_total_limit\n",
            "{'loss': 7.082, 'learning_rate': 3.72269515252985e-05, 'epoch': 0.26}\n",
            " 26% 32500/127221 [3:20:45<8:34:55,  3.07it/s][INFO|trainer.py:1885] 2021-05-16 11:41:24,112 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-32500\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 11:41:24,119 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-32500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 11:41:24,908 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-32500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 11:41:24,916 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-32500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 11:41:24,920 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-32500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 11:41:26,970 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-31500] due to args.save_total_limit\n",
            "{'loss': 7.1603, 'learning_rate': 3.703044308722617e-05, 'epoch': 0.26}\n",
            " 26% 33000/127221 [3:23:58<8:46:58,  2.98it/s][INFO|trainer.py:1885] 2021-05-16 11:44:37,231 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-33000\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 11:44:37,237 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-33000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 11:44:37,942 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-33000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 11:44:37,947 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-33000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 11:44:37,952 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-33000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 11:44:39,872 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-32000] due to args.save_total_limit\n",
            "{'loss': 7.0919, 'learning_rate': 3.683393464915383e-05, 'epoch': 0.26}\n",
            " 26% 33500/127221 [3:27:09<8:52:35,  2.93it/s][INFO|trainer.py:1885] 2021-05-16 11:47:47,755 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-33500\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 11:47:47,761 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-33500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 11:47:48,480 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-33500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 11:47:48,485 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-33500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 11:47:48,490 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-33500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 11:47:50,478 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-32500] due to args.save_total_limit\n",
            "{'loss': 7.0764, 'learning_rate': 3.6637426211081505e-05, 'epoch': 0.27}\n",
            " 27% 34000/127221 [3:30:14<8:43:32,  2.97it/s][INFO|trainer.py:1885] 2021-05-16 11:50:53,323 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-34000\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 11:50:53,329 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-34000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 11:50:54,175 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-34000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 11:50:54,180 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-34000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 11:50:54,185 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-34000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 11:50:56,086 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-33000] due to args.save_total_limit\n",
            "{'loss': 7.0541, 'learning_rate': 3.644091777300918e-05, 'epoch': 0.27}\n",
            " 27% 34500/127221 [3:33:21<8:04:37,  3.19it/s][INFO|trainer.py:1885] 2021-05-16 11:54:00,387 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-34500\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 11:54:00,394 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-34500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 11:54:01,142 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-34500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 11:54:01,148 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-34500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 11:54:01,152 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-34500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 11:54:03,066 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-33500] due to args.save_total_limit\n",
            "{'loss': 7.1035, 'learning_rate': 3.6244409334936844e-05, 'epoch': 0.28}\n",
            " 28% 35000/127221 [3:36:33<8:50:19,  2.90it/s][INFO|trainer.py:1885] 2021-05-16 11:57:11,561 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-35000\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 11:57:11,569 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-35000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 11:57:12,467 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-35000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 11:57:12,473 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-35000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 11:57:12,478 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-35000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 11:57:14,409 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-34000] due to args.save_total_limit\n",
            "{'loss': 7.1269, 'learning_rate': 3.604790089686451e-05, 'epoch': 0.28}\n",
            " 28% 35500/127221 [3:39:42<8:56:32,  2.85it/s][INFO|trainer.py:1885] 2021-05-16 12:00:20,883 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-35500\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 12:00:20,889 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-35500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 12:00:21,617 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-35500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 12:00:21,624 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-35500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 12:00:21,629 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-35500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 12:00:23,573 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-34500] due to args.save_total_limit\n",
            "{'loss': 7.1438, 'learning_rate': 3.585139245879218e-05, 'epoch': 0.28}\n",
            " 28% 36000/127221 [3:42:48<9:01:33,  2.81it/s][INFO|trainer.py:1885] 2021-05-16 12:03:26,617 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-36000\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 12:03:26,623 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-36000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 12:03:27,404 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-36000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 12:03:27,410 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-36000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 12:03:27,415 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-36000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 12:03:29,545 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-35000] due to args.save_total_limit\n",
            "{'loss': 7.0597, 'learning_rate': 3.5654884020719855e-05, 'epoch': 0.29}\n",
            " 29% 36500/127221 [3:45:54<10:21:21,  2.43it/s][INFO|trainer.py:1885] 2021-05-16 12:06:33,090 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-36500\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 12:06:33,096 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-36500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 12:06:33,840 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-36500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 12:06:33,865 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-36500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 12:06:33,870 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-36500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 12:06:35,782 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-35500] due to args.save_total_limit\n",
            "{'loss': 7.0577, 'learning_rate': 3.545837558264752e-05, 'epoch': 0.29}\n",
            " 29% 37000/127221 [3:49:02<8:17:05,  3.03it/s][INFO|trainer.py:1885] 2021-05-16 12:09:40,634 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-37000\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 12:09:40,640 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-37000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 12:09:41,354 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-37000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 12:09:41,360 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-37000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 12:09:41,393 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-37000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 12:09:43,337 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-36000] due to args.save_total_limit\n",
            "{'loss': 7.1045, 'learning_rate': 3.526186714457519e-05, 'epoch': 0.29}\n",
            " 29% 37500/127221 [3:52:13<10:28:18,  2.38it/s][INFO|trainer.py:1885] 2021-05-16 12:12:52,146 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-37500\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 12:12:52,152 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-37500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 12:12:52,969 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-37500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 12:12:52,974 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-37500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 12:12:52,979 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-37500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 12:12:55,057 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-36500] due to args.save_total_limit\n",
            "{'loss': 7.1118, 'learning_rate': 3.506535870650286e-05, 'epoch': 0.3}\n",
            " 30% 38000/127221 [3:55:17<11:48:26,  2.10it/s][INFO|trainer.py:1885] 2021-05-16 12:15:56,252 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-38000\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 12:15:56,259 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-38000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 12:15:56,997 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-38000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 12:15:57,003 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-38000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 12:15:57,008 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-38000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 12:15:59,076 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-37000] due to args.save_total_limit\n",
            "{'loss': 7.1016, 'learning_rate': 3.486885026843053e-05, 'epoch': 0.3}\n",
            " 30% 38500/127221 [3:58:22<7:29:45,  3.29it/s][INFO|trainer.py:1885] 2021-05-16 12:19:01,293 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-38500\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 12:19:01,299 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-38500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 12:19:02,011 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-38500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 12:19:02,020 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-38500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 12:19:02,026 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-38500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 12:19:04,299 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-37500] due to args.save_total_limit\n",
            "{'loss': 7.0865, 'learning_rate': 3.46723418303582e-05, 'epoch': 0.31}\n",
            " 31% 39000/127221 [4:01:30<8:30:59,  2.88it/s][INFO|trainer.py:1885] 2021-05-16 12:22:09,203 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-39000\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 12:22:09,210 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-39000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 12:22:10,114 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-39000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 12:22:10,122 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-39000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 12:22:10,129 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-39000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 12:22:12,362 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-38000] due to args.save_total_limit\n",
            "{'loss': 7.0291, 'learning_rate': 3.4475833392285864e-05, 'epoch': 0.31}\n",
            " 31% 39500/127221 [4:04:38<8:06:37,  3.00it/s][INFO|trainer.py:1885] 2021-05-16 12:25:16,574 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-39500\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 12:25:16,581 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-39500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 12:25:17,347 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-39500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 12:25:17,352 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-39500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 12:25:17,357 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-39500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 12:25:20,425 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-38500] due to args.save_total_limit\n",
            "{'loss': 7.1638, 'learning_rate': 3.4279324954213536e-05, 'epoch': 0.31}\n",
            " 31% 40000/127221 [4:07:51<7:59:36,  3.03it/s][INFO|trainer.py:1885] 2021-05-16 12:28:30,141 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-40000\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 12:28:30,147 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-40000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 12:28:30,894 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-40000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 12:28:30,900 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-40000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 12:28:30,904 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-40000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 12:28:34,150 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-39000] due to args.save_total_limit\n",
            "{'loss': 7.0609, 'learning_rate': 3.40828165161412e-05, 'epoch': 0.32}\n",
            " 32% 40500/127221 [4:11:03<7:49:13,  3.08it/s][INFO|trainer.py:1885] 2021-05-16 12:31:41,546 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-40500\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 12:31:41,552 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-40500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 12:31:42,327 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-40500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 12:31:42,333 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-40500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 12:31:42,338 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-40500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 12:31:44,542 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-39500] due to args.save_total_limit\n",
            "{'loss': 7.1039, 'learning_rate': 3.3886308078068875e-05, 'epoch': 0.32}\n",
            " 32% 41000/127221 [4:14:10<7:49:48,  3.06it/s][INFO|trainer.py:1885] 2021-05-16 12:34:48,552 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-41000\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 12:34:48,557 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-41000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 12:34:49,281 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-41000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 12:34:49,287 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-41000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 12:34:49,291 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-41000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 12:34:51,289 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-40000] due to args.save_total_limit\n",
            "{'loss': 7.0919, 'learning_rate': 3.368979963999655e-05, 'epoch': 0.33}\n",
            " 33% 41500/127221 [4:17:20<9:50:50,  2.42it/s][INFO|trainer.py:1885] 2021-05-16 12:37:58,650 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-41500\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 12:37:58,656 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-41500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 12:37:59,386 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-41500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 12:37:59,392 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-41500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 12:37:59,397 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-41500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 12:38:01,495 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-40500] due to args.save_total_limit\n",
            "{'loss': 7.1036, 'learning_rate': 3.349329120192421e-05, 'epoch': 0.33}\n",
            " 33% 42000/127221 [4:20:27<7:28:36,  3.17it/s][INFO|trainer.py:1885] 2021-05-16 12:41:06,487 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-42000\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 12:41:06,495 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-42000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 12:41:07,229 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-42000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 12:41:07,234 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-42000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 12:41:07,239 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-42000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 12:41:09,449 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-41000] due to args.save_total_limit\n",
            "{'loss': 7.1345, 'learning_rate': 3.329678276385188e-05, 'epoch': 0.33}\n",
            " 33% 42500/127221 [4:23:35<8:54:57,  2.64it/s][INFO|trainer.py:1885] 2021-05-16 12:44:13,973 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-42500\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 12:44:13,980 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-42500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 12:44:14,704 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-42500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 12:44:14,710 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-42500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 12:44:14,714 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-42500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 12:44:16,946 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-41500] due to args.save_total_limit\n",
            "{'loss': 7.1383, 'learning_rate': 3.310027432577955e-05, 'epoch': 0.34}\n",
            " 34% 43000/127221 [4:26:42<10:07:02,  2.31it/s][INFO|trainer.py:1885] 2021-05-16 12:47:21,313 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-43000\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 12:47:21,320 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-43000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 12:47:22,066 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-43000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 12:47:22,071 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-43000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 12:47:22,076 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-43000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 12:47:24,871 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-42000] due to args.save_total_limit\n",
            "{'loss': 7.1307, 'learning_rate': 3.2903765887707225e-05, 'epoch': 0.34}\n",
            " 34% 43500/127221 [4:29:53<8:22:12,  2.78it/s][INFO|trainer.py:1885] 2021-05-16 12:50:32,233 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-43500\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 12:50:32,239 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-43500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 12:50:33,132 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-43500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 12:50:33,137 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-43500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 12:50:33,142 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-43500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 12:50:35,117 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-42500] due to args.save_total_limit\n",
            "{'loss': 7.0601, 'learning_rate': 3.270725744963489e-05, 'epoch': 0.35}\n",
            " 35% 44000/127221 [4:33:01<8:20:46,  2.77it/s][INFO|trainer.py:1885] 2021-05-16 12:53:40,098 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-44000\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 12:53:40,108 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-44000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 12:53:41,058 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-44000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 12:53:41,063 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-44000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 12:53:41,067 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-44000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 12:53:43,055 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-43000] due to args.save_total_limit\n",
            "{'loss': 7.0658, 'learning_rate': 3.2510749011562556e-05, 'epoch': 0.35}\n",
            " 35% 44500/127221 [4:36:10<10:32:10,  2.18it/s][INFO|trainer.py:1885] 2021-05-16 12:56:48,555 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-44500\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 12:56:48,562 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-44500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 12:56:49,433 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-44500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 12:56:49,439 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-44500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 12:56:49,443 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-44500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 12:56:51,463 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-43500] due to args.save_total_limit\n",
            "{'loss': 7.1802, 'learning_rate': 3.231424057349023e-05, 'epoch': 0.35}\n",
            " 35% 45000/127221 [4:39:19<9:14:53,  2.47it/s][INFO|trainer.py:1885] 2021-05-16 12:59:58,518 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-45000\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 12:59:58,524 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-45000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 12:59:59,273 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-45000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 12:59:59,299 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-45000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 12:59:59,303 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-45000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 13:00:01,205 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-44000] due to args.save_total_limit\n",
            "{'loss': 7.0471, 'learning_rate': 3.2117732135417895e-05, 'epoch': 0.36}\n",
            " 36% 45500/127221 [4:42:29<7:25:58,  3.05it/s][INFO|trainer.py:1885] 2021-05-16 13:03:08,411 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-45500\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 13:03:08,417 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-45500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 13:03:09,163 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-45500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 13:03:09,169 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-45500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 13:03:09,174 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-45500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 13:03:11,217 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-44500] due to args.save_total_limit\n",
            "{'loss': 7.1309, 'learning_rate': 3.192122369734557e-05, 'epoch': 0.36}\n",
            " 36% 46000/127221 [4:45:38<7:15:52,  3.11it/s][INFO|trainer.py:1885] 2021-05-16 13:06:16,527 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-46000\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 13:06:16,534 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-46000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 13:06:17,268 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-46000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 13:06:17,274 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-46000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 13:06:17,279 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-46000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 13:06:19,712 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-45000] due to args.save_total_limit\n",
            "{'loss': 7.0891, 'learning_rate': 3.1724715259273233e-05, 'epoch': 0.37}\n",
            " 37% 46500/127221 [4:48:48<7:56:13,  2.82it/s][INFO|trainer.py:1885] 2021-05-16 13:09:26,900 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-46500\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 13:09:26,906 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-46500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 13:09:27,631 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-46500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 13:09:27,636 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-46500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 13:09:27,641 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-46500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 13:09:30,126 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-45500] due to args.save_total_limit\n",
            "{'loss': 7.1337, 'learning_rate': 3.1528206821200906e-05, 'epoch': 0.37}\n",
            " 37% 47000/127221 [4:51:58<8:52:29,  2.51it/s][INFO|trainer.py:1885] 2021-05-16 13:12:37,092 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-47000\n",
            "[INFO|configuration_utils.py:351] 2021-05-16 13:12:37,099 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-47000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-16 13:12:38,082 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-47000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-16 13:12:38,088 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-47000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-16 13:12:38,093 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-47000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-16 13:12:40,745 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-46000] due to args.save_total_limit\n",
            " 37% 47431/127221 [4:54:40<7:47:50,  2.84it/s]"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6R8TLMISRaPA",
        "outputId": "5e02b77a-5668-4e9a-b380-11685952445a"
      },
      "source": [
        "from transformers import AlbertTokenizerFast, AlbertModel\n",
        "\n",
        "tokenizer = AlbertTokenizerFast.from_pretrained('/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/checkpoint-1000')\n",
        "model = AlbertModel.from_pretrained('/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/checkpoint-1000')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/checkpoint-1000 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight']\n",
            "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of AlbertModel were not initialized from the model checkpoint at /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/checkpoint-1000 and are newly initialized: ['albert.pooler.weight', 'albert.pooler.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4twnD81qaRR"
      },
      "source": [
        "tokenizer.save_pretrained('/content/drive/MyDrive/Tokenizer_train/albert-xlarge-kor')\n",
        "model.save_pretrained('/content/drive/MyDrive/Tokenizer_train/albert-xlarge-kor')"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4ghZhMYuO38"
      },
      "source": [
        "# 기존 albert-kor-base 사용 \n",
        "\n",
        "https://huggingface.co/kykim/albert-kor-base"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245,
          "referenced_widgets": [
            "44757a3a7afe4e4eb95e95f11e94baed",
            "fee20f84e6204e7bad616f22de2b85ba",
            "673b5f7827d7437f92e1f3240161aa5a",
            "9170fc91ba304ad8b6a4c2d2ccb31355",
            "e561f9e07090434cb77090aeddc4789a",
            "de0381976d7c4fc68b06665cbae12834",
            "6d81c8e6bbaa47139a9161791c49e526",
            "db5a8e04fa0949de9cfc76eb7494f9ca",
            "f6fb86e6f6914670968ad70d09ab5852",
            "bd03d7a79e02416ebcdb5f6193414fe3",
            "b656b044f5964b18a69be602cf391abf",
            "3bb580a2181641c48ca8a14da6e7c2a0",
            "4ceff003d619437d8ecae1bf080db110",
            "c807b711d5e84e2fb60db9716f57a004",
            "6c478a8fd1214967a49ba63b7a5f5c2d",
            "2a15c90485f04cdab63dcd9704bc68d7",
            "cc2d8144b76b471a824dddd49b6c5889",
            "51dca6c50af5412994758f1c854a78de",
            "2448a43b9a3645cebcabe6a5c21b0d6f",
            "0c511bccc20b4815a1ff1a54d40f94da",
            "508cd332718045e290ac9a43cf950af3",
            "42119f7a708b411b93a877db42feae49",
            "5dad0c302f7d4533ada1482eebc97809",
            "73e88e9c09f44e86a9ae023ec7f6ed21"
          ]
        },
        "id": "OCTh5ViFuWUs",
        "outputId": "f56a2dd5-3949-42b5-b5c2-64f8ca9009f5"
      },
      "source": [
        "from transformers import BertTokenizerFast, AlbertModel\n",
        "\n",
        "tokenizer = BertTokenizerFast.from_pretrained('kykim/albert-kor-base')\n",
        "model = AlbertModel.from_pretrained('kykim/albert-kor-base')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "44757a3a7afe4e4eb95e95f11e94baed",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=344259.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f6fb86e6f6914670968ad70d09ab5852",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=684.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cc2d8144b76b471a824dddd49b6c5889",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=53325832.0, style=ProgressStyle(descrip…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at kykim/albert-kor-base were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'sop_classifier.classifier.bias', 'sop_classifier.classifier.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight']\n",
            "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfSy-90kvAqH",
        "outputId": "a771a469-dcab-4393-9de7-45c47785d1c7"
      },
      "source": [
        "tokenizer.save_pretrained('/content/drive/MyDrive/Tokenizer_train/albert-kor-base')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/Tokenizer_train/albert-kor-base/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/Tokenizer_train/albert-kor-base/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/Tokenizer_train/albert-kor-base/vocab.txt',\n",
              " '/content/drive/MyDrive/Tokenizer_train/albert-kor-base/added_tokens.json',\n",
              " '/content/drive/MyDrive/Tokenizer_train/albert-kor-base/tokenizer.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEb3ISnMvXnA"
      },
      "source": [
        "model.save_pretrained('/content/drive/MyDrive/Tokenizer_train/albert-kor-base')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296,
          "referenced_widgets": [
            "900992c9731b494a836b090acd817255",
            "4a609fde633448e4abedf96a3ca46514",
            "aee229bbb2cb4014a2aaaa7de42ecadc",
            "804ff19fd0c34b33a64d3db138eee97b",
            "628451d485bb41b687e0388a3a0733d9",
            "04f37ec1ff4a455b84732cde635a7a32",
            "1210ff77115442c8adea202c2384ad7a",
            "daa84cb7428547dda7903cbeb1648392",
            "20e74878ee6d4169838f5ebcd9bf9105",
            "2b8569172ab848749ef1d1fbaaf12666",
            "4922360857c941f8ba39fa6b714f368d",
            "b38b917be9b24e0088cc29f43318e8e5",
            "34b650cda7e6401281b3d38a515a9bbc",
            "0c0903beadef4ee9980a15bc0fac1125",
            "2ff724f00db740828f6f3167b7586b6b",
            "d558b31da7c84edc86e1b08c0f9c44dd",
            "733866c92eec4255b38e995e9b7ba0b8",
            "13052ef80a744def9c3537a2ac0750c3",
            "5d3816317c6843efbd58e22c6ffea1e2",
            "b7f8684692d740c2a684aba285114e1b",
            "89cf8cf5c0554bb886a5749336ca8ee7",
            "dc2ea1c1d9a448cd84cd1af0c731e4fe",
            "c66d419b769649b18f80dba829755316",
            "aa3bc3bdaf5d48d5b678ca4cf01930cb",
            "1e942b3ef56f449299c8d22c98b5443d",
            "c0282cbb9c9c42ec910ecfe70e6bb49c",
            "978efca384e942b4aec18babdf2e9b6a",
            "6502ffd64d1940509266fa7d5890b669",
            "86da887a18b44fa1a450129f2c8986d7",
            "132e520423e94ef39e88cb0b872a006e",
            "3a5049dee38641f08f01c4c58e4a09c1",
            "8f07e69a62694d099a85e21b39328ecf"
          ]
        },
        "id": "d1eykQOfZPnN",
        "outputId": "6b7b0329-78bb-436c-ef30-bb63c17c331f"
      },
      "source": [
        "from transformers import AlbertTokenizer, AlbertModel\n",
        "\n",
        "tokenizer = AlbertTokenizer.from_pretrained('albert-xlarge-v2')\n",
        "model = AlbertModel.from_pretrained('albert-xlarge-v2')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "900992c9731b494a836b090acd817255",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=760289.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "20e74878ee6d4169838f5ebcd9bf9105",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1312669.0, style=ProgressStyle(descript…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "733866c92eec4255b38e995e9b7ba0b8",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=685.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1e942b3ef56f449299c8d22c98b5443d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=236197176.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at albert-xlarge-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight']\n",
            "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeZAA14kZpLO"
      },
      "source": [
        "tokenizer.save_pretrained('/content/drive/MyDrive/Tokenizer_train/albert-kor-xlarge')\n",
        "model.save_pretrained('/content/drive/MyDrive/Tokenizer_train/albert-kor-xlarge')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13JDIF67_hWF"
      },
      "source": [
        "# 여기서부터 새로운 model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elahPhO3vijH"
      },
      "source": [
        "from transformers import AlbertTokenizerFast, AlbertForQuestionAnswering\n",
        "\n",
        "tokenizer = AlbertTokenizerFast.from_pretrained('/content/drive/MyDrive/Tokenizer_train/albert-xlarge-kor')\n",
        "#model = AlbertForQuestionAnswering.from_pretrained('/content/drive/MyDrive/Tokenizer_train/albert-kor-base')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JiwgEyUiv3E1",
        "outputId": "4873ba0e-3e58-42ee-b8c1-15acb5d438fe"
      },
      "source": [
        "op = tokenizer(\"나는 오늘 학교에 간다.\", return_tensors=\"pt\")\n",
        "print(op)\n",
        "print(\"Tokens (str)      : {}\".format([tokenizer.convert_ids_to_tokens(s) for s in op['input_ids'].tolist()[0]]))\n",
        "print(\"Tokens (int)      : {}\".format(op['input_ids'].tolist()[0]))\n",
        "print(\"Tokens (attn_mask): {}\\n\".format(op['attention_mask'].tolist()[0]))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'input_ids': tensor([[    5,   168,   459,   397,   707,   473,  1823, 17709,     6]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
            "Tokens (str)      : ['[CLS]', '▁나', '는', '▁오늘', '▁학교', '에', '▁간다', '.', '[SEP]']\n",
            "Tokens (int)      : [5, 168, 459, 397, 707, 473, 1823, 17709, 6]\n",
            "Tokens (attn_mask): [1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SN_gSqSrJQAL"
      },
      "source": [
        "# KoQuAD1.0 데이터 Load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277,
          "referenced_widgets": [
            "7a1ea1419ff94665a1679ebf1ef6c2c4",
            "5fe568b1040646d3a2f792b785d90d96",
            "0ecfea61dacd4d049417134627dcffbe",
            "ccb6c5752492462aa46991e3c8f29018",
            "81bb36671bfc4b34b8c5608eb8ae3641",
            "47d7c7b046c44f43803e2adb86b20a4f",
            "a109e9b9abf84b3d84c504dcd5521535",
            "0798de252e4747d683310debeffc81fc",
            "5deb1925808640979a566c5fa32eb6f2",
            "9f7eeb0e11504da4b2e23ef6744f6b17",
            "012d08bda7274ea3a5730d16ba66d829",
            "a1dfff15cf764d78a76d8888dccf3fc0",
            "a09b66730f654338b72598f897c75b2b",
            "cb1d69197f7d416b906735615f37388c",
            "7bcb67d12c87446d8889831d65ed6584",
            "7fdaa1bd04354070942b989cd999111a",
            "36f10e2a42d4499aab746161965d745e",
            "02c40ae99caf4187ac0e65579c9a14f1",
            "1febe86e12724dd3bf54924a9b1a9df9",
            "5803670df0b54a9b93270e4a28ae41e6",
            "efe766d9df5d4b559dee67bcf1f50c8c",
            "1010972e9ac348aba93396e550239712",
            "632141fc749b4320a5be37dccb2e64b0",
            "84b9036778044484bb19f272c7be3704",
            "4097582128074b04b685b0e0dcf81571",
            "4394c7a212e848d6b43fae8e794e5489",
            "b92831f0f6cf4b81bb5aaf07709e76c2",
            "95a39ea5baa6433a8a7945bc1321c0c2",
            "71a0a34e432c43e69408a0a58bb94d7f",
            "7676240a37dc451fae13593c0a14dd04",
            "cc04180e950142bfbbac22e9b3f50631",
            "8dbdf60713994ee487b866aef3e9d8fb",
            "cd13a20683d1434e81ff24c62c61062f",
            "468839e9821046dca2522a9f70d08a15",
            "d527560cc672479da23545843b5ce3d8",
            "ad489bcca0f543a3b4b7a129cd2b9ee6",
            "77056c1516c442d585d44ca811f483e3",
            "4e1749c0a95b49fc94b47f7b47f9bd4f",
            "2362bb41af734dd1a177b181db97d4f0",
            "041daf84b043406f9556ccdb781a9395",
            "868c74a8732d4bc4913a7f0cfbcf84af",
            "242debe30c79434493b1fa31743c038b",
            "cd0b327520c149519b65b39079428241",
            "500641e407e24282801352ff47fb1419",
            "a2c94d8efe9547a386ab4d394b5a0685",
            "d2a428c6fc674c0dace3089c2ecf8ba4",
            "febd425758d3491e8afaf7a59b0aaec1",
            "80904efc1d3d4c88915189bbf9e423d7"
          ]
        },
        "id": "GxHkhgjRJVFv",
        "outputId": "784aefa1-d619-48bb-d796-36ab1138cbb8"
      },
      "source": [
        "from datasets import list_datasets, load_dataset, list_metrics, load_metric, load_from_disk\n",
        "\n",
        "squad_dataset = load_dataset('squad_kor_v1') "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7a1ea1419ff94665a1679ebf1ef6c2c4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1710.0, style=ProgressStyle(description…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5deb1925808640979a566c5fa32eb6f2",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=962.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Downloading and preparing dataset squad_kor_v1/squad_kor_v1 (download: 40.44 MiB, generated: 87.40 MiB, post-processed: Unknown size, total: 127.84 MiB) to /root/.cache/huggingface/datasets/squad_kor_v1/squad_kor_v1/1.0.0/92f88eedc7d67b3f38389e8682eabe68caa450442cc4f7370a27873dbc045fe4...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "36f10e2a42d4499aab746161965d745e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=7568316.0, style=ProgressStyle(descript…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4097582128074b04b685b0e0dcf81571",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=770480.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cd13a20683d1434e81ff24c62c61062f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "868c74a8732d4bc4913a7f0cfbcf84af",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\rDataset squad_kor_v1 downloaded and prepared to /root/.cache/huggingface/datasets/squad_kor_v1/squad_kor_v1/1.0.0/92f88eedc7d67b3f38389e8682eabe68caa450442cc4f7370a27873dbc045fe4. Subsequent calls will reuse this data.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqU8Q1p2JkJL",
        "outputId": "4a81307a-2a56-49a7-f227-f30a4b243943"
      },
      "source": [
        "%%time\n",
        "\n",
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "\n",
        "squad_dataset_train = {}\n",
        "squad_dataset_train['id'] = []\n",
        "squad_dataset_train['title'] = []\n",
        "squad_dataset_train['context'] = []\n",
        "squad_dataset_train['question'] = []\n",
        "squad_dataset_train['answers'] = []\n",
        "\n",
        "\n",
        "for sdt in squad_dataset['train']:\n",
        "    if  len(sdt['answers']['answer_start']) > 0:\n",
        "        squad_dataset_train['id'].append(sdt['id'])\n",
        "        squad_dataset_train['title'].append(sdt['title'])\n",
        "        squad_dataset_train['context'].append(sdt['context'])\n",
        "        squad_dataset_train['question'].append(sdt['question'])\n",
        "        squad_dataset_train['answers'].append(sdt['answers'])\n",
        "\n",
        "dataset_train = Dataset.from_dict(squad_dataset_train)\n",
        "\n",
        "squad_dataset_val = {}\n",
        "squad_dataset_val['id'] = []\n",
        "squad_dataset_val['title'] = []\n",
        "squad_dataset_val['context'] = []\n",
        "squad_dataset_val['question'] = []\n",
        "squad_dataset_val['answers'] = []\n",
        "\n",
        "for sdt in squad_dataset['validation']:\n",
        "    if  len(sdt['answers']['answer_start']) > 0:\n",
        "        squad_dataset_val['id'].append(sdt['id'])\n",
        "        squad_dataset_val['title'].append(sdt['title'])\n",
        "        squad_dataset_val['context'].append(sdt['context'])\n",
        "        squad_dataset_val['question'].append(sdt['question'])\n",
        "        squad_dataset_val['answers'].append(sdt['answers'])\n",
        "\n",
        "dataset_val = Dataset.from_dict(squad_dataset_val)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 9.25 s, sys: 463 ms, total: 9.71 s\n",
            "Wall time: 9.38 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohLyW4TgJv1r",
        "outputId": "3cac8e75-35a7-4673-8a93-43558d1c10c6"
      },
      "source": [
        "%%time\n",
        "\n",
        "train_contexts = dataset_train['context']\n",
        "train_questions = dataset_train['question']\n",
        "train_answers = dataset_train['answers']\n",
        "\n",
        "val_contexts = dataset_val['context']\n",
        "val_questions = dataset_val['question']\n",
        "val_answers = dataset_val['answers']"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1.28 s, sys: 79.4 ms, total: 1.35 s\n",
            "Wall time: 1.35 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqvCK2x9J6TG"
      },
      "source": [
        "def add_end_idx(answers, contexts):\n",
        "    for answer, context in zip(answers, contexts):\n",
        "        try:\n",
        "        \n",
        "            gold_text = answer['text']\n",
        "            if isinstance(gold_text,list):\n",
        "                gold_text = gold_text[0]\n",
        "\n",
        "            start_idx = answer['answer_start']\n",
        "            if isinstance(start_idx,list):\n",
        "                start_idx = start_idx[0]\n",
        "\n",
        "            end_idx = start_idx + len(gold_text)\n",
        "\n",
        "            # sometimes squad answers are off by a character or two – fix this\n",
        "            if context[start_idx:end_idx] == gold_text:\n",
        "                answer['answer_end'] = [end_idx]\n",
        "            elif context[start_idx-1:end_idx-1] == gold_text:\n",
        "                answer['answer_start'] = [start_idx - 1]\n",
        "                answer['answer_end'] = [end_idx - 1]     # When the gold label is off by one character\n",
        "            elif context[start_idx-2:end_idx-2] == gold_text:\n",
        "                answer['answer_start'] = [start_idx - 2]\n",
        "                answer['answer_end'] = [end_idx - 2]     # When the gold label is off by two characters\n",
        "        except Exception as e:\n",
        "            pass\n",
        "            #print('context',context)\n",
        "            #print('gold_text',gold_text)\n",
        "\n",
        "\n",
        "add_end_idx(train_answers, train_contexts)\n",
        "add_end_idx(val_answers, val_contexts)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8hTx2G5J7jD"
      },
      "source": [
        "\n",
        "train_encodings = tokenizer(train_contexts, train_questions, max_length=512, truncation=True, padding=True)\n",
        "val_encodings = tokenizer(val_contexts, val_questions, max_length=512, truncation=True, padding=True)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2631889UxcA"
      },
      "source": [
        "squad_dataset = None"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfWPKZFDKFPr"
      },
      "source": [
        "def add_token_positions(encodings, answers):\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "    for i in range(len(answers)):\n",
        "        start_positions.append(encodings.char_to_token(i, answers[i]['answer_start'][0]))\n",
        "        end_positions.append(encodings.char_to_token(i, answers[i]['answer_end'][0] - 1))\n",
        "\n",
        "        # if start position is None, the answer passage has been truncated\n",
        "        if start_positions[-1] is None:\n",
        "            start_positions[-1] = tokenizer.model_max_length\n",
        "        if end_positions[-1] is None:\n",
        "            end_positions[-1] = tokenizer.model_max_length\n",
        "\n",
        "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
        "\n",
        "add_token_positions(train_encodings, train_answers)\n",
        "add_token_positions(val_encodings, val_answers)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p47o9_FNiqZG"
      },
      "source": [
        "# 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVuhMvRgKIBO"
      },
      "source": [
        "import torch\n",
        "\n",
        "class SquadDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "    #def __getitem__(self, idx):\n",
        "    #    return {key: val[idx] for key, val in self.encodings.items()}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings.input_ids)\n",
        "\n",
        "train_dataset = SquadDataset(train_encodings)\n",
        "val_dataset = SquadDataset(val_encodings)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4BQJiKzhHqq",
        "outputId": "5a5ddd3b-1d4f-4aff-9a4e-5475979db545"
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6ja7OlvLLeW",
        "outputId": "f7c0ba68-ea2b-4740-b59b-b04ac9dd65bf"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from transformers import AdamW\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model = AlbertForQuestionAnswering.from_pretrained('/content/drive/MyDrive/Tokenizer_train/albert-xlarge-kor')\n",
        "model.to(device)\n",
        "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
        "optim = AdamW(model.parameters(), lr=5e-5)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of AlbertForQuestionAnswering were not initialized from the model checkpoint at /content/drive/MyDrive/Tokenizer_train/albert-xlarge-kor and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdTGtftBLPpI"
      },
      "source": [
        "# Print iterations progress\n",
        "class ProgressBar:\n",
        "\n",
        "  def __init__(self,total=20, prefix = '', suffix = '', decimals = 1, length = 20, fill = '█', printEnd = \"\\r\"):\n",
        "    self.total = total\n",
        "    self.prefix = prefix\n",
        "    self.suffix = suffix\n",
        "    self.decimals = decimals\n",
        "    self.length = length\n",
        "    self.fill = fill\n",
        "    self.printEnd = printEnd\n",
        "    self.ite = 0\n",
        "\n",
        "  def printProgress(self,iteration, text):\n",
        "      self.ite += iteration\n",
        "      percent = (\"{0:.\" + str(self.decimals) + \"f}\").format(100 * (self.ite / float(self.total)))\n",
        "\n",
        "      filledLength = int(self.length * self.ite // self.total)\n",
        "      bar = self.fill * filledLength + '-' * (self.length - filledLength)\n",
        "      print(f'\\r{self.prefix} |{bar}| {percent}% {self.suffix}  {text}', end=\"\", flush=True)\n",
        "      # Print New Line on Complete\n",
        "      if self.ite == self.total: \n",
        "          print()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "3ePbOGWILUjQ",
        "outputId": "86d66262-7e32-4c01-cd72-ec7d057f6b42"
      },
      "source": [
        "\n",
        "batch_count = len(train_loader)\n",
        "model.to(device)\n",
        "model.train()\n",
        "#loss_graph = []\n",
        "epochs = 1\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    #seeding(1024+epoch)\n",
        "    pb = ProgressBar(total=batch_count,prefix='Epoch '+ str(epoch+1) + '/' + str(epochs))\n",
        "    current_batch = 0\n",
        "    for batch in train_loader:\n",
        "        current_batch+=1\n",
        "        #seeding(1024+epoch+current_batch)\n",
        "        optim.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        #print('input_ids',len(input_ids))\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        #print('attention_mask',len(attention_mask))\n",
        "        start_positions = batch['start_positions'].to(device)\n",
        "        #print('start_positions',len(start_positions))\n",
        "        end_positions = batch['end_positions'].to(device)\n",
        "        #print('end_positions',len(end_positions))\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
        "        loss = outputs[0]\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        pb.printProgress(+1,'loss:' + str(loss))\n",
        "        #loss_graph.append(loss.item())\n",
        "    model.save_pretrained('/content/drive/MyDrive/korQuAD1.0/model')\n",
        "\n",
        "model.eval()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1 |--------------------| 0.3%   loss:tensor(6.2383, device='cuda:0', grad_fn=<DivBackward0>)"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-efaf00a7b8ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_positions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_positions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_positions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_positions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mpb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintProgress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'loss:'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2kbBC4e_2Tm"
      },
      "source": [
        "\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='/content/drive/MyDrive/korQuAD1.0/model',          # output directory\n",
        "    num_train_epochs=3,              # total number of training epochs\n",
        "    per_device_train_batch_size=16,  # batch size per device during training\n",
        "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='/content/drive/MyDrive/korQuAD1.0/logs',            # directory for storing logs\n",
        "    logging_steps=10,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=val_dataset             # evaluation dataset\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCnkYdpoLeh1"
      },
      "source": [
        "model = AlbertForQuestionAnswering.from_pretrained('/content/drive/MyDrive/korQuAD1.0/model')\n",
        "#model = AlbertForQuestionAnswering.from_pretrained('/content/drive/MyDrive/Tokenizer_train/albert-kor-base')\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)\n",
        "\n",
        "examples = dataset_val\n",
        "\n",
        "def display_example(example):    \n",
        "    from pprint import pprint\n",
        "\n",
        "    #idx = qid_to_example_index[qid]\n",
        "    q = example['question']\n",
        "    c = example['context']\n",
        "    a = example['answers']['text']\n",
        "    \n",
        "    print(f'Example {example[\"id\"]} of {len(examples)}\\n---------------------')\n",
        "    print(f\"Q: {q}\\n\")\n",
        "    print(\"Context:\")\n",
        "    pprint(c)\n",
        "    print(f\"\\nTrue Answers:\\n{a}\")\n",
        "\n",
        "from pprint import pprint\n",
        "\n",
        "def get_prediction(example,display=False):\n",
        "\n",
        "    if display:\n",
        "        display_example(example)\n",
        "    # given a question id (qas_id or qid), load the example, get the model outputs and generate an answer\n",
        "    question = example['question'] #examples[qid_to_example_index[qid]].question_text\n",
        "    context = example['context'] #examples[qid_to_example_index[qid]].context_text\n",
        "\n",
        "    #print(f\"Q: {question}\\n\")\n",
        "    #print(\"Context:\")\n",
        "    #pprint(context)\n",
        "\n",
        "    inputs = tokenizer.encode_plus(question, context, max_length=256,return_tensors='pt')\n",
        "\n",
        "    inputs.to(device)\n",
        "    \n",
        "    outputs = model(**inputs)\n",
        "\n",
        "    #print(outputs)\n",
        "    #print(outputs)    \n",
        "    answer_start = torch.argmax(outputs[0])  # get the most likely beginning of answer with the argmax of the score\n",
        "    answer_end = torch.argmax(outputs[1]) + 1 \n",
        "\n",
        "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][answer_start:answer_end]))\n",
        "    if display:\n",
        "        print('')\n",
        "        print('Predict answer:',answer)\n",
        "    return answer\n",
        "\n",
        " # these functions are heavily influenced by the HF squad_metrics.py script\n",
        "def normalize_text(s):\n",
        "    \"\"\"Removing articles and punctuation, and standardizing whitespace are all typical text processing steps.\"\"\"\n",
        "    import string, re\n",
        "\n",
        "    def remove_articles(text):\n",
        "        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
        "        return re.sub(regex, \" \", text)\n",
        "\n",
        "    def white_space_fix(text):\n",
        "        return \" \".join(text.split())\n",
        "\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return \"\".join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "def compute_exact_match(prediction, truth):\n",
        "    return int(normalize_text(prediction) == normalize_text(truth))\n",
        "\n",
        "def compute_f1(prediction, truth):\n",
        "    pred_tokens = normalize_text(prediction).split()\n",
        "    truth_tokens = normalize_text(truth).split()\n",
        "    \n",
        "    # if either the prediction or the truth is no-answer then f1 = 1 if they agree, 0 otherwise\n",
        "    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
        "        return int(pred_tokens == truth_tokens)\n",
        "    \n",
        "    common_tokens = set(pred_tokens) & set(truth_tokens)\n",
        "    \n",
        "    # if there are no common tokens then f1 = 0\n",
        "    if len(common_tokens) == 0:\n",
        "        return 0\n",
        "    \n",
        "    prec = len(common_tokens) / len(pred_tokens)\n",
        "    rec = len(common_tokens) / len(truth_tokens)\n",
        "    \n",
        "    return 2 * (prec * rec) / (prec + rec)\n",
        "\n",
        "def get_gold_answers(example):\n",
        "    \"\"\"helper function that retrieves all possible true answers from a squad2.0 example\"\"\"\n",
        "    \n",
        "    gold_answers = example['answers']['text'] #[answer[\"text\"] for answer in example['answers'] if answer[\"text\"]]\n",
        "\n",
        "    # if gold_answers doesn't exist it's because this is a negative example - \n",
        "    # the only correct answer is an empty string\n",
        "    if not gold_answers:\n",
        "        gold_answers = [\"\"]\n",
        "        \n",
        "    return gold_answers\n",
        "\n",
        "def evaluate_example(example,display=False):\n",
        "    gold_answers = get_gold_answers(example)\n",
        "    prediction = get_prediction(example)\n",
        "    em_score = max((compute_exact_match(prediction, answer)) for answer in gold_answers)\n",
        "    f1_score = max((compute_f1(prediction, answer)) for answer in gold_answers)\n",
        "    if display:\n",
        "        print(f\"Question: {example['question']}\")\n",
        "        print(f\"Prediction: {prediction}\")\n",
        "        print(f\"True Answers: {gold_answers}\")\n",
        "        print(f\"EM: {em_score} \\t F1: {f1_score}\")\n",
        "    return em_score,f1_score"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYMzvq5-LlXC",
        "outputId": "bd9ed673-83a3-41d6-b2ee-79f10d605a34"
      },
      "source": [
        "em,f1 = evaluate_example(examples[0],display=True)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question: 임종석이 여의도 농민 폭력 시위를 주도한 혐의로 지명수배 된 날은?\n",
            "Prediction: 1989년 2월 15일\n",
            "True Answers: ['1989년 2월 15일']\n",
            "EM: 1 \t F1: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mF_CW4-uLob1",
        "outputId": "0533ba4a-8056-4548-94b2-07d3c2bbbf3b"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "ems = []\n",
        "f1s = []\n",
        "pb = ProgressBar(total=len(examples),prefix='Evaluate... ')\n",
        "for example in examples:\n",
        "    em,f1 = evaluate_example(example)\n",
        "    ems.append(em)\n",
        "    f1s.append(f1)\n",
        "    pb.printProgress(+1,'EM:' + str(em) + '  F1:' + str(f1))\n",
        "print('EM:',np.mean(ems))\n",
        "print('F1:',np.mean(f1s))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluate...  |████████████████████| 100.0%   EM:0  F1:0.5\n",
            "EM: 0.4664011084170419\n",
            "F1: 0.5565387760024785\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}