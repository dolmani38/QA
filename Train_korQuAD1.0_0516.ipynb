{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNVkFM25oRNDUefruq7D+dq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e0a234238a20442188bffeb50b8a82e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_508da04576294734b60f3fa90d40b2f5",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_31dda3106cc647fda699a60bf34a320c",
              "IPY_MODEL_7269bb94559540d8a2ad15890118c9a0"
            ]
          }
        },
        "508da04576294734b60f3fa90d40b2f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "31dda3106cc647fda699a60bf34a320c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4bbddc49854648ee809cab2744f19f03",
            "_dom_classes": [],
            "description": "Dl Completed...: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a6b91a068a2445da88b086a99123cca1"
          }
        },
        "7269bb94559540d8a2ad15890118c9a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a0c5df4e61074ded90aadad9d9123604",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 5/5 [01:57&lt;00:00, 23.44s/ url]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ffaa2c46811545579645138631dfcfb7"
          }
        },
        "4bbddc49854648ee809cab2744f19f03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a6b91a068a2445da88b086a99123cca1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a0c5df4e61074ded90aadad9d9123604": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ffaa2c46811545579645138631dfcfb7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b07bff8f8c0242cb9d14c4e9200fe065": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c138a58915804d3a82cdab0711e6ee13",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b2f387970f2b453db3beaedc6aaea0a0",
              "IPY_MODEL_10d6cdeac7824a1da0e7ee23a9857162"
            ]
          }
        },
        "c138a58915804d3a82cdab0711e6ee13": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b2f387970f2b453db3beaedc6aaea0a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8cde191a1d694cac973a9ff53c680c6f",
            "_dom_classes": [],
            "description": "Dl Size...: ",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f715a113aec4444f91d26969ea994d30"
          }
        },
        "10d6cdeac7824a1da0e7ee23a9857162": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8fad692c0da84f67b2981b9dc5085798",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 557/? [01:57&lt;00:00,  4.75 MiB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d94643c7f32d4c679260bb0abf305de3"
          }
        },
        "8cde191a1d694cac973a9ff53c680c6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f715a113aec4444f91d26969ea994d30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8fad692c0da84f67b2981b9dc5085798": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d94643c7f32d4c679260bb0abf305de3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a94f18110c324dc780e714e50491e9fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_be9e86e7e0b54aa1a8cceb4a7c44fbd9",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d6c60b10cea943e2b0fdba4f546ec849",
              "IPY_MODEL_d8100564205c4989b0cc9512ffdef942"
            ]
          }
        },
        "be9e86e7e0b54aa1a8cceb4a7c44fbd9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d6c60b10cea943e2b0fdba4f546ec849": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_83c6a479d4594efdaa7c8fdf758f86ef",
            "_dom_classes": [],
            "description": "Extraction completed...: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_edb7645bd9cf4ebcb0472ab8413f15ec"
          }
        },
        "d8100564205c4989b0cc9512ffdef942": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6dd9062c334a4755ba7b0a1bd86468ca",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2/2 [01:57&lt;00:00, 58.56s/ file]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0ecf99f564724d79b28843833d2e9c6b"
          }
        },
        "83c6a479d4594efdaa7c8fdf758f86ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "edb7645bd9cf4ebcb0472ab8413f15ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6dd9062c334a4755ba7b0a1bd86468ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0ecf99f564724d79b28843833d2e9c6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "141567d835724105af5415594e9a7f9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2293ab6f8444436d826a597626e581f1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e7df796c569f4432b158b7d360cea766",
              "IPY_MODEL_dc0e96d7d05e4ed0ac616431ffc17104"
            ]
          }
        },
        "2293ab6f8444436d826a597626e581f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e7df796c569f4432b158b7d360cea766": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_845e048e05db4a1ba2fbe08d84aefa40",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_15172d9909fd47dc98f1666eb3ab3102"
          }
        },
        "dc0e96d7d05e4ed0ac616431ffc17104": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_058be0a299de4e408294b8551c55bc63",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 222894/0 [15:10&lt;00:00, 231.40 examples/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6ecd6166ed4b41b9b09da5d04d26ada4"
          }
        },
        "845e048e05db4a1ba2fbe08d84aefa40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "15172d9909fd47dc98f1666eb3ab3102": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "058be0a299de4e408294b8551c55bc63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6ecd6166ed4b41b9b09da5d04d26ada4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "aa2bb3c703cd47bbae4c3725ebbe8607": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6d0cb13c40b94435b8cc28c53e04be9f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a82eba20d147451fae82808feebe2ee6",
              "IPY_MODEL_f8338f63e39e4297960120064bcd6cfa"
            ]
          }
        },
        "6d0cb13c40b94435b8cc28c53e04be9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a82eba20d147451fae82808feebe2ee6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3c771b594639409980363ad629509820",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 570,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 570,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_97618f3b65be4d0dbc736b7f5d57bbbc"
          }
        },
        "f8338f63e39e4297960120064bcd6cfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_aeeb3800b6e54c43947521d2b1db8bac",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 570/570 [00:15&lt;00:00, 36.9B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6765b2b669cb4ccc9b95023fd76b36b9"
          }
        },
        "3c771b594639409980363ad629509820": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "97618f3b65be4d0dbc736b7f5d57bbbc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "aeeb3800b6e54c43947521d2b1db8bac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6765b2b669cb4ccc9b95023fd76b36b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a5f744a372114cd1aa1b205d7fe38a78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_bd8b2b2d73e1470ba0c082828d499ef3",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6c79c2c1bee44c5487cdd7d54e4974ca",
              "IPY_MODEL_1b0c3cf5a34d4a23a50378e3111fbaab"
            ]
          }
        },
        "bd8b2b2d73e1470ba0c082828d499ef3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6c79c2c1bee44c5487cdd7d54e4974ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0cd1019099024c8bb465eb84371dd313",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 526681800,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 526681800,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_266830fdd6bf499698715a415a909773"
          }
        },
        "1b0c3cf5a34d4a23a50378e3111fbaab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_378f1fde5736470094c345f994541a65",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 527M/527M [00:14&lt;00:00, 36.3MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_17250d596553432d9a0c313cd3be22e8"
          }
        },
        "0cd1019099024c8bb465eb84371dd313": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "266830fdd6bf499698715a415a909773": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "378f1fde5736470094c345f994541a65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "17250d596553432d9a0c313cd3be22e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ad87e8b4d9dd4c50a623bc0c5b3231b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6ee83dcf024a4fc59dedf8ed6793c961",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ba3a7416f39b48ad98221f3445345c39",
              "IPY_MODEL_23a939dca94b4400a25b84a8f17feeca"
            ]
          }
        },
        "6ee83dcf024a4fc59dedf8ed6793c961": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ba3a7416f39b48ad98221f3445345c39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_03d48499e49a4b1588cf99006f1ee2ca",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 435779157,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 435779157,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8d0c6df303274deb900ec2e4e60645fc"
          }
        },
        "23a939dca94b4400a25b84a8f17feeca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_66de0432831d40a9b58728f36d6469e7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 436M/436M [07:32&lt;00:00, 963kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_86ccde2a1f0f44ea9c079baa0d7ccc88"
          }
        },
        "03d48499e49a4b1588cf99006f1ee2ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8d0c6df303274deb900ec2e4e60645fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "66de0432831d40a9b58728f36d6469e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "86ccde2a1f0f44ea9c079baa0d7ccc88": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "44757a3a7afe4e4eb95e95f11e94baed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_fee20f84e6204e7bad616f22de2b85ba",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_673b5f7827d7437f92e1f3240161aa5a",
              "IPY_MODEL_9170fc91ba304ad8b6a4c2d2ccb31355"
            ]
          }
        },
        "fee20f84e6204e7bad616f22de2b85ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "673b5f7827d7437f92e1f3240161aa5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e561f9e07090434cb77090aeddc4789a",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 344259,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 344259,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_de0381976d7c4fc68b06665cbae12834"
          }
        },
        "9170fc91ba304ad8b6a4c2d2ccb31355": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6d81c8e6bbaa47139a9161791c49e526",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 344k/344k [00:01&lt;00:00, 303kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_db5a8e04fa0949de9cfc76eb7494f9ca"
          }
        },
        "e561f9e07090434cb77090aeddc4789a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "de0381976d7c4fc68b06665cbae12834": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6d81c8e6bbaa47139a9161791c49e526": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "db5a8e04fa0949de9cfc76eb7494f9ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f6fb86e6f6914670968ad70d09ab5852": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_bd03d7a79e02416ebcdb5f6193414fe3",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b656b044f5964b18a69be602cf391abf",
              "IPY_MODEL_3bb580a2181641c48ca8a14da6e7c2a0"
            ]
          }
        },
        "bd03d7a79e02416ebcdb5f6193414fe3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b656b044f5964b18a69be602cf391abf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4ceff003d619437d8ecae1bf080db110",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 684,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 684,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c807b711d5e84e2fb60db9716f57a004"
          }
        },
        "3bb580a2181641c48ca8a14da6e7c2a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6c478a8fd1214967a49ba63b7a5f5c2d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 684/684 [00:00&lt;00:00, 802B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2a15c90485f04cdab63dcd9704bc68d7"
          }
        },
        "4ceff003d619437d8ecae1bf080db110": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c807b711d5e84e2fb60db9716f57a004": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6c478a8fd1214967a49ba63b7a5f5c2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2a15c90485f04cdab63dcd9704bc68d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cc2d8144b76b471a824dddd49b6c5889": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_51dca6c50af5412994758f1c854a78de",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2448a43b9a3645cebcabe6a5c21b0d6f",
              "IPY_MODEL_0c511bccc20b4815a1ff1a54d40f94da"
            ]
          }
        },
        "51dca6c50af5412994758f1c854a78de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2448a43b9a3645cebcabe6a5c21b0d6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_508cd332718045e290ac9a43cf950af3",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 53325832,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 53325832,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_42119f7a708b411b93a877db42feae49"
          }
        },
        "0c511bccc20b4815a1ff1a54d40f94da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5dad0c302f7d4533ada1482eebc97809",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 53.3M/53.3M [00:52&lt;00:00, 1.02MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_73e88e9c09f44e86a9ae023ec7f6ed21"
          }
        },
        "508cd332718045e290ac9a43cf950af3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "42119f7a708b411b93a877db42feae49": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5dad0c302f7d4533ada1482eebc97809": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "73e88e9c09f44e86a9ae023ec7f6ed21": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e80484b7bf7e42d8955a783527efa449": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8594184c30fd4f42884018665132071c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_bff33c907fae4468992a75d52c9b45cb",
              "IPY_MODEL_bdbd5317006d4cc988f2be327b60b9cc"
            ]
          }
        },
        "8594184c30fd4f42884018665132071c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bff33c907fae4468992a75d52c9b45cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1e0dea7098b14f9aaa9c05c45506771f",
            "_dom_classes": [],
            "description": "Downloading: ",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1710,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1710,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_27cf9f0a2ee1454e87c7f29425996149"
          }
        },
        "bdbd5317006d4cc988f2be327b60b9cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ba6b60c555d54ef7a5340884d7402a1e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 4.50k/? [00:00&lt;00:00, 17.2kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3deff3b92bbd4d5d90a9892b9bbf9b2b"
          }
        },
        "1e0dea7098b14f9aaa9c05c45506771f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "27cf9f0a2ee1454e87c7f29425996149": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ba6b60c555d54ef7a5340884d7402a1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3deff3b92bbd4d5d90a9892b9bbf9b2b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "37032dfb4c0447c99452bc2e1ed224cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6b87adb4e2ad44e58eac98160c435707",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b05a6f92b18846fdaef17065384a4f6d",
              "IPY_MODEL_5a2b32d20b684cfbad69671b1096d735"
            ]
          }
        },
        "6b87adb4e2ad44e58eac98160c435707": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b05a6f92b18846fdaef17065384a4f6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6a77122558a04f71adb0ef07660f001f",
            "_dom_classes": [],
            "description": "Downloading: ",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 962,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 962,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_adff7f3201f44b959c075871f1e3763f"
          }
        },
        "5a2b32d20b684cfbad69671b1096d735": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_158279eddc0b4daf95694635fb1e9316",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2.24k/? [00:00&lt;00:00, 9.95kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bb89231a6fab474db83927011f0799a3"
          }
        },
        "6a77122558a04f71adb0ef07660f001f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "adff7f3201f44b959c075871f1e3763f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "158279eddc0b4daf95694635fb1e9316": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bb89231a6fab474db83927011f0799a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b47b97389a99431cbae5685a6b1dad88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_67e36562ff6e4f10875a151f38de1849",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b7d7104883a24660845acba3f4e1be67",
              "IPY_MODEL_831402e822244e3dbe4859edd8a1e7e2"
            ]
          }
        },
        "67e36562ff6e4f10875a151f38de1849": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b7d7104883a24660845acba3f4e1be67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_33e67f2c219d4cee8cfc55b62da5c447",
            "_dom_classes": [],
            "description": "Downloading: ",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 7568316,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 7568316,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ab065fb1a10b4a079a5db50ca171184c"
          }
        },
        "831402e822244e3dbe4859edd8a1e7e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_62d20f0a59fc4ef5866b5b318fca0774",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 38.5M/? [00:01&lt;00:00, 36.6MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6fc869f1b5c9456994968eeb848bcaeb"
          }
        },
        "33e67f2c219d4cee8cfc55b62da5c447": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ab065fb1a10b4a079a5db50ca171184c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "62d20f0a59fc4ef5866b5b318fca0774": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6fc869f1b5c9456994968eeb848bcaeb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5ab973ee2a8b4c7487679b5749c5d988": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b79cb0d6551f4e6da16addea54df8089",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_76273585da6e465e87b43791649e7c14",
              "IPY_MODEL_2df0d84d9ec542e5b59dc17772440871"
            ]
          }
        },
        "b79cb0d6551f4e6da16addea54df8089": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "76273585da6e465e87b43791649e7c14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1094cb85854f4804bec39b414d8c346a",
            "_dom_classes": [],
            "description": "Downloading: ",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 770480,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 770480,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a9c85025461c44a0adec75f6b77c1c3e"
          }
        },
        "2df0d84d9ec542e5b59dc17772440871": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2c7b2acabfbf4cd8892560b657aca898",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 3.88M/? [00:00&lt;00:00, 16.2MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_617ee24100854a0fb804092c7ae73c72"
          }
        },
        "1094cb85854f4804bec39b414d8c346a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a9c85025461c44a0adec75f6b77c1c3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2c7b2acabfbf4cd8892560b657aca898": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "617ee24100854a0fb804092c7ae73c72": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a2663bcd7d2b46e89ed6239c56a8c0c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2a815e7963e347ac9e9a94d8bc8bc261",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_64ecb12825ab420ca3958762c024481c",
              "IPY_MODEL_d6c45b4eca264116a3fa09c56735988e"
            ]
          }
        },
        "2a815e7963e347ac9e9a94d8bc8bc261": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "64ecb12825ab420ca3958762c024481c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9184d682db724db1bfaf8be7641df92e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_adcec87047594835a0cb4af50a148f03"
          }
        },
        "d6c45b4eca264116a3fa09c56735988e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c9b1770e22b14fdca79c58c221ccede9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 60407/0 [00:04&lt;00:00, 13770.68 examples/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8e9a855880254e3b8622b6605077ba04"
          }
        },
        "9184d682db724db1bfaf8be7641df92e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "adcec87047594835a0cb4af50a148f03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c9b1770e22b14fdca79c58c221ccede9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8e9a855880254e3b8622b6605077ba04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9640d999963b4e85b39e1b16d36e55f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c18d2b23f4e942b1b4e35801e165819f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6967c92c15c84363a7fcba6f9e4dae8e",
              "IPY_MODEL_02cc2b431c9c4bd2a29d3c84d1c08706"
            ]
          }
        },
        "c18d2b23f4e942b1b4e35801e165819f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6967c92c15c84363a7fcba6f9e4dae8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c9f49e619a414fa0848ff3eff55224ee",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_43c00307ffe84f6db6a09ce92d83c0ff"
          }
        },
        "02cc2b431c9c4bd2a29d3c84d1c08706": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_eca6b0e7275e433eb5198045de51121d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 5774/0 [00:00&lt;00:00, 27.41 examples/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_11f0f543b5204d33992272bdb4817545"
          }
        },
        "c9f49e619a414fa0848ff3eff55224ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "43c00307ffe84f6db6a09ce92d83c0ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "eca6b0e7275e433eb5198045de51121d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "11f0f543b5204d33992272bdb4817545": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dolmani38/QA/blob/main/Train_korQuAD1.0_0516.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21tlIhtCjJia"
      },
      "source": [
        "# 한국어 Albert Tokenizer 학습!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zc1weyK_jPwQ",
        "outputId": "0136e561-8a94-4513-83cd-e553d53eb5eb"
      },
      "source": [
        "\n",
        "if True:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5DRavVNRey3"
      },
      "source": [
        "## 참조\n",
        "\n",
        "https://colab.research.google.com/github/parmarsuraj99/suraj-parmar/blob/master/_notebooks/2020-05-02-SanskritALBERT.ipynb#scrollTo=VNAOMXjpMHZD\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "038IYm33Muol"
      },
      "source": [
        "import os\n",
        "import gc\n",
        "import glob\n",
        "import torch\n",
        "import pickle\n",
        "import joblib\n",
        "from tqdm.auto import tqdm"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVCIQF4UMmyF",
        "outputId": "8380782d-a9cc-4bb8-cf4f-4cb9f27c234c"
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers\n",
        "!pip install transformers/.\n",
        "!pip install sentencepiece==0.1.95\n",
        "!pip install datasets==1.6.2"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 72664, done.\u001b[K\n",
            "remote: Counting objects: 100% (702/702), done.\u001b[K\n",
            "remote: Compressing objects: 100% (380/380), done.\u001b[K\n",
            "remote: Total 72664 (delta 393), reused 481 (delta 282), pack-reused 71962\u001b[K\n",
            "Receiving objects: 100% (72664/72664), 56.21 MiB | 27.57 MiB/s, done.\n",
            "Resolving deltas: 100% (51512/51512), done.\n",
            "Processing ./transformers\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (3.0.12)\n",
            "Collecting huggingface-hub==0.0.8\n",
            "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 12.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (20.9)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 27.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (4.0.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.7.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.7.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.7.0.dev0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.7.0.dev0) (2020.12.5)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.7.0.dev0) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.7.0.dev0) (8.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.7.0.dev0) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.7.0.dev0) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.7.0.dev0) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.7.0.dev0) (3.7.4.3)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.7.0.dev0-cp37-none-any.whl size=2259809 sha256=ac277b0bf9508c00784535fda1679c2a9e3a9de073ccd233484f808cc58dbdab\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-2ty09wu1/wheels/23/19/dd/2561a4e47240cf6b307729d58e56f8077dd0c698f5992216cf\n",
            "Successfully built transformers\n",
            "Installing collected packages: huggingface-hub, sacremoses, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.0.8 sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.7.0.dev0\n",
            "Collecting sentencepiece==0.1.95\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 7.8MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.95\n",
            "Collecting datasets==1.6.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/1a/b9f9b3bfef624686ae81c070f0a6bb635047b17cdb3698c7ad01281e6f9a/datasets-1.6.2-py3-none-any.whl (221kB)\n",
            "\u001b[K     |████████████████████████████████| 225kB 8.8MB/s \n",
            "\u001b[?25hCollecting fsspec\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/52/816d1a3a599176057bf29dfacb1f8fadb61d35fbd96cb1bab4aaa7df83c0/fsspec-2021.5.0-py3-none-any.whl (111kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 46.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub<0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets==1.6.2) (0.0.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets==1.6.2) (1.1.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets==1.6.2) (4.0.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets==1.6.2) (20.9)\n",
            "Collecting xxhash\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/4f/0a862cad26aa2ed7a7cd87178cbbfa824fc1383e472d63596a0d018374e7/xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 37.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets==1.6.2) (0.3.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets==1.6.2) (1.19.5)\n",
            "Requirement already satisfied: pyarrow>=1.0.0<4.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets==1.6.2) (3.0.0)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets==1.6.2) (4.41.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets==1.6.2) (2.23.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets==1.6.2) (0.70.11.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets==1.6.2) (3.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.6.2) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.6.2) (2018.9)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets==1.6.2) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets==1.6.2) (3.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets==1.6.2) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.6.2) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.6.2) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.6.2) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.6.2) (3.0.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets==1.6.2) (1.15.0)\n",
            "Installing collected packages: fsspec, xxhash, datasets\n",
            "Successfully installed datasets-1.6.2 fsspec-2021.5.0 xxhash-2.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQt6ukg-M5p9"
      },
      "source": [
        "## mecab 설치 참조\n",
        "\n",
        "https://somjang.tistory.com/entry/Google-Colab%EC%97%90%EC%84%9C-Mecab-koMecab-ko-dic-%EC%89%BD%EA%B2%8C-%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7E1YDPFdMslI",
        "outputId": "7eddce76-d321-49ee-b688-21ccba51ba2d"
      },
      "source": [
        "!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Mecab-ko-for-Google-Colab'...\n",
            "remote: Enumerating objects: 91, done.\u001b[K\n",
            "remote: Counting objects:   1% (1/91)\u001b[K\rremote: Counting objects:   2% (2/91)\u001b[K\rremote: Counting objects:   3% (3/91)\u001b[K\rremote: Counting objects:   4% (4/91)\u001b[K\rremote: Counting objects:   5% (5/91)\u001b[K\rremote: Counting objects:   6% (6/91)\u001b[K\rremote: Counting objects:   7% (7/91)\u001b[K\rremote: Counting objects:   8% (8/91)\u001b[K\rremote: Counting objects:   9% (9/91)\u001b[K\rremote: Counting objects:  10% (10/91)\u001b[K\rremote: Counting objects:  12% (11/91)\u001b[K\rremote: Counting objects:  13% (12/91)\u001b[K\rremote: Counting objects:  14% (13/91)\u001b[K\rremote: Counting objects:  15% (14/91)\u001b[K\rremote: Counting objects:  16% (15/91)\u001b[K\rremote: Counting objects:  17% (16/91)\u001b[K\rremote: Counting objects:  18% (17/91)\u001b[K\rremote: Counting objects:  19% (18/91)\u001b[K\rremote: Counting objects:  20% (19/91)\u001b[K\rremote: Counting objects:  21% (20/91)\u001b[K\rremote: Counting objects:  23% (21/91)\u001b[K\rremote: Counting objects:  24% (22/91)\u001b[K\rremote: Counting objects:  25% (23/91)\u001b[K\rremote: Counting objects:  26% (24/91)\u001b[K\rremote: Counting objects:  27% (25/91)\u001b[K\rremote: Counting objects:  28% (26/91)\u001b[K\rremote: Counting objects:  29% (27/91)\u001b[K\rremote: Counting objects:  30% (28/91)\u001b[K\rremote: Counting objects:  31% (29/91)\u001b[K\rremote: Counting objects:  32% (30/91)\u001b[K\rremote: Counting objects:  34% (31/91)\u001b[K\rremote: Counting objects:  35% (32/91)\u001b[K\rremote: Counting objects:  36% (33/91)\u001b[K\rremote: Counting objects:  37% (34/91)\u001b[K\rremote: Counting objects:  38% (35/91)\u001b[K\rremote: Counting objects:  39% (36/91)\u001b[K\rremote: Counting objects:  40% (37/91)\u001b[K\rremote: Counting objects:  41% (38/91)\u001b[K\rremote: Counting objects:  42% (39/91)\u001b[K\rremote: Counting objects:  43% (40/91)\u001b[K\rremote: Counting objects:  45% (41/91)\u001b[K\rremote: Counting objects:  46% (42/91)\u001b[K\rremote: Counting objects:  47% (43/91)\u001b[K\rremote: Counting objects:  48% (44/91)\u001b[K\rremote: Counting objects:  49% (45/91)\u001b[K\rremote: Counting objects:  50% (46/91)\u001b[K\rremote: Counting objects:  51% (47/91)\u001b[K\rremote: Counting objects:  52% (48/91)\u001b[K\rremote: Counting objects:  53% (49/91)\u001b[K\rremote: Counting objects:  54% (50/91)\u001b[K\rremote: Counting objects:  56% (51/91)\u001b[K\rremote: Counting objects:  57% (52/91)\u001b[K\rremote: Counting objects:  58% (53/91)\u001b[K\rremote: Counting objects:  59% (54/91)\u001b[K\rremote: Counting objects:  60% (55/91)\u001b[K\rremote: Counting objects:  61% (56/91)\u001b[K\rremote: Counting objects:  62% (57/91)\u001b[K\rremote: Counting objects:  63% (58/91)\u001b[K\rremote: Counting objects:  64% (59/91)\u001b[K\rremote: Counting objects:  65% (60/91)\u001b[K\rremote: Counting objects:  67% (61/91)\u001b[K\rremote: Counting objects:  68% (62/91)\u001b[K\rremote: Counting objects:  69% (63/91)\u001b[K\rremote: Counting objects:  70% (64/91)\u001b[K\rremote: Counting objects:  71% (65/91)\u001b[K\rremote: Counting objects:  72% (66/91)\u001b[K\rremote: Counting objects:  73% (67/91)\u001b[K\rremote: Counting objects:  74% (68/91)\u001b[K\rremote: Counting objects:  75% (69/91)\u001b[K\rremote: Counting objects:  76% (70/91)\u001b[K\rremote: Counting objects:  78% (71/91)\u001b[K\rremote: Counting objects:  79% (72/91)\u001b[K\rremote: Counting objects:  80% (73/91)\u001b[K\rremote: Counting objects:  81% (74/91)\u001b[K\rremote: Counting objects:  82% (75/91)\u001b[K\rremote: Counting objects:  83% (76/91)\u001b[K\rremote: Counting objects:  84% (77/91)\u001b[K\rremote: Counting objects:  85% (78/91)\u001b[K\rremote: Counting objects:  86% (79/91)\u001b[K\rremote: Counting objects:  87% (80/91)\u001b[K\rremote: Counting objects:  89% (81/91)\u001b[K\rremote: Counting objects:  90% (82/91)\u001b[K\rremote: Counting objects:  91% (83/91)\u001b[K\rremote: Counting objects:  92% (84/91)\u001b[K\rremote: Counting objects:  93% (85/91)\u001b[K\rremote: Counting objects:  94% (86/91)\u001b[K\rremote: Counting objects:  95% (87/91)\u001b[K\rremote: Counting objects:  96% (88/91)\u001b[K\rremote: Counting objects:  97% (89/91)\u001b[K\rremote: Counting objects:  98% (90/91)\u001b[K\rremote: Counting objects: 100% (91/91)\u001b[K\rremote: Counting objects: 100% (91/91), done.\u001b[K\n",
            "remote: Compressing objects:   1% (1/85)\u001b[K\rremote: Compressing objects:   2% (2/85)\u001b[K\rremote: Compressing objects:   3% (3/85)\u001b[K\rremote: Compressing objects:   4% (4/85)\u001b[K\rremote: Compressing objects:   5% (5/85)\u001b[K\rremote: Compressing objects:   7% (6/85)\u001b[K\rremote: Compressing objects:   8% (7/85)\u001b[K\rremote: Compressing objects:   9% (8/85)\u001b[K\rremote: Compressing objects:  10% (9/85)\u001b[K\rremote: Compressing objects:  11% (10/85)\u001b[K\rremote: Compressing objects:  12% (11/85)\u001b[K\rremote: Compressing objects:  14% (12/85)\u001b[K\rremote: Compressing objects:  15% (13/85)\u001b[K\rremote: Compressing objects:  16% (14/85)\u001b[K\rremote: Compressing objects:  17% (15/85)\u001b[K\rremote: Compressing objects:  18% (16/85)\u001b[K\rremote: Compressing objects:  20% (17/85)\u001b[K\rremote: Compressing objects:  21% (18/85)\u001b[K\rremote: Compressing objects:  22% (19/85)\u001b[K\rremote: Compressing objects:  23% (20/85)\u001b[K\rremote: Compressing objects:  24% (21/85)\u001b[K\rremote: Compressing objects:  25% (22/85)\u001b[K\rremote: Compressing objects:  27% (23/85)\u001b[K\rremote: Compressing objects:  28% (24/85)\u001b[K\rremote: Compressing objects:  29% (25/85)\u001b[K\rremote: Compressing objects:  30% (26/85)\u001b[K\rremote: Compressing objects:  31% (27/85)\u001b[K\rremote: Compressing objects:  32% (28/85)\u001b[K\rremote: Compressing objects:  34% (29/85)\u001b[K\rremote: Compressing objects:  35% (30/85)\u001b[K\rremote: Compressing objects:  36% (31/85)\u001b[K\rremote: Compressing objects:  37% (32/85)\u001b[K\rremote: Compressing objects:  38% (33/85)\u001b[K\rremote: Compressing objects:  40% (34/85)\u001b[K\rremote: Compressing objects:  41% (35/85)\u001b[K\rremote: Compressing objects:  42% (36/85)\u001b[K\rremote: Compressing objects:  43% (37/85)\u001b[K\rremote: Compressing objects:  44% (38/85)\u001b[K\rremote: Compressing objects:  45% (39/85)\u001b[K\rremote: Compressing objects:  47% (40/85)\u001b[K\rremote: Compressing objects:  48% (41/85)\u001b[K\rremote: Compressing objects:  49% (42/85)\u001b[K\rremote: Compressing objects:  50% (43/85)\u001b[K\rremote: Compressing objects:  51% (44/85)\u001b[K\rremote: Compressing objects:  52% (45/85)\u001b[K\rremote: Compressing objects:  54% (46/85)\u001b[K\rremote: Compressing objects:  55% (47/85)\u001b[K\rremote: Compressing objects:  56% (48/85)\u001b[K\rremote: Compressing objects:  57% (49/85)\u001b[K\rremote: Compressing objects:  58% (50/85)\u001b[K\rremote: Compressing objects:  60% (51/85)\u001b[K\rremote: Compressing objects:  61% (52/85)\u001b[K\rremote: Compressing objects:  62% (53/85)\u001b[K\rremote: Compressing objects:  63% (54/85)\u001b[K\rremote: Compressing objects:  64% (55/85)\u001b[K\rremote: Compressing objects:  65% (56/85)\u001b[K\rremote: Compressing objects:  67% (57/85)\u001b[K\rremote: Compressing objects:  68% (58/85)\u001b[K\rremote: Compressing objects:  69% (59/85)\u001b[K\rremote: Compressing objects:  70% (60/85)\u001b[K\rremote: Compressing objects:  71% (61/85)\u001b[K\rremote: Compressing objects:  72% (62/85)\u001b[K\rremote: Compressing objects:  74% (63/85)\u001b[K\rremote: Compressing objects:  75% (64/85)\u001b[K\rremote: Compressing objects:  76% (65/85)\u001b[K\rremote: Compressing objects:  77% (66/85)\u001b[K\rremote: Compressing objects:  78% (67/85)\u001b[K\rremote: Compressing objects:  80% (68/85)\u001b[K\rremote: Compressing objects:  81% (69/85)\u001b[K\rremote: Compressing objects:  82% (70/85)\u001b[K\rremote: Compressing objects:  83% (71/85)\u001b[K\rremote: Compressing objects:  84% (72/85)\u001b[K\rremote: Compressing objects:  85% (73/85)\u001b[K\rremote: Compressing objects:  87% (74/85)\u001b[K\rremote: Compressing objects:  88% (75/85)\u001b[K\rremote: Compressing objects:  89% (76/85)\u001b[K\rremote: Compressing objects:  90% (77/85)\u001b[K\rremote: Compressing objects:  91% (78/85)\u001b[K\rremote: Compressing objects:  92% (79/85)\u001b[K\rremote: Compressing objects:  94% (80/85)\u001b[K\rremote: Compressing objects:  95% (81/85)\u001b[K\rremote: Compressing objects:  96% (82/85)\u001b[K\rremote: Compressing objects:  97% (83/85)\u001b[K\rremote: Compressing objects:  98% (84/85)\u001b[K\rremote: Compressing objects: 100% (85/85)\u001b[K\rremote: Compressing objects: 100% (85/85), done.\u001b[K\n",
            "Unpacking objects:   1% (1/91)   \rUnpacking objects:   2% (2/91)   \rUnpacking objects:   3% (3/91)   \rUnpacking objects:   4% (4/91)   \rUnpacking objects:   5% (5/91)   \rUnpacking objects:   6% (6/91)   \rUnpacking objects:   7% (7/91)   \rUnpacking objects:   8% (8/91)   \rUnpacking objects:   9% (9/91)   \rUnpacking objects:  10% (10/91)   \rUnpacking objects:  12% (11/91)   \rUnpacking objects:  13% (12/91)   \rUnpacking objects:  14% (13/91)   \rUnpacking objects:  15% (14/91)   \rUnpacking objects:  16% (15/91)   \rUnpacking objects:  17% (16/91)   \rUnpacking objects:  18% (17/91)   \rUnpacking objects:  19% (18/91)   \rUnpacking objects:  20% (19/91)   \rUnpacking objects:  21% (20/91)   \rUnpacking objects:  23% (21/91)   \rUnpacking objects:  24% (22/91)   \rUnpacking objects:  25% (23/91)   \rUnpacking objects:  26% (24/91)   \rUnpacking objects:  27% (25/91)   \rUnpacking objects:  28% (26/91)   \rUnpacking objects:  29% (27/91)   \rUnpacking objects:  30% (28/91)   \rUnpacking objects:  31% (29/91)   \rUnpacking objects:  32% (30/91)   \rUnpacking objects:  34% (31/91)   \rUnpacking objects:  35% (32/91)   \rUnpacking objects:  36% (33/91)   \rUnpacking objects:  37% (34/91)   \rUnpacking objects:  38% (35/91)   \rUnpacking objects:  39% (36/91)   \rUnpacking objects:  40% (37/91)   \rUnpacking objects:  41% (38/91)   \rUnpacking objects:  42% (39/91)   \rUnpacking objects:  43% (40/91)   \rUnpacking objects:  45% (41/91)   \rUnpacking objects:  46% (42/91)   \rremote: Total 91 (delta 43), reused 22 (delta 6), pack-reused 0\u001b[K\n",
            "Unpacking objects:  47% (43/91)   \rUnpacking objects:  48% (44/91)   \rUnpacking objects:  49% (45/91)   \rUnpacking objects:  50% (46/91)   \rUnpacking objects:  51% (47/91)   \rUnpacking objects:  52% (48/91)   \rUnpacking objects:  53% (49/91)   \rUnpacking objects:  54% (50/91)   \rUnpacking objects:  56% (51/91)   \rUnpacking objects:  57% (52/91)   \rUnpacking objects:  58% (53/91)   \rUnpacking objects:  59% (54/91)   \rUnpacking objects:  60% (55/91)   \rUnpacking objects:  61% (56/91)   \rUnpacking objects:  62% (57/91)   \rUnpacking objects:  63% (58/91)   \rUnpacking objects:  64% (59/91)   \rUnpacking objects:  65% (60/91)   \rUnpacking objects:  67% (61/91)   \rUnpacking objects:  68% (62/91)   \rUnpacking objects:  69% (63/91)   \rUnpacking objects:  70% (64/91)   \rUnpacking objects:  71% (65/91)   \rUnpacking objects:  72% (66/91)   \rUnpacking objects:  73% (67/91)   \rUnpacking objects:  74% (68/91)   \rUnpacking objects:  75% (69/91)   \rUnpacking objects:  76% (70/91)   \rUnpacking objects:  78% (71/91)   \rUnpacking objects:  79% (72/91)   \rUnpacking objects:  80% (73/91)   \rUnpacking objects:  81% (74/91)   \rUnpacking objects:  82% (75/91)   \rUnpacking objects:  83% (76/91)   \rUnpacking objects:  84% (77/91)   \rUnpacking objects:  85% (78/91)   \rUnpacking objects:  86% (79/91)   \rUnpacking objects:  87% (80/91)   \rUnpacking objects:  89% (81/91)   \rUnpacking objects:  90% (82/91)   \rUnpacking objects:  91% (83/91)   \rUnpacking objects:  92% (84/91)   \rUnpacking objects:  93% (85/91)   \rUnpacking objects:  94% (86/91)   \rUnpacking objects:  95% (87/91)   \rUnpacking objects:  96% (88/91)   \rUnpacking objects:  97% (89/91)   \rUnpacking objects:  98% (90/91)   \rUnpacking objects: 100% (91/91)   \rUnpacking objects: 100% (91/91), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1XdrdGGM9ky",
        "outputId": "4c785985-f752-4e7a-a824-8656e2190560"
      },
      "source": [
        "!bash ./Mecab-ko-for-Google-Colab/install_mecab-ko_on_colab190912.sh"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Installing konlpy.....\n",
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 18.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 11.2MB/s \n",
            "\u001b[?25hCollecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Collecting JPype1>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/a5/9781e2ef4ca92d09912c4794642c1653aea7607f473e156cf4d423a881a1/JPype1-1.2.1-cp37-cp37m-manylinux2010_x86_64.whl (457kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 30.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Installing collected packages: beautifulsoup4, colorama, JPype1, konlpy\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-1.2.1 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2\n",
            "Done\n",
            "Installing mecab-0.996-ko-0.9.2.tar.gz.....\n",
            "Downloading mecab-0.996-ko-0.9.2.tar.gz.......\n",
            "from https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz\n",
            "--2021-05-15 05:44:43--  https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz\n",
            "Resolving bitbucket.org (bitbucket.org)... 104.192.141.1, 2406:da00:ff00::22c2:513, 2406:da00:ff00::22cd:e0db, ...\n",
            "Connecting to bitbucket.org (bitbucket.org)|104.192.141.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://bbuseruploads.s3.amazonaws.com/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz?Signature=Uk44Vx4uApIAENpmDG12Df%2B0DEA%3D&Expires=1621058695&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=null&response-content-disposition=attachment%3B%20filename%3D%22mecab-0.996-ko-0.9.2.tar.gz%22&response-content-encoding=None [following]\n",
            "--2021-05-15 05:44:43--  https://bbuseruploads.s3.amazonaws.com/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz?Signature=Uk44Vx4uApIAENpmDG12Df%2B0DEA%3D&Expires=1621058695&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=null&response-content-disposition=attachment%3B%20filename%3D%22mecab-0.996-ko-0.9.2.tar.gz%22&response-content-encoding=None\n",
            "Resolving bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)... 52.217.138.113\n",
            "Connecting to bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)|52.217.138.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1414979 (1.3M) [application/x-tar]\n",
            "Saving to: ‘mecab-0.996-ko-0.9.2.tar.gz’\n",
            "\n",
            "mecab-0.996-ko-0.9. 100%[===================>]   1.35M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2021-05-15 05:44:43 (29.5 MB/s) - ‘mecab-0.996-ko-0.9.2.tar.gz’ saved [1414979/1414979]\n",
            "\n",
            "Done\n",
            "Unpacking mecab-0.996-ko-0.9.2.tar.gz.......\n",
            "Done\n",
            "Change Directory to mecab-0.996-ko-0.9.2.......\n",
            "installing mecab-0.996-ko-0.9.2.tar.gz........\n",
            "configure\n",
            "make\n",
            "make check\n",
            "make install\n",
            "ldconfig\n",
            "Done\n",
            "Change Directory to /content\n",
            "Downloading mecab-ko-dic-2.1.1-20180720.tar.gz.......\n",
            "from https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz\n",
            "--2021-05-15 05:46:12--  https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz\n",
            "Resolving bitbucket.org (bitbucket.org)... 18.205.93.0, 18.205.93.1, 18.205.93.2, ...\n",
            "Connecting to bitbucket.org (bitbucket.org)|18.205.93.0|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://bbuseruploads.s3.amazonaws.com/a4fcd83e-34f1-454e-a6ac-c242c7d434d3/downloads/b5a0c703-7b64-45ed-a2d7-180e962710b6/mecab-ko-dic-2.1.1-20180720.tar.gz?Signature=3IIoVirT64wRtZasSICG2JsJ7Hg%3D&Expires=1621059025&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=tzyxc1TtnZU_zEuaaQDGN4F76hPDpyFq&response-content-disposition=attachment%3B%20filename%3D%22mecab-ko-dic-2.1.1-20180720.tar.gz%22&response-content-encoding=None [following]\n",
            "--2021-05-15 05:46:12--  https://bbuseruploads.s3.amazonaws.com/a4fcd83e-34f1-454e-a6ac-c242c7d434d3/downloads/b5a0c703-7b64-45ed-a2d7-180e962710b6/mecab-ko-dic-2.1.1-20180720.tar.gz?Signature=3IIoVirT64wRtZasSICG2JsJ7Hg%3D&Expires=1621059025&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=tzyxc1TtnZU_zEuaaQDGN4F76hPDpyFq&response-content-disposition=attachment%3B%20filename%3D%22mecab-ko-dic-2.1.1-20180720.tar.gz%22&response-content-encoding=None\n",
            "Resolving bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)... 52.217.18.100\n",
            "Connecting to bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)|52.217.18.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 49775061 (47M) [application/x-tar]\n",
            "Saving to: ‘mecab-ko-dic-2.1.1-20180720.tar.gz’\n",
            "\n",
            "mecab-ko-dic-2.1.1- 100%[===================>]  47.47M   100MB/s    in 0.5s    \n",
            "\n",
            "2021-05-15 05:46:12 (100 MB/s) - ‘mecab-ko-dic-2.1.1-20180720.tar.gz’ saved [49775061/49775061]\n",
            "\n",
            "Done\n",
            "Unpacking  mecab-ko-dic-2.1.1-20180720.tar.gz.......\n",
            "Done\n",
            "Change Directory to mecab-ko-dic-2.1.1-20180720\n",
            "Done\n",
            "installing........\n",
            "configure\n",
            "make\n",
            "make install\n",
            "apt-get update\n",
            "apt-get upgrade\n",
            "apt install curl\n",
            "apt install git\n",
            "bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\n",
            "Done\n",
            "Successfully Installed\n",
            "Now you can use Mecab\n",
            "from konlpy.tag import Mecab\n",
            "mecab = Mecab()\n",
            "사용자 사전 추가 방법 : https://bit.ly/3k0ZH53\n",
            "NameError: name 'Tagger' is not defined 오류 발생 시 런타임을 재실행 해주세요\n",
            "블로그에 해결 방법을 남겨주신 tana님 감사합니다.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecJdu9LKxh2z"
      },
      "source": [
        "# 한국어 Tokenizer 학습"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaP5MmCejnq_"
      },
      "source": [
        "## dataset 만들기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqRjnbbJlpBl"
      },
      "source": [
        "### 네이버 뉴스.... 평화 평가 댓글... 에서 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUJMmmyZmpqX",
        "outputId": "0a3126f9-b89f-470c-9639-d3362a5b1426"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ea31IIPRm2cy"
      },
      "source": [
        "\n",
        "# 간단한 전처리\n",
        "def clean_text(txt):\n",
        "    txt = txt.replace('\\n',' ')\n",
        "    txt = txt.replace('\\r',' ')    \n",
        "    txt = txt.replace('=','')\n",
        "    txt = txt.replace('\\\"','')   \n",
        "    txt = txt.replace('\\'','')\n",
        "    #txt = txt.replace(',','')\n",
        "    txt = txt.replace('..','')\n",
        "    txt = txt.replace('...','')\n",
        "    #txt = txt.replace('.','. ')\n",
        "    txt = txt.replace('.','. ')\n",
        "    txt = txt.replace('  ',' ')\n",
        "    txt = txt.replace('  ',' ')    \n",
        "    txt = txt.replace('  ',' ')   \n",
        "    txt = txt.replace('  ',' ')           \n",
        "    txt = txt.replace('  ',' ')\n",
        "    txt = txt.replace('  ',' ')    \n",
        "    txt = txt.replace('  ',' ')   \n",
        "    txt = txt.replace('  ',' ')             \n",
        "    return txt.strip()\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RijcRl1ukG3Z",
        "outputId": "dae0f266-f09f-40a0-b13a-081719b9d7e8"
      },
      "source": [
        "%%time\n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "import sys\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/summary/korean_news_corpus.csv')\n",
        "# 검사...\n",
        "pattens = [\"[34569][0-9]{3}[\\;.\\;-\\; ][0-9]{4}[\\;.\\;-\\; ][0-9]{4}[\\;.\\;-\\; ][0-9]{4}\",\n",
        "           \"[0-9]{2,3}[\\:\\s\\;.\\;,\\;-;)][0-9]{3,4}[\\:\\s\\;.\\;,\\;-][0-9]{4}\",\n",
        "           \"[0-9]{1}[0-9]{1}[\\W]?[0-1]{1}[0-9]{1}[\\W]?[0-3]{1}[\\W]?[0-9]{1}[\\W]?[1-4]{1}[\\W]?[0-9]{1}[\\W]?[0-9]{1}[\\W]?[0-9]{1}[\\W]?[0-9]{1}[\\W]?[0-9]{1}[\\W]?[0-9]{1}\",\n",
        "           \"[0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{6}|[0-9]{3}[\\:\\s\\;.\\;,\\;-]([0-9]{5,6}[\\:\\s\\;.\\;,\\;-][0-9]{3}|[0-9]{6}[\\:\\s\\;.\\;,\\;-][0-9]{5}|[0-9]{2,3}[\\:\\s\\;.\\;,\\;-][0-9]{6}|[0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{7}|[0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{4,6}[\\:\\s\\;.\\;,\\;-][0-9]|[0-9]{5}[\\:\\s\\;.\\;,\\;-][0-9]{3}[\\:\\s\\;.\\;,\\;-][0-9]{2}|[0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{5}[\\:\\s\\;.\\;,\\;-][0-9]{3}|[0-9]{4}[\\:\\s\\;.\\;,\\;-][0-9]{4}[\\:\\s\\;.\\;,\\;-][0-9]{3}|[0-9]{6}[\\:\\s\\;.\\;,\\;-][0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{3}|[0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{7})|[0-9]{4}[\\:\\s\\;.\\;,\\;-]([0-9]{3}[\\:\\s\\;.\\;,\\;-][0-9]{6}|[0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{6}[\\:\\s\\;.\\;,\\;-][0-9])|[0-9]{5}[\\:\\s\\;.\\;,\\;-][0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{6}|[0-9]{6}[\\:\\s\\;.\\;,\\;-][0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{5,6}\"\n",
        "           ]\n",
        "\n",
        "filters = []\n",
        "for p in pattens:\n",
        "    filters.append(re.compile(p))\n",
        "\n",
        "sentences = []\n",
        "df = df.dropna(axis=0)\n",
        "cnt = df['contents'].count()\n",
        "print('Total row count:',cnt)\n",
        "i=0\n",
        "for raw_text in df['contents']:\n",
        "    i=i+1\n",
        "    try:\n",
        "        if i%100 == 0:\n",
        "            percent = (\"{0:.2f}\").format(100 * (i / float(cnt)))\n",
        "            print(f'\\r {percent}% {i}/{str(cnt)}', end=\"\", flush=True)\n",
        "\n",
        "        docs = nltk.sent_tokenize(clean_text(raw_text))\n",
        "        for txt in docs:\n",
        "            if txt.find('▶') > -1 or txt.find('@') > -1 or txt.find('ⓒ') > -1: \n",
        "                pass\n",
        "            else:\n",
        "                txt = txt.strip()\n",
        "                if any(chr.isdigit() for chr in txt) :\n",
        "                    pass\n",
        "                else:\n",
        "                    sentences.append(txt)\n",
        "    except KeyboardInterrupt as ki:\n",
        "        raise ki        \n",
        "    except:\n",
        "        pass #print(\"Unexpected error:\", sys.exc_info()[0])\n",
        "\n",
        "print('')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total row count: 140536\n",
            " 99.97% 140500/140536\n",
            "CPU times: user 3min 20s, sys: 2.5 s, total: 3min 22s\n",
            "Wall time: 3min 24s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-U-syo92kQis",
        "outputId": "30e3feac-3c7d-4b90-95e8-9fc499b38c4e"
      },
      "source": [
        "len(sentences)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2967202"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FcekjFG6nvX9",
        "outputId": "83f36837-f043-4381-dc46-f0f99d643337"
      },
      "source": [
        "sentences[:10]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['연합뉴스 문재인 대통령이 변창흠 국토교통부 장관의 사의 표명을 사실상 수용했다.',\n",
              " '문 대통령은 변 장관의 사의 표명에 책임지는 모습을 보일 수밖에 없다고 밝혔다.',\n",
              " '이는 문 대통령이 사실상 변 장관의 사의를 수용한 것으로 해석된다.',\n",
              " '이에 앞서 변창흠 국토교통부 장관은 LH 땅 투기 의혹 사건과 관련한 책임론에 대해 “자리에 연연하지 않는다”면서 “(청와대의) 결정에 따르겠다”고 말했다.',\n",
              " '변 장관은 이날 국회 국토교통위원회 전체회의에 참석해 “LH 사태로 국민들이 걱정하는 부분을 해소할 수 있게 최대한 대안을 만들고 LH가 근본적으로 다시 태어날 수 있도록 책임지고 추진하겠다”고 언급하고 “그 역할이 충분하다고 평가되지 못했을 때 언제든지 자리에 연연하지 않고 결정에 따르겠다”고 말했다.',\n",
              " 'kr ) 무단 전재 및 재배포 금지',\n",
              " 'kr ) 무단 전재 및 재배포 금지',\n",
              " 'kr ) 무단 전재 및 재배포 금지',\n",
              " '인재 양성·소외 계층 지원 등 계획 “부친 질병·가난 악순환 끊기 원해 국내 최고 넘어 세계적 병원 되길 정몽구 현대차그룹 명예회장.',\n",
              " '그는 “질병과 가난이 악순환되는 고리를 끊기 위해 아산재단과 서울아산병원을 설립했던 아버님의 뜻을 이어 우리 사회의 어려운 이웃을 돕는 데 보탬이 되기를 바란다”고 기부 취지를 밝혔다.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGheAmmIkVQ4"
      },
      "source": [
        "\n",
        "import re\n",
        "import sys\n",
        "import io\n",
        "\n",
        "#텍스트 정제(전처리)\n",
        "def cleanText(readData):\n",
        "    #텍스트에 포함되어 있는 특수 문자 제거\n",
        "    text = re.sub('[-=+#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》◆◇●🎧○▲\\t―△━▷]', '', readData)\n",
        "    return text"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb6oBQFrkV_3",
        "outputId": "688560b1-4287-484b-8d3b-78d6df5423af"
      },
      "source": [
        "%%time\n",
        "c_sentences = []\n",
        "for sentence in sentences:\n",
        "    s = cleanText(sentence)\n",
        "    c = len(s.split())\n",
        "    if c >= 3 and c < 10 and s.find('재배포') < 0 and s.find('기자') < 0  and s.find('유투브') < 0 and s.find('www') < 0 and s.find('com') < 0 and s.find('접속하기') < 0 and s.find('http') < 0 and s.find('뉴스') < 0 and s.find('일보') < 0 :\n",
        "        if s.endswith(('다','요')):\n",
        "            c_sentences.append(s.strip())\n",
        "\n",
        "\n",
        "# 한국어 영화 리뷰를 Load.\n",
        "ds = pd.read_csv(\"https://raw.githubusercontent.com/dolmani38/similarly/master/ratings_test.txt\",sep='\\t')\n",
        "for row in ds.iterrows():\n",
        "    doc_id = row[1][0]\n",
        "    doc_cont = str(row[1][1])\n",
        "    c_sentences.append(doc_cont)\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "코퍼스 평균/총 단어 갯수 : 6.6 / 6029075\n",
            "CPU times: user 17.2 s, sys: 155 ms, total: 17.3 s\n",
            "Wall time: 17.6 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDANhUJ9qDel"
      },
      "source": [
        "import random\n",
        "\n",
        "random.shuffle(c_sentences)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugNi7PH5qtw2"
      },
      "source": [
        "### 한국어... 교착어 처리..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMRu1dDHqre-",
        "outputId": "e615fd1f-0d60-46b2-b815-a98a7ab0b5b2"
      },
      "source": [
        "%%time\n",
        "# mecab for window는 아래 코드 사용\n",
        "from konlpy.tag import Mecab  # install mecab for window: https://hong-yp-ml-records.tistory.com/91\n",
        "mecab_tokenizer = Mecab().morphs\n",
        "print('mecab check :', mecab_tokenizer('어릴때보고 지금다시봐도 재밌어요ㅋㅋ'))\n",
        "\n",
        "ko_sentences = []\n",
        "# 50만개의 문장만 사용...\n",
        "for sentence in c_sentences[:500000]:\n",
        "    # 문장단위 mecab 적용\n",
        "    morph_sentence= mecab_tokenizer(sentence)\n",
        "    # 문장단위 저장\n",
        "    ko_sentences.append( (' '.join(morph_sentence)).strip())\n",
        "        \n",
        "num_word_list = [len(sentence.split()) for sentence in ko_sentences]\n",
        "print('\\n힌국어 코퍼스 평균/총 단어 갯수 : %.1f / %d' % (sum(num_word_list)/len(num_word_list), sum(num_word_list)))       "
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mecab check : ['어릴', '때', '보', '고', '지금', '다시', '봐도', '재밌', '어요', 'ㅋㅋ']\n",
            "\n",
            "힌국어 코퍼스 평균/총 단어 갯수 : 13.2 / 6584031\n",
            "CPU times: user 40.2 s, sys: 192 ms, total: 40.4 s\n",
            "Wall time: 40.5 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sZ4yn-toGXw"
      },
      "source": [
        "### CNN/Daily main Dataset에서 영어... 문장 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 647,
          "referenced_widgets": [
            "e0a234238a20442188bffeb50b8a82e0",
            "508da04576294734b60f3fa90d40b2f5",
            "31dda3106cc647fda699a60bf34a320c",
            "7269bb94559540d8a2ad15890118c9a0",
            "4bbddc49854648ee809cab2744f19f03",
            "a6b91a068a2445da88b086a99123cca1",
            "a0c5df4e61074ded90aadad9d9123604",
            "ffaa2c46811545579645138631dfcfb7",
            "b07bff8f8c0242cb9d14c4e9200fe065",
            "c138a58915804d3a82cdab0711e6ee13",
            "b2f387970f2b453db3beaedc6aaea0a0",
            "10d6cdeac7824a1da0e7ee23a9857162",
            "8cde191a1d694cac973a9ff53c680c6f",
            "f715a113aec4444f91d26969ea994d30",
            "8fad692c0da84f67b2981b9dc5085798",
            "d94643c7f32d4c679260bb0abf305de3",
            "a94f18110c324dc780e714e50491e9fa",
            "be9e86e7e0b54aa1a8cceb4a7c44fbd9",
            "d6c60b10cea943e2b0fdba4f546ec849",
            "d8100564205c4989b0cc9512ffdef942",
            "83c6a479d4594efdaa7c8fdf758f86ef",
            "edb7645bd9cf4ebcb0472ab8413f15ec",
            "6dd9062c334a4755ba7b0a1bd86468ca",
            "0ecf99f564724d79b28843833d2e9c6b",
            "141567d835724105af5415594e9a7f9a",
            "2293ab6f8444436d826a597626e581f1",
            "e7df796c569f4432b158b7d360cea766",
            "dc0e96d7d05e4ed0ac616431ffc17104",
            "845e048e05db4a1ba2fbe08d84aefa40",
            "15172d9909fd47dc98f1666eb3ab3102",
            "058be0a299de4e408294b8551c55bc63",
            "6ecd6166ed4b41b9b09da5d04d26ada4"
          ]
        },
        "id": "gpv2vn9Bom28",
        "outputId": "c0512baf-6051-4174-90f4-ec6cbecbca8a"
      },
      "source": [
        "\n",
        "import tensorflow_datasets as tfds\n",
        "train_data, test_data = tfds.load(name=\"cnn_dailymail\",split=(tfds.Split.TRAIN,tfds.Split.TEST),with_info=True)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mDownloading and preparing dataset cnn_dailymail/plain_text/3.0.0 (download: 558.32 MiB, generated: 1.27 GiB, total: 1.82 GiB) to /root/tensorflow_datasets/cnn_dailymail/plain_text/3.0.0...\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e0a234238a20442188bffeb50b8a82e0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Completed...', max=1.0, style=Progre…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b07bff8f8c0242cb9d14c4e9200fe065",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Size...', max=1.0, style=ProgressSty…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a94f18110c324dc780e714e50491e9fa",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Extraction completed...', max=1.0, styl…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "141567d835724105af5415594e9a7f9a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-7ea54526221a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_datasets\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cnn_dailymail\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSplit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtfds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSplit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTEST\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwith_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/load.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, split, data_dir, batch_size, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)\u001b[0m\n\u001b[1;32m    342\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[0mdownload_and_prepare_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_and_prepare_kwargs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m     \u001b[0mdbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_and_prepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdownload_and_prepare_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mas_dataset_kwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/dataset_builder.py\u001b[0m in \u001b[0;36mdownload_and_prepare\u001b[0;34m(self, download_dir, download_config)\u001b[0m\n\u001b[1;32m    385\u001b[0m           self._download_and_prepare(\n\u001b[1;32m    386\u001b[0m               \u001b[0mdl_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdl_manager\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m               download_config=download_config)\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m           \u001b[0;31m# NOTE: If modifying the lines below to put additional information in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/dataset_builder.py\u001b[0m in \u001b[0;36m_download_and_prepare\u001b[0;34m(self, dl_manager, download_config)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     super(GeneratorBasedBuilder, self)._download_and_prepare(\n\u001b[1;32m   1023\u001b[0m         \u001b[0mdl_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdl_manager\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m         \u001b[0mmax_examples_per_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_examples_per_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m     )\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/dataset_builder.py\u001b[0m in \u001b[0;36m_download_and_prepare\u001b[0;34m(self, dl_manager, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m    972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m       \u001b[0;31m# Prepare split will record examples associated to the split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 974\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mprepare_split_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    975\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m     \u001b[0;31m# Update the info object with the splits.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/dataset_builder.py\u001b[0m in \u001b[0;36m_prepare_split\u001b[0;34m(self, split_generator, max_examples_per_split)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                      hash_salt=split_generator.name)\n\u001b[1;32m   1038\u001b[0m     for key, record in utils.tqdm(generator, unit=\" examples\",\n\u001b[0;32m-> 1039\u001b[0;31m                                   total=split_info.num_examples, leave=False):\n\u001b[0m\u001b[1;32m   1040\u001b[0m       \u001b[0mexample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[1;32m   1103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1104\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1105\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m             \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/summarization/cnn_dailymail.py\u001b[0m in \u001b[0;36m_generate_examples\u001b[0;34m(self, files)\u001b[0m\n\u001b[1;32m    306\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_generate_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m       \u001b[0marticle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhighlights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_art_abs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0marticle\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhighlights\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/summarization/cnn_dailymail.py\u001b[0m in \u001b[0;36m_get_art_abs\u001b[0;34m(story_file, tfds_version)\u001b[0m\n\u001b[1;32m    187\u001b[0m   \u001b[0;31m#     make_datafiles.py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m   \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_text_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstory_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m   \u001b[0;31m# The github code lowercase the text and we removed it in 3.0.0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/summarization/cnn_dailymail.py\u001b[0m in \u001b[0;36m_read_text_file\u001b[0;34m(text_file)\u001b[0m\n\u001b[1;32m    177\u001b[0m   \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m       \u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m     \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mreadline\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;34mr\"\"\"Reads the next line, keeping \\n. At EOF, returns ''.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preread_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_buf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xc7bauWlov2n"
      },
      "source": [
        "%%time\n",
        "en_sentences = []\n",
        "iterator = iter(test_data[0])\n",
        "for data in iterator:\n",
        "    article = nltk.sent_tokenize(data['article'].numpy().decode('UTF-8'))\n",
        "    for sent in article:    \n",
        "        en_sentences.append(sent)\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VhwqYk_o32m"
      },
      "source": [
        "num_word_list = [len(sentence.split()) for sentence in en_sentences]\n",
        "print('\\n영어 코퍼스 평균/총 단어 갯수 : %.1f / %d' % (sum(num_word_list)/len(num_word_list), sum(num_word_list)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3V3QEInEkhEl",
        "outputId": "899d9a85-fd37-466d-b87c-2999f9f7d720"
      },
      "source": [
        "%%time\n",
        "# subword 학습을 위해 문장만 따로 저장\n",
        "with open('/content/drive/MyDrive/Tokenizer_train/data/train_tokenizer.txt', 'w', encoding='utf-8') as f:\n",
        "    for line in ko_sentences:\n",
        "        f.write(line+'\\n')\n",
        "\n",
        "f.close()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 288 ms, sys: 40.3 ms, total: 328 ms\n",
            "Wall time: 1.35 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkA1gOfTRr6q"
      },
      "source": [
        "# pass"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m23ogFfIztOR",
        "outputId": "0fe5b2a5-fdc6-4160-a3c6-069f8907baa3"
      },
      "source": [
        "## check\n",
        "import sentencepiece as spm\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.Load('{}.model'.format(sp_model_path))\n",
        "\n",
        "tokens = sp.encode_as_pieces('나는 오늘 아침밥을 먹었다.')\n",
        "ids = sp.encode_as_ids('나는 오늘 아침밥을 먹었다.')\n",
        "\n",
        "print(ids)\n",
        "print(tokens)\n",
        "\n",
        "tokens = sp.decode_pieces(tokens)\n",
        "ids = sp.decode_ids(ids)\n",
        "\n",
        "print(ids)\n",
        "print(tokens)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[168, 459, 397, 1582, 2940, 2137, 487, 1789, 176, 17709]\n",
            "['▁나', '는', '▁오늘', '▁아침', '밥', '을', '▁먹', '었', '다', '.']\n",
            "나는 오늘 아침밥을 먹었다.\n",
            "나는 오늘 아침밥을 먹었다.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSNUOU1i0J2k",
        "outputId": "33abd03b-03ad-4e4d-fbb5-be697ebe32a1"
      },
      "source": [
        "!pip install transformers==4.4.1\n",
        "!pip install tokenizers==0.10.1"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers==4.4.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2c/d8/5144b0712f7f82229a8da5983a8fbb8d30cec5fbd5f8d12ffe1854dcea67/transformers-4.4.1-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 4.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.1) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.1) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.1) (2.23.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 21.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.1) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.1) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.1) (4.0.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.1) (20.9)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 38.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.4.1) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.4.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.4.1) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.4.1) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.4.1) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.4.1) (8.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.4.1) (1.15.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.4.1) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.4.1) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.4.1) (2.4.7)\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.4.1\n",
            "Collecting tokenizers==0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 4.1MB/s \n",
            "\u001b[?25hInstalling collected packages: tokenizers\n",
            "  Found existing installation: tokenizers 0.10.2\n",
            "    Uninstalling tokenizers-0.10.2:\n",
            "      Successfully uninstalled tokenizers-0.10.2\n",
            "Successfully installed tokenizers-0.10.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPYF4fkL1X2G"
      },
      "source": [
        "## mecab 설치 참조\n",
        "\n",
        "https://somjang.tistory.com/entry/Google-Colab%EC%97%90%EC%84%9C-Mecab-koMecab-ko-dic-%EC%89%BD%EA%B2%8C-%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEMkMuYe1mMy",
        "outputId": "24f5206b-52ea-4a58-8381-8f8454c68690"
      },
      "source": [
        "!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Mecab-ko-for-Google-Colab'...\n",
            "remote: Enumerating objects: 91, done.\u001b[K\n",
            "remote: Counting objects: 100% (91/91), done.\u001b[K\n",
            "remote: Compressing objects: 100% (85/85), done.\u001b[K\n",
            "remote: Total 91 (delta 43), reused 22 (delta 6), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (91/91), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9we-LODi1stV",
        "outputId": "c5568883-f3b6-4f20-88fe-b7784164ca18"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive  Mecab-ko-for-Google-Colab  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQS-tyl_115Q",
        "outputId": "95bf7cfb-b07f-41bd-da51-2d2263fa96b9"
      },
      "source": [
        "cd Mecab-ko-for-Google-Colab"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Mecab-ko-for-Google-Colab\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7zG-vWR18UD",
        "outputId": "18ef6572-5d85-4111-ab0d-56c7e609e5d6"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "images\t\t\t\t\t   LICENSE\n",
            "install_mecab-ko_on_colab190912.sh\t   README.md\n",
            "install_mecab-ko_on_colab_light_210108.sh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Myd_HnLC2TkH",
        "outputId": "e194847c-dca2-4552-8df9-a9470a0cb4a8"
      },
      "source": [
        "!bash ./Mecab-ko-for-Google-Colab/install_mecab-ko_on_colab190912.sh"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Installing konlpy.....\n",
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 4.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 10.3MB/s \n",
            "\u001b[?25hCollecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Collecting JPype1>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/a5/9781e2ef4ca92d09912c4794642c1653aea7607f473e156cf4d423a881a1/JPype1-1.2.1-cp37-cp37m-manylinux2010_x86_64.whl (457kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 35.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Installing collected packages: beautifulsoup4, colorama, JPype1, konlpy\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-1.2.1 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2\n",
            "Done\n",
            "Installing mecab-0.996-ko-0.9.2.tar.gz.....\n",
            "Downloading mecab-0.996-ko-0.9.2.tar.gz.......\n",
            "from https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz\n",
            "--2021-05-15 02:38:28--  https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz\n",
            "Resolving bitbucket.org (bitbucket.org)... 104.192.141.1, 2406:da00:ff00::6b17:d1f5, 2406:da00:ff00::22c2:513, ...\n",
            "Connecting to bitbucket.org (bitbucket.org)|104.192.141.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://bbuseruploads.s3.amazonaws.com/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz?Signature=3MsI8SU4yWB1Uvr8Nz7Jp%2F6txLI%3D&Expires=1621047452&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=null&response-content-disposition=attachment%3B%20filename%3D%22mecab-0.996-ko-0.9.2.tar.gz%22&response-content-encoding=None [following]\n",
            "--2021-05-15 02:38:29--  https://bbuseruploads.s3.amazonaws.com/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz?Signature=3MsI8SU4yWB1Uvr8Nz7Jp%2F6txLI%3D&Expires=1621047452&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=null&response-content-disposition=attachment%3B%20filename%3D%22mecab-0.996-ko-0.9.2.tar.gz%22&response-content-encoding=None\n",
            "Resolving bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)... 52.216.20.203\n",
            "Connecting to bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)|52.216.20.203|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1414979 (1.3M) [application/x-tar]\n",
            "Saving to: ‘mecab-0.996-ko-0.9.2.tar.gz’\n",
            "\n",
            "mecab-0.996-ko-0.9. 100%[===================>]   1.35M  1.08MB/s    in 1.3s    \n",
            "\n",
            "2021-05-15 02:38:31 (1.08 MB/s) - ‘mecab-0.996-ko-0.9.2.tar.gz’ saved [1414979/1414979]\n",
            "\n",
            "Done\n",
            "Unpacking mecab-0.996-ko-0.9.2.tar.gz.......\n",
            "Done\n",
            "Change Directory to mecab-0.996-ko-0.9.2.......\n",
            "installing mecab-0.996-ko-0.9.2.tar.gz........\n",
            "configure\n",
            "make\n",
            "make check\n",
            "make install\n",
            "ldconfig\n",
            "Done\n",
            "Change Directory to /content\n",
            "Downloading mecab-ko-dic-2.1.1-20180720.tar.gz.......\n",
            "from https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz\n",
            "--2021-05-15 02:40:01--  https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz\n",
            "Resolving bitbucket.org (bitbucket.org)... 104.192.141.1, 2406:da00:ff00::6b17:d1f5, 2406:da00:ff00::22cd:e0db, ...\n",
            "Connecting to bitbucket.org (bitbucket.org)|104.192.141.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://bbuseruploads.s3.amazonaws.com/a4fcd83e-34f1-454e-a6ac-c242c7d434d3/downloads/b5a0c703-7b64-45ed-a2d7-180e962710b6/mecab-ko-dic-2.1.1-20180720.tar.gz?Signature=zT%2B5khLv3A4fh9LrGoc2lUtGAus%3D&Expires=1621047547&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=tzyxc1TtnZU_zEuaaQDGN4F76hPDpyFq&response-content-disposition=attachment%3B%20filename%3D%22mecab-ko-dic-2.1.1-20180720.tar.gz%22&response-content-encoding=None [following]\n",
            "--2021-05-15 02:40:02--  https://bbuseruploads.s3.amazonaws.com/a4fcd83e-34f1-454e-a6ac-c242c7d434d3/downloads/b5a0c703-7b64-45ed-a2d7-180e962710b6/mecab-ko-dic-2.1.1-20180720.tar.gz?Signature=zT%2B5khLv3A4fh9LrGoc2lUtGAus%3D&Expires=1621047547&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=tzyxc1TtnZU_zEuaaQDGN4F76hPDpyFq&response-content-disposition=attachment%3B%20filename%3D%22mecab-ko-dic-2.1.1-20180720.tar.gz%22&response-content-encoding=None\n",
            "Resolving bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)... 52.217.168.73\n",
            "Connecting to bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)|52.217.168.73|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 49775061 (47M) [application/x-tar]\n",
            "Saving to: ‘mecab-ko-dic-2.1.1-20180720.tar.gz’\n",
            "\n",
            "mecab-ko-dic-2.1.1- 100%[===================>]  47.47M  12.4MB/s    in 5.0s    \n",
            "\n",
            "2021-05-15 02:40:08 (9.50 MB/s) - ‘mecab-ko-dic-2.1.1-20180720.tar.gz’ saved [49775061/49775061]\n",
            "\n",
            "Done\n",
            "Unpacking  mecab-ko-dic-2.1.1-20180720.tar.gz.......\n",
            "Done\n",
            "Change Directory to mecab-ko-dic-2.1.1-20180720\n",
            "Done\n",
            "installing........\n",
            "configure\n",
            "make\n",
            "make install\n",
            "apt-get update\n",
            "apt-get upgrade\n",
            "apt install curl\n",
            "apt install git\n",
            "bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\n",
            "Done\n",
            "Successfully Installed\n",
            "Now you can use Mecab\n",
            "from konlpy.tag import Mecab\n",
            "mecab = Mecab()\n",
            "사용자 사전 추가 방법 : https://bit.ly/3k0ZH53\n",
            "NameError: name 'Tagger' is not defined 오류 발생 시 런타임을 재실행 해주세요\n",
            "블로그에 해결 방법을 남겨주신 tana님 감사합니다.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAan_c2I49-S",
        "outputId": "b7e536ef-cb8e-42bb-afd8-6c7a9ebb47a5"
      },
      "source": [
        "cd .."
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dq7J8XOV4Ic-"
      },
      "source": [
        "from konlpy.tag import Mecab\n",
        "mecab = Mecab()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1ia4Y8E0Nw7",
        "outputId": "aa7aaf41-f539-430e-d558-9cceb812b15d"
      },
      "source": [
        "%%time\n",
        "\n",
        "# load korean corpus for tokenizer training\n",
        "with open(input_file, 'r', encoding='utf-8') as f:\n",
        "    data = f.read().split('\\n')\n",
        "print(data[:3])\n",
        "\n",
        "# mecab for window는 아래 코드 사용\n",
        "from konlpy.tag import Mecab  # install mecab for window: https://hong-yp-ml-records.tistory.com/91\n",
        "mecab_tokenizer = Mecab().morphs\n",
        "print('mecab check :', mecab_tokenizer('어릴때보고 지금다시봐도 재밌어요ㅋㅋ'))\n",
        "\n",
        "for_generation = False # or normal\n",
        "\n",
        "if for_generation:\n",
        "    # 1: '어릴때' -> '어릴, ##때' for generation model\n",
        "    total_morph=[]\n",
        "    for sentence in data:\n",
        "        # 문장단위 mecab 적용\n",
        "        morph_sentence= []\n",
        "        count = 0\n",
        "        for token_mecab in mecab_tokenizer(sentence):\n",
        "            token_mecab_save = token_mecab\n",
        "            if count > 0:\n",
        "                token_mecab_save = \"##\" + token_mecab_save  # 앞에 ##를 부친다\n",
        "                morph_sentence.append(token_mecab_save)\n",
        "            else:\n",
        "                morph_sentence.append(token_mecab_save)\n",
        "                count += 1\n",
        "        # 문장단위 저장\n",
        "        total_morph.append(morph_sentence)\n",
        "\n",
        "else:\n",
        "    # 2: '어릴때' -> '어릴, 때'   for normal case\n",
        "    total_morph=[]\n",
        "    for sentence in data:\n",
        "        # 문장단위 mecab 적용\n",
        "        morph_sentence= mecab_tokenizer(sentence)\n",
        "        # 문장단위 저장\n",
        "        total_morph.append(morph_sentence)\n",
        "                        \n",
        "print(total_morph[:3])\n",
        "print(len(total_morph))\n",
        "\n",
        "# mecab 적용한 데이터 저장\n",
        "# ex) 1 line: '어릴 때 보 고 지금 다시 봐도 재밌 어요 ㅋㅋ'\n",
        "with open('/content/drive/MyDrive/Tokenizer_train/data/after_mecab.txt', 'w', encoding='utf-8') as f:\n",
        "    for line in total_morph:\n",
        "        f.write(' '.join(line)+'\\n')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['굳 ㅋ', 'GDNTOPCLASSINTHECLUB', '뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아']\n",
            "mecab check : ['어릴', '때', '보', '고', '지금', '다시', '봐도', '재밌', '어요', 'ㅋㅋ']\n",
            "[['굳', 'ㅋ'], ['GDNTOPCLASSINTHECLUB'], ['뭐', '야', '이', '평점', '들', '은', '.', '...', '나쁘', '진', '않', '지만', '10', '점', '짜리', '는', '더더욱', '아니', '잖아']]\n",
            "49998\n",
            "CPU times: user 5.15 s, sys: 244 ms, total: 5.4 s\n",
            "Wall time: 6.19 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jr_uwLua66Dj",
        "outputId": "ee88d651-170c-4200-8497-d5d598276ef9"
      },
      "source": [
        "## 1) define special tokens\n",
        "user_defined_symbols = ['[BOS]','[EOS]','[UNK0]','[UNK1]','[UNK2]','[UNK3]','[UNK4]','[UNK5]','[UNK6]','[UNK7]','[UNK8]','[UNK9]']\n",
        "unused_token_num = 200\n",
        "unused_list = ['[unused{}]'.format(n) for n in range(unused_token_num)]\n",
        "user_defined_symbols = user_defined_symbols + unused_list\n",
        "\n",
        "print(user_defined_symbols)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[BOS]', '[EOS]', '[UNK0]', '[UNK1]', '[UNK2]', '[UNK3]', '[UNK4]', '[UNK5]', '[UNK6]', '[UNK7]', '[UNK8]', '[UNK9]', '[unused0]', '[unused1]', '[unused2]', '[unused3]', '[unused4]', '[unused5]', '[unused6]', '[unused7]', '[unused8]', '[unused9]', '[unused10]', '[unused11]', '[unused12]', '[unused13]', '[unused14]', '[unused15]', '[unused16]', '[unused17]', '[unused18]', '[unused19]', '[unused20]', '[unused21]', '[unused22]', '[unused23]', '[unused24]', '[unused25]', '[unused26]', '[unused27]', '[unused28]', '[unused29]', '[unused30]', '[unused31]', '[unused32]', '[unused33]', '[unused34]', '[unused35]', '[unused36]', '[unused37]', '[unused38]', '[unused39]', '[unused40]', '[unused41]', '[unused42]', '[unused43]', '[unused44]', '[unused45]', '[unused46]', '[unused47]', '[unused48]', '[unused49]', '[unused50]', '[unused51]', '[unused52]', '[unused53]', '[unused54]', '[unused55]', '[unused56]', '[unused57]', '[unused58]', '[unused59]', '[unused60]', '[unused61]', '[unused62]', '[unused63]', '[unused64]', '[unused65]', '[unused66]', '[unused67]', '[unused68]', '[unused69]', '[unused70]', '[unused71]', '[unused72]', '[unused73]', '[unused74]', '[unused75]', '[unused76]', '[unused77]', '[unused78]', '[unused79]', '[unused80]', '[unused81]', '[unused82]', '[unused83]', '[unused84]', '[unused85]', '[unused86]', '[unused87]', '[unused88]', '[unused89]', '[unused90]', '[unused91]', '[unused92]', '[unused93]', '[unused94]', '[unused95]', '[unused96]', '[unused97]', '[unused98]', '[unused99]', '[unused100]', '[unused101]', '[unused102]', '[unused103]', '[unused104]', '[unused105]', '[unused106]', '[unused107]', '[unused108]', '[unused109]', '[unused110]', '[unused111]', '[unused112]', '[unused113]', '[unused114]', '[unused115]', '[unused116]', '[unused117]', '[unused118]', '[unused119]', '[unused120]', '[unused121]', '[unused122]', '[unused123]', '[unused124]', '[unused125]', '[unused126]', '[unused127]', '[unused128]', '[unused129]', '[unused130]', '[unused131]', '[unused132]', '[unused133]', '[unused134]', '[unused135]', '[unused136]', '[unused137]', '[unused138]', '[unused139]', '[unused140]', '[unused141]', '[unused142]', '[unused143]', '[unused144]', '[unused145]', '[unused146]', '[unused147]', '[unused148]', '[unused149]', '[unused150]', '[unused151]', '[unused152]', '[unused153]', '[unused154]', '[unused155]', '[unused156]', '[unused157]', '[unused158]', '[unused159]', '[unused160]', '[unused161]', '[unused162]', '[unused163]', '[unused164]', '[unused165]', '[unused166]', '[unused167]', '[unused168]', '[unused169]', '[unused170]', '[unused171]', '[unused172]', '[unused173]', '[unused174]', '[unused175]', '[unused176]', '[unused177]', '[unused178]', '[unused179]', '[unused180]', '[unused181]', '[unused182]', '[unused183]', '[unused184]', '[unused185]', '[unused186]', '[unused187]', '[unused188]', '[unused189]', '[unused190]', '[unused191]', '[unused192]', '[unused193]', '[unused194]', '[unused195]', '[unused196]', '[unused197]', '[unused198]', '[unused199]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BrNgVd067cd",
        "outputId": "3bdf23cd-262b-489f-aace-8b6de97262dd"
      },
      "source": [
        "%%time\n",
        "## 2) train\n",
        "import os\n",
        "from tokenizers import BertWordPieceTokenizer, SentencePieceBPETokenizer, CharBPETokenizer, ByteLevelBPETokenizer\n",
        "\n",
        "# 4가지중 tokenizer 선택\n",
        "# how_to_tokenize = BertWordPieceTokenizer  # The famous Bert tokenizer, using WordPiece\n",
        "# how_to_tokenize = SentencePieceBPETokenizer  # A BPE implementation compatible with the one used by SentencePiece\n",
        "# how_to_tokenize = CharBPETokenizer  # The original BPE\n",
        "# how_to_tokenize = ByteLevelBPETokenizer  # The byte level version of the BPE\n",
        "#tokenizer = BertWordPieceTokenizer(strip_accents=False,  # Must be False if cased model\n",
        "#                                       lowercase=False)\n",
        "\n",
        "\n",
        "tokenizer = BertWordPieceTokenizer()\n",
        "\n",
        "corpus_file   = ['/content/drive/MyDrive/Tokenizer_train/data/after_mecab.txt']  # data path\n",
        "vocab_size    = 32000\n",
        "limit_alphabet= 6000\n",
        "#output_path   = '/content/drive/MyDrive/Tokenizer_train/hugging_%d'%(vocab_size)\n",
        "min_frequency = 5\n",
        "\n",
        "# Then train it!\n",
        "tokenizer.train(files=corpus_file,\n",
        "               vocab_size=vocab_size,\n",
        "               min_frequency=min_frequency,  # 단어의 최소 발생 빈도, 5\n",
        "               limit_alphabet=limit_alphabet,  # ByteLevelBPETokenizer 학습시엔 주석처리 필요\n",
        "               show_progress=True)\n",
        "print('train complete')\n",
        "\n",
        "sentence = '나는 오늘 아침밥을 먹었다.'\n",
        "output = tokenizer.encode(sentence)\n",
        "print(sentence)\n",
        "print('=>idx   : %s'%output.ids)\n",
        "print('=>tokens: %s'%output.tokens)\n",
        "print('=>offset: %s'%output.offsets)\n",
        "print('=>decode: %s\\n'%tokenizer.decode(output.ids))\n",
        "\n",
        "sentence = 'I want to go my hometown'\n",
        "output = tokenizer.encode(sentence)\n",
        "print(sentence)\n",
        "print('=>idx   : %s'%output.ids)\n",
        "print('=>tokens: %s'%output.tokens)\n",
        "print('=>offset: %s'%output.offsets)\n",
        "print('=>decode: %s\\n'%tokenizer.decode(output.ids))\n",
        "\n",
        "# save tokenizer\n",
        "hf_model_path='/content/drive/MyDrive/Tokenizer_train/tokenizer_model'\n",
        "if not os.path.isdir(hf_model_path):\n",
        "    os.mkdir(hf_model_path)\n",
        "tokenizer.save_model(hf_model_path)  # vocab.txt 파일 한개가 만들어진다"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train complete\n",
            "나는 오늘 아침밥을 먹었다.\n",
            "=>idx   : [4768, 539, 1658, 2742, 3950, 1828, 1167, 1324, 566, 18]\n",
            "=>tokens: ['나ᄂ', '##ᅳᆫ', '오늘', '아침', '##밥', '##을', '먹', '##었', '##다', '.']\n",
            "=>offset: [(0, 2), (1, 2), (3, 5), (6, 8), (8, 9), (9, 10), (11, 12), (12, 13), (13, 14), (14, 15)]\n",
            "=>decode: 나는 오늘 아침밥을 먹었다.\n",
            "\n",
            "I want to go my hometown\n",
            "=>idx   : [51, 65, 10620, 3576, 6994, 5482, 50, 4275, 418, 414, 4438, 419]\n",
            "=>tokens: ['i', 'w', '##ant', 'to', 'go', 'my', 'h', '##om', '##e', '##t', '##ow', '##n']\n",
            "=>offset: [(0, 1), (2, 3), (3, 6), (7, 9), (10, 12), (13, 15), (16, 17), (17, 19), (19, 20), (20, 21), (21, 23), (23, 24)]\n",
            "=>decode: i want to go my hometown\n",
            "\n",
            "CPU times: user 5.1 s, sys: 267 ms, total: 5.37 s\n",
            "Wall time: 2.94 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Z6jIMEkX8UAB",
        "outputId": "91389575-b571-4069-b6ba-07aa45154e38"
      },
      "source": [
        "from transformers import BertTokenizerFast, AlbertTokenizerFast\n",
        "\n",
        "tokenizer_for_load = AlbertTokenizerFast.from_pretrained(hf_model_path,\n",
        "                                                       strip_accents=False,  # Must be False if cased model\n",
        "                                                       lowercase=False)  # 로드\n",
        "\n",
        "print('vocab size : %d' % tokenizer_for_load.vocab_size)\n",
        "# tokenized_input_for_pytorch = tokenizer_for_load(\"i am very hungry\", return_tensors=\"pt\")\n",
        "tokenized_input_for_pytorch = tokenizer_for_load(\"나는 오늘 아침밥을 먹었다.\", return_tensors=\"pt\")\n",
        "tokenized_input_for_tensorflow = tokenizer_for_load(\"나는 오늘 아침밥을 먹었다.\", return_tensors=\"tf\")\n",
        "\n",
        "print(\"Tokens (str)      : {}\".format([tokenizer_for_load.convert_ids_to_tokens(s) for s in tokenized_input_for_pytorch['input_ids'].tolist()[0]]))\n",
        "print(\"Tokens (int)      : {}\".format(tokenized_input_for_pytorch['input_ids'].tolist()[0]))\n",
        "print(\"Tokens (attn_mask): {}\\n\".format(tokenized_input_for_pytorch['attention_mask'].tolist()[0]))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-071d2ecc36f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m tokenizer_for_load = AlbertTokenizerFast.from_pretrained(hf_model_path,\n\u001b[1;32m      4\u001b[0m                                                        \u001b[0mstrip_accents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Must be False if cased model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                                                        lowercase=False)  # 로드\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vocab size : %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtokenizer_for_load\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1691\u001b[0m                 \u001b[0;34mf\"- or '{pretrained_model_name_or_path}' is the correct path to a directory containing relevant tokenizer files\\n\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1692\u001b[0m             )\n\u001b[0;32m-> 1693\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1695\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfile_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvocab_files\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for '/content/drive/MyDrive/Tokenizer_train/tokenizer_model'. Make sure that:\n\n- '/content/drive/MyDrive/Tokenizer_train/tokenizer_model' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or '/content/drive/MyDrive/Tokenizer_train/tokenizer_model' is the correct path to a directory containing relevant tokenizer files\n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzGw-0CB8lmK",
        "outputId": "b4563c37-7442-4d40-8ef5-88d6db0f86f8"
      },
      "source": [
        "# special token check\n",
        "tokenizer_for_load.all_special_tokens # 추가하기 전 기본적인 special token"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBUrh8sM8xuX",
        "outputId": "6d630fd3-3da0-4b21-aeb4-f10f01b2e84e"
      },
      "source": [
        "# tokenizer에 special token 추가\n",
        "special_tokens_dict = {'additional_special_tokens': user_defined_symbols}\n",
        "tokenizer_for_load.add_special_tokens(special_tokens_dict)\n",
        "\n",
        "# check tokenizer vocab with special tokens\n",
        "print('check special tokens : %s'%tokenizer_for_load.all_special_tokens[:20])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "check special tokens : ['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]', '[BOS]', '[EOS]', '[UNK0]', '[UNK1]', '[UNK2]', '[UNK3]', '[UNK4]', '[UNK5]', '[UNK6]', '[UNK7]', '[UNK8]', '[UNK9]', '[unused0]', '[unused1]', '[unused2]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYV69C2e9DfA",
        "outputId": "512a7131-96b3-4baf-9fb7-24ddbf999b01"
      },
      "source": [
        "# save tokenizer model with special tokens\n",
        "tokenizer_for_load.save_pretrained(hf_model_path+'_special')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/Tokenizer_train/tokenizer_model_special/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/Tokenizer_train/tokenizer_model_special/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/Tokenizer_train/tokenizer_model_special/vocab.txt',\n",
              " '/content/drive/MyDrive/Tokenizer_train/tokenizer_model_special/added_tokens.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LE6N8QdS9Nqk",
        "outputId": "c2e8f1f0-d524-423e-9bb3-32ad0a4e2918"
      },
      "source": [
        "# check special tokens\n",
        "from transformers import BertTokenizerFast, AlbertTokenizerFast\n",
        "tokenizer_check = BertTokenizerFast.from_pretrained(hf_model_path+'_special')\n",
        "\n",
        "print('check special tokens : %s'%tokenizer_check.all_special_tokens[:20])\n",
        "\n",
        "print('vocab size : %d' % tokenizer_check.vocab_size)\n",
        "tokenized_input_for_pytorch = tokenizer_check(\"나는 오늘 아침밥을 먹었다.\", return_tensors=\"pt\")\n",
        "tokenized_input_for_tensorflow = tokenizer_check(\"나는 오늘 아침밥을 먹었다.\", return_tensors=\"tf\")\n",
        "\n",
        "print(\"Tokens (str)      : {}\".format([tokenizer_check.convert_ids_to_tokens(s) for s in tokenized_input_for_pytorch['input_ids'].tolist()[0]]))\n",
        "print(\"Tokens (int)      : {}\".format(tokenized_input_for_pytorch['input_ids'].tolist()[0]))\n",
        "print(\"Tokens (attn_mask): {}\\n\".format(tokenized_input_for_pytorch['attention_mask'].tolist()[0]))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "check special tokens : ['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]', '[BOS]', '[EOS]', '[UNK0]', '[UNK1]', '[UNK2]', '[UNK3]', '[UNK4]', '[UNK5]', '[UNK6]', '[UNK7]', '[UNK8]', '[UNK9]', '[unused0]', '[unused1]', '[unused2]']\n",
            "vocab size : 13201\n",
            "Tokens (str)      : ['[CLS]', '나', '##는', '오늘', '아침', '##밥', '##을', '먹', '##었', '##다', '.', '[SEP]']\n",
            "Tokens (int)      : [2, 555, 2425, 4213, 4895, 2793, 2639, 1014, 2521, 2426, 18, 3]\n",
            "Tokens (attn_mask): [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0,
          "referenced_widgets": [
            "aa2bb3c703cd47bbae4c3725ebbe8607",
            "6d0cb13c40b94435b8cc28c53e04be9f",
            "a82eba20d147451fae82808feebe2ee6",
            "f8338f63e39e4297960120064bcd6cfa",
            "3c771b594639409980363ad629509820",
            "97618f3b65be4d0dbc736b7f5d57bbbc",
            "aeeb3800b6e54c43947521d2b1db8bac",
            "6765b2b669cb4ccc9b95023fd76b36b9",
            "a5f744a372114cd1aa1b205d7fe38a78",
            "bd8b2b2d73e1470ba0c082828d499ef3",
            "6c79c2c1bee44c5487cdd7d54e4974ca",
            "1b0c3cf5a34d4a23a50378e3111fbaab",
            "0cd1019099024c8bb465eb84371dd313",
            "266830fdd6bf499698715a415a909773",
            "378f1fde5736470094c345f994541a65",
            "17250d596553432d9a0c313cd3be22e8",
            "ad87e8b4d9dd4c50a623bc0c5b3231b9",
            "6ee83dcf024a4fc59dedf8ed6793c961",
            "ba3a7416f39b48ad98221f3445345c39",
            "23a939dca94b4400a25b84a8f17feeca",
            "03d48499e49a4b1588cf99006f1ee2ca",
            "8d0c6df303274deb900ec2e4e60645fc",
            "66de0432831d40a9b58728f36d6469e7",
            "86ccde2a1f0f44ea9c079baa0d7ccc88"
          ]
        },
        "id": "S7CzjOER-SbQ",
        "outputId": "ce743ac8-13bb-4ba8-cbfb-21960275f140"
      },
      "source": [
        "# test to tf&pytorch bert model\n",
        "from transformers import TFBertModel, BertModel\n",
        "\n",
        "# load a BERT model for TensorFlow and PyTorch\n",
        "model_tf = TFBertModel.from_pretrained('bert-base-cased')\n",
        "model_pt = BertModel.from_pretrained('bert-base-cased')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aa2bb3c703cd47bbae4c3725ebbe8607",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=570.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a5f744a372114cd1aa1b205d7fe38a78",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=526681800.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ad87e8b4d9dd4c50a623bc0c5b3231b9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=435779157.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYeMzWLq-aAc",
        "outputId": "9b385d3c-6a30-4ebd-c378-d5bf4c8613fe"
      },
      "source": [
        "## tf vs torch bert output\n",
        "# transformers generates a ready to use dictionary with all the required parameters for the specific framework.\n",
        "input_tf = tokenizer_check(\"나는 오늘 아침밥을 먹었다.\", return_tensors=\"tf\")\n",
        "input_pt = tokenizer_check(\"나는 오늘 아침밥을 먹었다.\", return_tensors=\"pt\")\n",
        "\n",
        "# Let's compare the outputs\n",
        "output_tf, output_pt = model_tf(input_tf), model_pt(**input_pt)\n",
        "\n",
        "print('final layer output shape : %s'%(output_pt['last_hidden_state'].shape,))\n",
        "\n",
        "# Models outputs 2 values (The value for each tokens, the pooled representation of the input sentence)\n",
        "# Here we compare the output differences between PyTorch and TensorFlow.\n",
        "\n",
        "print('\\ntorch vs tf 결과차이')\n",
        "for name in [\"last_hidden_state\", \"pooler_output\"]:\n",
        "    print(\"   => {} differences: {:.5}\".format(name, (output_tf[name].numpy() - output_pt[name].detach().numpy()).sum()))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "final layer output shape : torch.Size([1, 12, 768])\n",
            "\n",
            "torch vs tf 결과차이\n",
            "   => last_hidden_state differences: 2.5626e-06\n",
            "   => pooler_output differences: -3.0823e-06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNXWYH5FRyu1"
      },
      "source": [
        "# 여기서부터 다시"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-ircJrdZKh6",
        "outputId": "8dc8f79e-559a-4f93-d6e0-aabed99e3aac"
      },
      "source": [
        "%%time\n",
        "import sentencepiece as spm\n",
        "import os\n",
        "# spm_train --input=data/train_tokenizer.txt  --model_prefix=sentencepiece/sp --vocab_size=32000 character_coverage=1.0 --model_type=\"unigram\"\n",
        "\n",
        "input_file = '/content/drive/MyDrive/Tokenizer_train/data/train_tokenizer.txt'\n",
        "vocab_size = 32000\n",
        "\n",
        "sp_model_root='/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model'\n",
        "if not os.path.isdir(sp_model_root):\n",
        "    os.mkdir(sp_model_root)\n",
        "sp_model_name = 'spiece'\n",
        "sp_model_path = os.path.join(sp_model_root, sp_model_name)\n",
        "model_type = 'unigram'  # 학습할 모델 선택, unigram이 더 성능이 좋음'bpe'\n",
        "character_coverage  = 0.9995  # 전체를 cover 하기 위해, default=0.9995\n",
        "user_defined_symbols = '[PAD],[UNK],[CLS],[SEP],[MASK],[BOS],[EOS],[UNK0],[UNK1],[UNK2],[UNK3],[UNK4],[UNK5],[UNK6],[UNK7],[UNK8],[UNK9],[unused0],[unused1],[unused2],[unused3],[unused4],[unused5],[unused6],[unused7],[unused8],[unused9],[unused10],[unused11],[unused12],[unused13],[unused14],[unused15],[unused16],[unused17],[unused18],[unused19],[unused20],[unused21],[unused22],[unused23],[unused24],[unused25],[unused26],[unused27],[unused28],[unused29],[unused30],[unused31],[unused32],[unused33],[unused34],[unused35],[unused36],[unused37],[unused38],[unused39],[unused40],[unused41],[unused42],[unused43],[unused44],[unused45],[unused46],[unused47],[unused48],[unused49],[unused50],[unused51],[unused52],[unused53],[unused54],[unused55],[unused56],[unused57],[unused58],[unused59],[unused60],[unused61],[unused62],[unused63],[unused64],[unused65],[unused66],[unused67],[unused68],[unused69],[unused70],[unused71],[unused72],[unused73],[unused74],[unused75],[unused76],[unused77],[unused78],[unused79],[unused80],[unused81],[unused82],[unused83],[unused84],[unused85],[unused86],[unused87],[unused88],[unused89],[unused90],[unused91],[unused92],[unused93],[unused94],[unused95],[unused96],[unused97],[unused98],[unused99]'\n",
        "\n",
        "input_argument = '--input=%s --model_prefix=%s --vocab_size=%s --user_defined_symbols=%s --model_type=%s --character_coverage=%s'\n",
        "cmd = input_argument%(input_file, sp_model_path, vocab_size,user_defined_symbols, model_type, character_coverage)\n",
        "\n",
        "spm.SentencePieceTrainer.Train(cmd)\n",
        "print('train done')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train done\n",
            "CPU times: user 14.5 s, sys: 366 ms, total: 14.9 s\n",
            "Wall time: 15.2 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55T0J0MDFtrm",
        "outputId": "811f5aa7-c871-43e6-ed4c-fc995518021d"
      },
      "source": [
        "## 1) define special tokens\n",
        "user_defined_symbols = ['[BOS]','[EOS]','[UNK0]','[UNK1]','[UNK2]','[UNK3]','[UNK4]','[UNK5]','[UNK6]','[UNK7]','[UNK8]','[UNK9]']\n",
        "unused_token_num = 200\n",
        "unused_list = ['[unused{}]'.format(n) for n in range(unused_token_num)]\n",
        "user_defined_symbols = user_defined_symbols + unused_list\n",
        "\n",
        "print(user_defined_symbols)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[BOS]', '[EOS]', '[UNK0]', '[UNK1]', '[UNK2]', '[UNK3]', '[UNK4]', '[UNK5]', '[UNK6]', '[UNK7]', '[UNK8]', '[UNK9]', '[unused0]', '[unused1]', '[unused2]', '[unused3]', '[unused4]', '[unused5]', '[unused6]', '[unused7]', '[unused8]', '[unused9]', '[unused10]', '[unused11]', '[unused12]', '[unused13]', '[unused14]', '[unused15]', '[unused16]', '[unused17]', '[unused18]', '[unused19]', '[unused20]', '[unused21]', '[unused22]', '[unused23]', '[unused24]', '[unused25]', '[unused26]', '[unused27]', '[unused28]', '[unused29]', '[unused30]', '[unused31]', '[unused32]', '[unused33]', '[unused34]', '[unused35]', '[unused36]', '[unused37]', '[unused38]', '[unused39]', '[unused40]', '[unused41]', '[unused42]', '[unused43]', '[unused44]', '[unused45]', '[unused46]', '[unused47]', '[unused48]', '[unused49]', '[unused50]', '[unused51]', '[unused52]', '[unused53]', '[unused54]', '[unused55]', '[unused56]', '[unused57]', '[unused58]', '[unused59]', '[unused60]', '[unused61]', '[unused62]', '[unused63]', '[unused64]', '[unused65]', '[unused66]', '[unused67]', '[unused68]', '[unused69]', '[unused70]', '[unused71]', '[unused72]', '[unused73]', '[unused74]', '[unused75]', '[unused76]', '[unused77]', '[unused78]', '[unused79]', '[unused80]', '[unused81]', '[unused82]', '[unused83]', '[unused84]', '[unused85]', '[unused86]', '[unused87]', '[unused88]', '[unused89]', '[unused90]', '[unused91]', '[unused92]', '[unused93]', '[unused94]', '[unused95]', '[unused96]', '[unused97]', '[unused98]', '[unused99]', '[unused100]', '[unused101]', '[unused102]', '[unused103]', '[unused104]', '[unused105]', '[unused106]', '[unused107]', '[unused108]', '[unused109]', '[unused110]', '[unused111]', '[unused112]', '[unused113]', '[unused114]', '[unused115]', '[unused116]', '[unused117]', '[unused118]', '[unused119]', '[unused120]', '[unused121]', '[unused122]', '[unused123]', '[unused124]', '[unused125]', '[unused126]', '[unused127]', '[unused128]', '[unused129]', '[unused130]', '[unused131]', '[unused132]', '[unused133]', '[unused134]', '[unused135]', '[unused136]', '[unused137]', '[unused138]', '[unused139]', '[unused140]', '[unused141]', '[unused142]', '[unused143]', '[unused144]', '[unused145]', '[unused146]', '[unused147]', '[unused148]', '[unused149]', '[unused150]', '[unused151]', '[unused152]', '[unused153]', '[unused154]', '[unused155]', '[unused156]', '[unused157]', '[unused158]', '[unused159]', '[unused160]', '[unused161]', '[unused162]', '[unused163]', '[unused164]', '[unused165]', '[unused166]', '[unused167]', '[unused168]', '[unused169]', '[unused170]', '[unused171]', '[unused172]', '[unused173]', '[unused174]', '[unused175]', '[unused176]', '[unused177]', '[unused178]', '[unused179]', '[unused180]', '[unused181]', '[unused182]', '[unused183]', '[unused184]', '[unused185]', '[unused186]', '[unused187]', '[unused188]', '[unused189]', '[unused190]', '[unused191]', '[unused192]', '[unused193]', '[unused194]', '[unused195]', '[unused196]', '[unused197]', '[unused198]', '[unused199]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XF_Q_0VTFzPv",
        "outputId": "655600a8-ced9-4c6e-fde7-6c19d9b218dc"
      },
      "source": [
        "#'/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model'\n",
        "\n",
        "from transformers import AlbertTokenizer, AlbertTokenizerFast\n",
        "\n",
        "albet_tokenizer_model = '/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model'\n",
        "\n",
        "tokenizer = AlbertTokenizerFast.from_pretrained(albet_tokenizer_model)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "QB0SlhA4GMQQ",
        "outputId": "c7b50d8e-4654-4643-d8f3-58b4f7e94e58"
      },
      "source": [
        "op = tokenizer.encode(\"나는 오늘 학교에 간다.\")\n",
        "print(op)\n",
        "tokenizer.decode(op)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[5, 172, 274, 402, 720, 368, 1870, 2802, 6]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[CLS] 나는 오늘 학교에 간다.[SEP]'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMDfH8GZF_mK",
        "outputId": "a9cf5449-a83f-4e2c-b8b6-7a2b1e12f0b7"
      },
      "source": [
        "# tokenizer에 special token 추가\n",
        "special_tokens_dict = {'additional_special_tokens': user_defined_symbols}\n",
        "tokenizer.add_special_tokens(special_tokens_dict)\n",
        "\n",
        "# check tokenizer vocab with special tokens\n",
        "print('check special tokens : %s'%tokenizer.all_special_tokens[:20])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "check special tokens : ['[CLS]', '[SEP]', '<unk>', '<pad>', '[MASK]', '[BOS]', '[EOS]', '[UNK0]', '[UNK1]', '[UNK2]', '[UNK3]', '[UNK4]', '[UNK5]', '[UNK6]', '[UNK7]', '[UNK8]', '[UNK9]', '[unused0]', '[unused1]', '[unused2]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Inr3cDwQG6Fs",
        "outputId": "256500e0-4f5a-4206-82b4-0afeefb5c251"
      },
      "source": [
        "# save tokenizer model with special tokens\n",
        "tokenizer.save_pretrained(albet_tokenizer_model+'_special2')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/spiece.model',\n",
              " '/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/added_tokens.json',\n",
              " '/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/tokenizer.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwhlWrXXHr6L"
      },
      "source": [
        "tokenizer = AlbertTokenizerFast.from_pretrained(albet_tokenizer_model+'_special2')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgHhYsZbH3zr",
        "outputId": "3353a1a4-0fa6-4c72-b524-e7d1d79315fe"
      },
      "source": [
        "op = tokenizer(\"나는 오늘 학교에 간다.\", return_tensors=\"pt\")\n",
        "print(op)\n",
        "print(\"Tokens (str)      : {}\".format([tokenizer.convert_ids_to_tokens(s) for s in op['input_ids'].tolist()[0]]))\n",
        "print(\"Tokens (int)      : {}\".format(op['input_ids'].tolist()[0]))\n",
        "print(\"Tokens (attn_mask): {}\\n\".format(op['attention_mask'].tolist()[0]))\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'input_ids': tensor([[   5,  172,  274,  402,  720,  368, 1870, 2802,    6]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
            "Tokens (str)      : ['[CLS]', '▁나', '는', '▁오늘', '▁학교', '에', '▁간다', '.', '[SEP]']\n",
            "Tokens (int)      : [5, 172, 274, 402, 720, 368, 1870, 2802, 6]\n",
            "Tokens (attn_mask): [1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "paElX_hhJAoh",
        "outputId": "03b7c800-c8f1-4fbd-ca50-6050c365a869"
      },
      "source": [
        "#Checking vocabulary size\n",
        "vocab_size=tokenizer.vocab_size ; vocab_size"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "9RPI1UerOSCB",
        "outputId": "c80926e1-b981-429a-9895-938777b9499e"
      },
      "source": [
        "import json\n",
        "\n",
        "config = {\n",
        "    \"architectures\": [\n",
        "        \"AlbertModel\"\n",
        "    ],\n",
        "\t\"attention_probs_dropout_prob\": 0.1,\n",
        "\t\"hidden_act\": \"gelu\",\n",
        "\t\"hidden_dropout_prob\": 0.1,\n",
        "\t\"hidden_size\": 768,\n",
        "\t\"initializer_range\": 0.02,\n",
        "\t\"intermediate_size\": 3072,\n",
        "\t\"layer_norm_eps\": 1e-05,\n",
        "\t\"max_position_embeddings\": 514,\n",
        "\t\"model_type\": \"albert\",\n",
        "\t\"num_attention_heads\": 12,\n",
        "\t\"num_hidden_layers\": 6,\n",
        "\t\"type_vocab_size\": 1,\n",
        "\t\"vocab_size\": vocab_size\n",
        "}\n",
        "with open(albet_tokenizer_model + \"_special2/config.json\", 'w') as fp:\n",
        "    json.dump(config, fp)\n",
        "\n",
        "\n",
        "#Configuration for tokenizer.\n",
        "#Note: I set do_lower_case: False, and keep_accents:True\n",
        "# Opening JSON file\n",
        "f = open(albet_tokenizer_model+ \"_special2/tokenizer_config.json\")\n",
        "   \n",
        "# returns JSON object as \n",
        "# a dictionary\n",
        "tokenizer_config = json.load(f)\n",
        "\n",
        "tokenizer_config['max_len'] = 512\n",
        "tokenizer_config['model_type'] = 'albert'\n",
        "tokenizer_config['do_lower_case'] = False\n",
        "tokenizer_config['keep_accents'] = True\n",
        "\n",
        "with open(albet_tokenizer_model+ \"_special2/tokenizer_config.json\", 'w') as outfile:\n",
        "    json.dump(tokenizer_config, outfile)\n",
        "'''\n",
        "tokenizer_config = {\n",
        "\t\"max_len\": 512,\n",
        "\t\"model_type\": \"albert\",\n",
        "\t\"do_lower_case\":False, \n",
        "\t\"keep_accents\":True\n",
        "}\n",
        "with open(albet_tokenizer_model+ \"_special/tokenizer_config.json\", 'w') as fp:\n",
        "    json.dump(tokenizer_config, fp)\n",
        "'''"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ntokenizer_config = {\\n\\t\"max_len\": 512,\\n\\t\"model_type\": \"albert\",\\n\\t\"do_lower_case\":False, \\n\\t\"keep_accents\":True\\n}\\nwith open(albet_tokenizer_model+ \"_special/tokenizer_config.json\", \\'w\\') as fp:\\n    json.dump(tokenizer_config, fp)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdjOOoG6QX2p",
        "outputId": "96af5aee-9a55-48c6-e9ea-f735b643fd5e"
      },
      "source": [
        "\n",
        "#To train from scratch\n",
        "!python /content/transformers/examples/pytorch/language-modeling/run_mlm.py \\\n",
        "        --model_type albert-base-v2 \\\n",
        "        --config_name /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/ \\\n",
        "        --tokenizer_name /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/ \\\n",
        "        --train_file /content/drive/MyDrive/Tokenizer_train/data/train_tokenizer.txt \\\n",
        "        --output_dir /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model \\\n",
        "        --use_fast_tokenizer \\\n",
        "        --do_train \\\n",
        "        --line_by_line \\\n",
        "        --save_steps 500 \\\n",
        "        --logging_steps 500 \\\n",
        "        --save_total_limit 2 \\\n",
        "        --num_train_epochs 1 \\\n",
        "        --seed 108 \\\n",
        "        --overwrite_output_dir \\\n",
        "        --logging_dir /content/drive/MyDrive/Tokenizer_train/logs"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-15 14:48:53.519946: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "05/15/2021 14:48:55 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "05/15/2021 14:48:55 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model, overwrite_output_dir=True, do_train=True, do_eval=False, do_predict=False, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=/content/drive/MyDrive/Tokenizer_train/logs, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.STEPS, save_steps=500, save_total_limit=2, no_cuda=False, seed=108, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, _n_gpu=1, mp_parameters=)\n",
            "05/15/2021 14:48:55 - WARNING - datasets.builder -   Using custom data configuration default-c6d99882842b208d\n",
            "Downloading and preparing dataset text/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/text/default-c6d99882842b208d/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5...\n",
            "Dataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-c6d99882842b208d/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5. Subsequent calls will reuse this data.\n",
            "[INFO|configuration_utils.py:515] 2021-05-15 14:48:56,699 >> loading configuration file /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/config.json\n",
            "[INFO|configuration_utils.py:553] 2021-05-15 14:48:56,699 >> Model config AlbertConfig {\n",
            "  \"architectures\": [\n",
            "    \"AlbertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.7.0.dev0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:515] 2021-05-15 14:48:56,700 >> loading configuration file /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/config.json\n",
            "[INFO|configuration_utils.py:553] 2021-05-15 14:48:56,701 >> Model config AlbertConfig {\n",
            "  \"architectures\": [\n",
            "    \"AlbertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.7.0.dev0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1715] 2021-05-15 14:48:56,702 >> loading file /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/spiece.model\n",
            "[INFO|tokenization_utils_base.py:1715] 2021-05-15 14:48:56,702 >> loading file /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1715] 2021-05-15 14:48:56,703 >> loading file /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:1715] 2021-05-15 14:48:56,703 >> loading file /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:1715] 2021-05-15 14:48:56,703 >> loading file /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/tokenizer_config.json\n",
            "05/15/2021 14:48:56 - INFO - __main__ -   Training new model from scratch\n",
            "100% 500/500 [00:27<00:00, 18.04ba/s]\n",
            "[INFO|trainer.py:516] 2021-05-15 14:49:32,537 >> The following columns in the training set  don't have a corresponding argument in `AlbertForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[INFO|trainer.py:1145] 2021-05-15 14:49:32,554 >> ***** Running training *****\n",
            "[INFO|trainer.py:1146] 2021-05-15 14:49:32,554 >>   Num examples = 500000\n",
            "[INFO|trainer.py:1147] 2021-05-15 14:49:32,554 >>   Num Epochs = 1\n",
            "[INFO|trainer.py:1148] 2021-05-15 14:49:32,554 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1149] 2021-05-15 14:49:32,554 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:1150] 2021-05-15 14:49:32,555 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1151] 2021-05-15 14:49:32,555 >>   Total optimization steps = 62500\n",
            "{'loss': 9.0006, 'learning_rate': 4.96e-05, 'epoch': 0.01}\n",
            "  1% 500/62500 [00:18<39:04, 26.45it/s][INFO|trainer.py:1885] 2021-05-15 14:49:51,567 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-500\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 14:49:51,572 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 14:49:51,743 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 14:49:51,749 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 14:49:51,752 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-500/special_tokens_map.json\n",
            "{'loss': 7.4974, 'learning_rate': 4.92e-05, 'epoch': 0.02}\n",
            "  2% 1000/62500 [00:38<37:48, 27.12it/s][INFO|trainer.py:1885] 2021-05-15 14:50:11,031 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-1000\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 14:50:11,036 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-1000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 14:50:11,185 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-1000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 14:50:11,190 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 14:50:11,193 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-1000/special_tokens_map.json\n",
            "{'loss': 7.1824, 'learning_rate': 4.88e-05, 'epoch': 0.02}\n",
            "  2% 1500/62500 [00:57<36:54, 27.55it/s][INFO|trainer.py:1885] 2021-05-15 14:50:30,050 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-1500\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 14:50:30,055 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-1500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 14:50:30,208 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-1500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 14:50:30,213 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-1500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 14:50:30,217 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-1500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 14:50:30,574 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-500] due to args.save_total_limit\n",
            "{'loss': 7.1671, 'learning_rate': 4.8400000000000004e-05, 'epoch': 0.03}\n",
            "  3% 2000/62500 [01:17<38:52, 25.94it/s][INFO|trainer.py:1885] 2021-05-15 14:50:49,845 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-2000\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 14:50:49,850 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-2000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 14:50:49,995 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-2000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 14:50:50,000 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-2000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 14:50:50,003 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-2000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 14:50:50,359 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-1000] due to args.save_total_limit\n",
            "{'loss': 7.1238, 'learning_rate': 4.8e-05, 'epoch': 0.04}\n",
            "  4% 2500/62500 [01:36<36:00, 27.78it/s][INFO|trainer.py:1885] 2021-05-15 14:51:08,758 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-2500\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 14:51:08,763 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-2500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 14:51:08,917 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-2500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 14:51:08,921 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-2500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 14:51:08,924 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-2500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 14:51:09,286 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-1500] due to args.save_total_limit\n",
            "{'loss': 7.1057, 'learning_rate': 4.76e-05, 'epoch': 0.05}\n",
            "  5% 3000/62500 [01:55<38:52, 25.51it/s][INFO|trainer.py:1885] 2021-05-15 14:51:28,165 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-3000\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 14:51:28,170 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-3000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 14:51:28,317 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-3000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 14:51:28,322 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-3000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 14:51:28,325 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-3000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 14:51:28,677 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-2000] due to args.save_total_limit\n",
            "{'loss': 7.0158, 'learning_rate': 4.72e-05, 'epoch': 0.06}\n",
            "  6% 3500/62500 [02:14<34:09, 28.79it/s][INFO|trainer.py:1885] 2021-05-15 14:51:47,232 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-3500\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 14:51:47,237 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-3500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 14:51:47,386 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-3500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 14:51:47,391 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-3500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 14:51:47,394 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-3500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 14:51:47,796 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-2500] due to args.save_total_limit\n",
            "{'loss': 7.0093, 'learning_rate': 4.6800000000000006e-05, 'epoch': 0.06}\n",
            "  6% 4000/62500 [02:33<35:02, 27.82it/s][INFO|trainer.py:1885] 2021-05-15 14:52:06,456 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-4000\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 14:52:06,461 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-4000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 14:52:06,607 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-4000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 14:52:06,612 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-4000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 14:52:06,615 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-4000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 14:52:06,978 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-3000] due to args.save_total_limit\n",
            "{'loss': 6.9714, 'learning_rate': 4.64e-05, 'epoch': 0.07}\n",
            "  7% 4500/62500 [02:53<35:03, 27.57it/s][INFO|trainer.py:1885] 2021-05-15 14:52:26,052 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-4500\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 14:52:26,058 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-4500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 14:52:26,211 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-4500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 14:52:26,216 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-4500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 14:52:26,220 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-4500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 14:52:26,601 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-3500] due to args.save_total_limit\n",
            "{'loss': 6.9536, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.08}\n",
            "  8% 5000/62500 [03:13<35:43, 26.83it/s][INFO|trainer.py:1885] 2021-05-15 14:52:46,016 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-5000\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 14:52:46,021 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-5000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 14:52:46,176 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-5000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 14:52:46,181 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-5000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 14:52:46,185 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-5000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 14:52:46,552 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-4000] due to args.save_total_limit\n",
            "{'loss': 6.9482, 'learning_rate': 4.5600000000000004e-05, 'epoch': 0.09}\n",
            "  9% 5500/62500 [03:33<35:37, 26.66it/s][INFO|trainer.py:1885] 2021-05-15 14:53:06,025 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-5500\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 14:53:06,030 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-5500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 14:53:06,212 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-5500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 14:53:06,218 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-5500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 14:53:06,221 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-5500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 14:53:06,609 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-4500] due to args.save_total_limit\n",
            "{'loss': 6.9006, 'learning_rate': 4.52e-05, 'epoch': 0.1}\n",
            " 10% 6000/62500 [03:53<33:50, 27.83it/s][INFO|trainer.py:1885] 2021-05-15 14:53:25,860 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-6000\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 14:53:25,865 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-6000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 14:53:26,017 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-6000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 14:53:26,021 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-6000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 14:53:26,025 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-6000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 14:53:26,425 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-5000] due to args.save_total_limit\n",
            "{'loss': 6.8282, 'learning_rate': 4.4800000000000005e-05, 'epoch': 0.1}\n",
            " 10% 6500/62500 [04:12<34:31, 27.03it/s][INFO|trainer.py:1885] 2021-05-15 14:53:45,560 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-6500\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 14:53:45,565 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-6500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 14:53:45,716 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-6500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 14:53:45,720 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-6500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 14:53:45,724 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-6500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 14:53:46,106 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-5500] due to args.save_total_limit\n",
            "{'loss': 6.8195, 'learning_rate': 4.44e-05, 'epoch': 0.11}\n",
            " 11% 7000/62500 [04:32<34:19, 26.94it/s][INFO|trainer.py:1885] 2021-05-15 14:54:04,799 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-7000\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 14:54:04,804 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-7000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 14:54:04,954 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-7000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 14:54:04,959 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-7000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 14:54:04,962 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-7000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 14:54:05,347 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-6000] due to args.save_total_limit\n",
            "{'loss': 6.8174, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.12}\n",
            " 12% 7500/62500 [04:51<33:16, 27.55it/s][INFO|trainer.py:1885] 2021-05-15 14:54:24,390 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-7500\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 14:54:24,395 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-7500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 14:54:24,544 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-7500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 14:54:24,549 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-7500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 14:54:24,553 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-7500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 14:54:24,914 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-6500] due to args.save_total_limit\n",
            "{'loss': 6.7972, 'learning_rate': 4.36e-05, 'epoch': 0.13}\n",
            " 13% 8000/62500 [05:11<35:34, 25.53it/s][INFO|trainer.py:1885] 2021-05-15 14:54:43,596 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-8000\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 14:54:43,602 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-8000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 14:54:43,753 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-8000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 14:54:43,758 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-8000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 14:54:43,765 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-8000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 14:54:44,172 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-7000] due to args.save_total_limit\n",
            "{'loss': 6.7755, 'learning_rate': 4.32e-05, 'epoch': 0.14}\n",
            " 14% 8500/62500 [05:30<34:13, 26.30it/s][INFO|trainer.py:1885] 2021-05-15 14:55:02,997 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-8500\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 14:55:03,003 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-8500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 14:55:03,153 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-8500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 14:55:03,172 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-8500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 14:55:03,176 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-8500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 14:55:03,547 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-7500] due to args.save_total_limit\n",
            "{'loss': 6.779, 'learning_rate': 4.2800000000000004e-05, 'epoch': 0.14}\n",
            " 14% 9000/62500 [05:49<32:52, 27.13it/s][INFO|trainer.py:1885] 2021-05-15 14:55:22,116 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-9000\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 14:55:22,121 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-9000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 14:55:22,275 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-9000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 14:55:22,280 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-9000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 14:55:22,284 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-9000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 14:55:22,662 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-8000] due to args.save_total_limit\n",
            "{'loss': 6.7872, 'learning_rate': 4.24e-05, 'epoch': 0.15}\n",
            " 15% 9500/62500 [06:08<31:55, 27.67it/s][INFO|trainer.py:1885] 2021-05-15 14:55:41,172 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-9500\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 14:55:41,177 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-9500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 14:55:41,323 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-9500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 14:55:41,329 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-9500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 14:55:41,332 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-9500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 14:55:41,712 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-8500] due to args.save_total_limit\n",
            "{'loss': 6.7477, 'learning_rate': 4.2e-05, 'epoch': 0.16}\n",
            " 16% 10000/62500 [06:27<33:22, 26.21it/s][INFO|trainer.py:1885] 2021-05-15 14:56:00,425 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-10000\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 14:56:00,431 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-10000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 14:56:00,576 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-10000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 14:56:00,581 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-10000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 14:56:00,584 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-10000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 14:56:00,941 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-9000] due to args.save_total_limit\n",
            "{'loss': 6.7508, 'learning_rate': 4.16e-05, 'epoch': 0.17}\n",
            " 17% 10500/62500 [06:46<32:09, 26.95it/s][INFO|trainer.py:1885] 2021-05-15 14:56:19,560 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-10500\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 14:56:19,565 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-10500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 14:56:19,724 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-10500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 14:56:19,730 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-10500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 14:56:19,734 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-10500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 14:56:20,147 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-9500] due to args.save_total_limit\n",
            "{'loss': 6.7208, 'learning_rate': 4.12e-05, 'epoch': 0.18}\n",
            " 18% 11000/62500 [07:06<30:35, 28.05it/s][INFO|trainer.py:1885] 2021-05-15 14:56:39,257 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-11000\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 14:56:39,262 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-11000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 14:56:39,410 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-11000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 14:56:39,414 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-11000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 14:56:39,418 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-11000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 14:56:39,781 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-10000] due to args.save_total_limit\n",
            "{'loss': 6.6642, 'learning_rate': 4.08e-05, 'epoch': 0.18}\n",
            " 18% 11500/62500 [07:26<33:59, 25.00it/s][INFO|trainer.py:1885] 2021-05-15 14:56:58,655 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-11500\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 14:56:58,661 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-11500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 14:56:58,828 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-11500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 14:56:58,832 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-11500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 14:56:58,836 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-11500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 14:56:59,193 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-10500] due to args.save_total_limit\n",
            "{'loss': 6.6729, 'learning_rate': 4.0400000000000006e-05, 'epoch': 0.19}\n",
            " 19% 12000/62500 [07:45<32:11, 26.14it/s][INFO|trainer.py:1885] 2021-05-15 14:57:17,865 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-12000\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 14:57:17,870 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-12000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 14:57:18,027 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-12000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 14:57:18,031 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-12000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 14:57:18,035 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-12000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 14:57:18,410 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-11000] due to args.save_total_limit\n",
            "{'loss': 6.6778, 'learning_rate': 4e-05, 'epoch': 0.2}\n",
            " 20% 12500/62500 [08:04<30:06, 27.68it/s][INFO|trainer.py:1885] 2021-05-15 14:57:37,505 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-12500\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 14:57:37,510 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-12500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 14:57:37,646 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-12500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 14:57:37,650 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-12500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 14:57:37,653 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-12500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 14:57:38,035 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-11500] due to args.save_total_limit\n",
            "{'loss': 6.6486, 'learning_rate': 3.960000000000001e-05, 'epoch': 0.21}\n",
            " 21% 13000/62500 [08:24<29:41, 27.79it/s][INFO|trainer.py:1885] 2021-05-15 14:57:57,125 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-13000\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 14:57:57,131 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-13000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 14:57:57,280 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-13000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 14:57:57,285 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-13000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 14:57:57,289 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-13000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 14:57:57,667 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-12000] due to args.save_total_limit\n",
            "{'loss': 6.5899, 'learning_rate': 3.9200000000000004e-05, 'epoch': 0.22}\n",
            " 22% 13500/62500 [08:43<29:48, 27.39it/s][INFO|trainer.py:1885] 2021-05-15 14:58:16,196 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-13500\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 14:58:16,201 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-13500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 14:58:16,352 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-13500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 14:58:16,357 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-13500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 14:58:16,360 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-13500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 14:58:16,831 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-12500] due to args.save_total_limit\n",
            "{'loss': 6.6214, 'learning_rate': 3.88e-05, 'epoch': 0.22}\n",
            " 22% 14000/62500 [09:03<31:34, 25.60it/s][INFO|trainer.py:1885] 2021-05-15 14:58:36,142 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-14000\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 14:58:36,148 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-14000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 14:58:36,300 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-14000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 14:58:36,305 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-14000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 14:58:36,310 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-14000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 14:58:36,673 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-13000] due to args.save_total_limit\n",
            "{'loss': 6.6096, 'learning_rate': 3.8400000000000005e-05, 'epoch': 0.23}\n",
            " 23% 14500/62500 [09:22<29:53, 26.77it/s][INFO|trainer.py:1885] 2021-05-15 14:58:55,524 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-14500\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 14:58:55,529 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-14500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 14:58:55,696 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-14500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 14:58:55,702 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-14500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 14:58:55,706 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-14500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 14:58:56,057 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-13500] due to args.save_total_limit\n",
            "{'loss': 6.6442, 'learning_rate': 3.8e-05, 'epoch': 0.24}\n",
            " 24% 15000/62500 [09:42<28:29, 27.78it/s][INFO|trainer.py:1885] 2021-05-15 14:59:15,107 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-15000\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 14:59:15,112 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-15000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 14:59:15,268 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-15000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 14:59:15,272 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-15000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 14:59:15,276 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-15000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 14:59:15,660 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-14000] due to args.save_total_limit\n",
            "{'loss': 6.5481, 'learning_rate': 3.76e-05, 'epoch': 0.25}\n",
            " 25% 15500/62500 [10:01<28:22, 27.60it/s][INFO|trainer.py:1885] 2021-05-15 14:59:34,518 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-15500\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 14:59:34,523 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-15500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 14:59:34,695 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-15500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 14:59:34,701 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-15500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 14:59:34,705 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-15500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 14:59:35,086 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-14500] due to args.save_total_limit\n",
            "{'loss': 6.6187, 'learning_rate': 3.72e-05, 'epoch': 0.26}\n",
            " 26% 16000/62500 [10:21<28:21, 27.32it/s][INFO|trainer.py:1885] 2021-05-15 14:59:54,127 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-16000\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 14:59:54,132 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-16000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 14:59:54,285 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-16000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 14:59:54,290 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-16000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 14:59:54,294 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-16000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 14:59:54,666 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-15000] due to args.save_total_limit\n",
            "{'loss': 6.5931, 'learning_rate': 3.68e-05, 'epoch': 0.26}\n",
            " 26% 16500/62500 [10:41<29:27, 26.02it/s][INFO|trainer.py:1885] 2021-05-15 15:00:13,782 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-16500\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:00:13,787 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-16500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:00:13,937 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-16500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:00:13,942 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-16500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:00:13,945 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-16500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:00:14,318 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-15500] due to args.save_total_limit\n",
            "{'loss': 6.4367, 'learning_rate': 3.6400000000000004e-05, 'epoch': 0.27}\n",
            " 27% 17000/62500 [11:00<27:18, 27.77it/s][INFO|trainer.py:1885] 2021-05-15 15:00:33,266 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-17000\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:00:33,272 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-17000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:00:33,430 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-17000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:00:33,435 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-17000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:00:33,439 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-17000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:00:33,804 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-16000] due to args.save_total_limit\n",
            "{'loss': 6.5587, 'learning_rate': 3.6e-05, 'epoch': 0.28}\n",
            " 28% 17500/62500 [11:20<29:15, 25.64it/s][INFO|trainer.py:1885] 2021-05-15 15:00:52,768 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-17500\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:00:52,777 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-17500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:00:52,929 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-17500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:00:52,934 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-17500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:00:52,938 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-17500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:00:53,309 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-16500] due to args.save_total_limit\n",
            "{'loss': 6.5264, 'learning_rate': 3.56e-05, 'epoch': 0.29}\n",
            " 29% 18000/62500 [11:39<26:49, 27.65it/s][INFO|trainer.py:1885] 2021-05-15 15:01:11,958 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-18000\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:01:11,964 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-18000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:01:12,118 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-18000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:01:12,123 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-18000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:01:12,144 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-18000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:01:12,499 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-17000] due to args.save_total_limit\n",
            "{'loss': 6.5537, 'learning_rate': 3.52e-05, 'epoch': 0.3}\n",
            " 30% 18500/62500 [11:58<27:51, 26.33it/s][INFO|trainer.py:1885] 2021-05-15 15:01:31,229 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-18500\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:01:31,235 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-18500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:01:31,387 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-18500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:01:31,392 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-18500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:01:31,396 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-18500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:01:31,800 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-17500] due to args.save_total_limit\n",
            "{'loss': 6.5424, 'learning_rate': 3.48e-05, 'epoch': 0.3}\n",
            " 30% 19000/62500 [12:17<28:19, 25.60it/s][INFO|trainer.py:1885] 2021-05-15 15:01:50,581 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-19000\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:01:50,586 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-19000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:01:50,827 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-19000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:01:50,833 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-19000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:01:50,837 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-19000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:01:51,230 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-18000] due to args.save_total_limit\n",
            "{'loss': 6.4687, 'learning_rate': 3.4399999999999996e-05, 'epoch': 0.31}\n",
            " 31% 19500/62500 [12:37<27:40, 25.89it/s][INFO|trainer.py:1885] 2021-05-15 15:02:09,628 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-19500\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:02:09,633 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-19500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:02:09,778 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-19500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:02:09,783 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-19500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:02:09,786 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-19500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:02:10,161 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-18500] due to args.save_total_limit\n",
            "{'loss': 6.5106, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.32}\n",
            " 32% 20000/62500 [12:56<26:39, 26.57it/s][INFO|trainer.py:1885] 2021-05-15 15:02:28,960 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-20000\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:02:28,965 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-20000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:02:29,122 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-20000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:02:29,126 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-20000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:02:29,130 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-20000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:02:29,514 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-19000] due to args.save_total_limit\n",
            "{'loss': 6.4987, 'learning_rate': 3.3600000000000004e-05, 'epoch': 0.33}\n",
            " 33% 20500/62500 [13:15<27:52, 25.11it/s][INFO|trainer.py:1885] 2021-05-15 15:02:48,340 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-20500\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:02:48,346 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-20500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:02:48,497 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-20500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:02:48,502 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-20500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:02:48,505 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-20500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:02:48,863 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-19500] due to args.save_total_limit\n",
            "{'loss': 6.4687, 'learning_rate': 3.32e-05, 'epoch': 0.34}\n",
            " 34% 21000/62500 [13:35<25:15, 27.38it/s][INFO|trainer.py:1885] 2021-05-15 15:03:08,048 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-21000\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:03:08,053 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-21000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:03:08,212 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-21000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:03:08,233 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-21000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:03:08,237 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-21000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:03:08,603 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-20000] due to args.save_total_limit\n",
            "{'loss': 6.4907, 'learning_rate': 3.2800000000000004e-05, 'epoch': 0.34}\n",
            " 34% 21500/62500 [13:54<28:25, 24.04it/s][INFO|trainer.py:1885] 2021-05-15 15:03:27,473 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-21500\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:03:27,478 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-21500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:03:27,636 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-21500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:03:27,642 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-21500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:03:27,646 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-21500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:03:28,038 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-20500] due to args.save_total_limit\n",
            "{'loss': 6.4745, 'learning_rate': 3.24e-05, 'epoch': 0.35}\n",
            " 35% 22000/62500 [14:14<25:44, 26.22it/s][INFO|trainer.py:1885] 2021-05-15 15:03:46,951 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-22000\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:03:46,956 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-22000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:03:47,103 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-22000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:03:47,108 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-22000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:03:47,112 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-22000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:03:47,507 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-21000] due to args.save_total_limit\n",
            "{'loss': 6.4041, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.36}\n",
            " 36% 22500/62500 [14:33<27:02, 24.65it/s][INFO|trainer.py:1885] 2021-05-15 15:04:06,350 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-22500\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:04:06,356 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-22500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:04:06,510 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-22500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:04:06,515 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-22500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:04:06,519 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-22500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:04:06,896 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-21500] due to args.save_total_limit\n",
            "{'loss': 6.4012, 'learning_rate': 3.16e-05, 'epoch': 0.37}\n",
            " 37% 23000/62500 [14:53<23:37, 27.86it/s][INFO|trainer.py:1885] 2021-05-15 15:04:25,670 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-23000\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:04:25,677 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-23000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:04:25,828 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-23000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:04:25,832 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-23000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:04:25,836 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-23000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:04:26,199 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-22000] due to args.save_total_limit\n",
            "{'loss': 6.4345, 'learning_rate': 3.12e-05, 'epoch': 0.38}\n",
            " 38% 23500/62500 [15:12<24:41, 26.33it/s][INFO|trainer.py:1885] 2021-05-15 15:04:44,961 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-23500\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:04:44,966 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-23500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:04:45,225 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-23500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:04:45,230 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-23500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:04:45,234 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-23500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:04:45,590 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-22500] due to args.save_total_limit\n",
            "{'loss': 6.4457, 'learning_rate': 3.08e-05, 'epoch': 0.38}\n",
            " 38% 24000/62500 [15:31<22:48, 28.14it/s][INFO|trainer.py:1885] 2021-05-15 15:05:04,069 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-24000\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:05:04,075 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-24000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:05:04,247 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-24000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:05:04,252 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-24000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:05:04,255 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-24000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:05:04,644 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-23000] due to args.save_total_limit\n",
            "{'loss': 6.4626, 'learning_rate': 3.04e-05, 'epoch': 0.39}\n",
            " 39% 24500/62500 [15:50<23:22, 27.09it/s][INFO|trainer.py:1885] 2021-05-15 15:05:23,252 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-24500\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:05:23,257 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-24500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:05:23,416 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-24500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:05:23,420 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-24500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:05:23,424 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-24500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:05:23,817 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-23500] due to args.save_total_limit\n",
            "{'loss': 6.4215, 'learning_rate': 3e-05, 'epoch': 0.4}\n",
            " 40% 25000/62500 [16:09<22:51, 27.33it/s][INFO|trainer.py:1885] 2021-05-15 15:05:42,479 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-25000\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:05:42,485 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-25000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:05:42,633 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-25000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:05:42,641 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-25000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:05:42,645 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-25000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:05:43,018 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-24000] due to args.save_total_limit\n",
            "{'loss': 6.4145, 'learning_rate': 2.96e-05, 'epoch': 0.41}\n",
            " 41% 25500/62500 [16:29<22:11, 27.79it/s][INFO|trainer.py:1885] 2021-05-15 15:06:01,806 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-25500\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:06:01,811 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-25500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:06:01,962 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-25500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:06:01,967 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-25500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:06:01,970 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-25500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:06:02,483 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-24500] due to args.save_total_limit\n",
            "{'loss': 6.4134, 'learning_rate': 2.9199999999999998e-05, 'epoch': 0.42}\n",
            " 42% 26000/62500 [16:48<21:29, 28.30it/s][INFO|trainer.py:1885] 2021-05-15 15:06:21,073 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-26000\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:06:21,079 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-26000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:06:21,225 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-26000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:06:21,230 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-26000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:06:21,237 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-26000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:06:21,597 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-25000] due to args.save_total_limit\n",
            "{'loss': 6.3377, 'learning_rate': 2.88e-05, 'epoch': 0.42}\n",
            " 42% 26500/62500 [17:07<21:50, 27.48it/s][INFO|trainer.py:1885] 2021-05-15 15:06:40,101 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-26500\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:06:40,106 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-26500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:06:40,260 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-26500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:06:40,265 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-26500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:06:40,269 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-26500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:06:40,626 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-25500] due to args.save_total_limit\n",
            "{'loss': 6.3966, 'learning_rate': 2.84e-05, 'epoch': 0.43}\n",
            " 43% 27000/62500 [17:27<23:09, 25.54it/s][INFO|trainer.py:1885] 2021-05-15 15:06:59,623 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-27000\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:06:59,629 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-27000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:06:59,773 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-27000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:06:59,777 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-27000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:06:59,781 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-27000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:07:00,135 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-26000] due to args.save_total_limit\n",
            "{'loss': 6.3643, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.44}\n",
            " 44% 27500/62500 [17:46<22:26, 26.00it/s][INFO|trainer.py:1885] 2021-05-15 15:07:18,800 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-27500\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:07:18,806 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-27500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:07:18,950 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-27500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:07:18,975 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-27500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:07:18,979 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-27500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:07:19,343 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-26500] due to args.save_total_limit\n",
            "{'loss': 6.339, 'learning_rate': 2.7600000000000003e-05, 'epoch': 0.45}\n",
            " 45% 28000/62500 [18:05<21:17, 27.00it/s][INFO|trainer.py:1885] 2021-05-15 15:07:37,990 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-28000\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:07:37,996 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-28000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:07:38,158 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-28000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:07:38,163 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-28000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:07:38,166 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-28000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:07:38,550 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-27000] due to args.save_total_limit\n",
            "{'loss': 6.3995, 'learning_rate': 2.7200000000000004e-05, 'epoch': 0.46}\n",
            " 46% 28500/62500 [18:24<19:37, 28.87it/s][INFO|trainer.py:1885] 2021-05-15 15:07:57,403 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-28500\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:07:57,407 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-28500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:07:57,556 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-28500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:07:57,560 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-28500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:07:57,563 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-28500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:07:57,959 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-27500] due to args.save_total_limit\n",
            "{'loss': 6.3257, 'learning_rate': 2.6800000000000004e-05, 'epoch': 0.46}\n",
            " 46% 29000/62500 [18:44<22:25, 24.90it/s][INFO|trainer.py:1885] 2021-05-15 15:08:17,091 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-29000\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:08:17,096 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-29000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:08:17,243 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-29000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:08:17,248 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-29000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:08:17,251 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-29000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:08:17,626 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-28000] due to args.save_total_limit\n",
            "{'loss': 6.3364, 'learning_rate': 2.64e-05, 'epoch': 0.47}\n",
            " 47% 29500/62500 [19:04<19:41, 27.94it/s][INFO|trainer.py:1885] 2021-05-15 15:08:36,614 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-29500\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:08:36,619 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-29500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:08:36,779 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-29500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:08:36,783 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-29500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:08:36,786 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-29500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:08:37,150 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-28500] due to args.save_total_limit\n",
            "{'loss': 6.3268, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.48}\n",
            " 48% 30000/62500 [19:23<21:21, 25.36it/s][INFO|trainer.py:1885] 2021-05-15 15:08:55,774 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-30000\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:08:55,779 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-30000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:08:55,929 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-30000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:08:55,933 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-30000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:08:55,936 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-30000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:08:56,288 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-29000] due to args.save_total_limit\n",
            "{'loss': 6.2801, 'learning_rate': 2.5600000000000002e-05, 'epoch': 0.49}\n",
            " 49% 30500/62500 [19:42<20:25, 26.11it/s][INFO|trainer.py:1885] 2021-05-15 15:09:15,071 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-30500\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:09:15,076 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-30500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:09:15,236 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-30500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:09:15,241 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-30500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:09:15,244 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-30500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:09:15,603 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-29500] due to args.save_total_limit\n",
            "{'loss': 6.2799, 'learning_rate': 2.5200000000000003e-05, 'epoch': 0.5}\n",
            " 50% 31000/62500 [20:02<20:32, 25.55it/s][INFO|trainer.py:1885] 2021-05-15 15:09:34,656 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-31000\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:09:34,660 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-31000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:09:34,820 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-31000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:09:34,825 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-31000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:09:34,829 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-31000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:09:35,200 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-30000] due to args.save_total_limit\n",
            "{'loss': 6.2955, 'learning_rate': 2.48e-05, 'epoch': 0.5}\n",
            " 50% 31500/62500 [20:21<18:24, 28.08it/s][INFO|trainer.py:1885] 2021-05-15 15:09:53,730 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-31500\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:09:53,735 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-31500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:09:53,871 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-31500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:09:53,876 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-31500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:09:53,879 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-31500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:09:54,248 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-30500] due to args.save_total_limit\n",
            "{'loss': 6.2723, 'learning_rate': 2.44e-05, 'epoch': 0.51}\n",
            " 51% 32000/62500 [20:39<18:41, 27.20it/s][INFO|trainer.py:1885] 2021-05-15 15:10:12,570 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-32000\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:10:12,575 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-32000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:10:12,721 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-32000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:10:12,725 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-32000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:10:12,729 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-32000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:10:13,110 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-31000] due to args.save_total_limit\n",
            "{'loss': 6.2659, 'learning_rate': 2.4e-05, 'epoch': 0.52}\n",
            " 52% 32500/62500 [20:59<20:51, 23.97it/s][INFO|trainer.py:1885] 2021-05-15 15:10:31,995 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-32500\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:10:32,000 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-32500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:10:32,153 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-32500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:10:32,157 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-32500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:10:32,162 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-32500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:10:32,543 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-31500] due to args.save_total_limit\n",
            "{'loss': 6.2179, 'learning_rate': 2.36e-05, 'epoch': 0.53}\n",
            " 53% 33000/62500 [21:19<18:53, 26.02it/s][INFO|trainer.py:1885] 2021-05-15 15:10:51,635 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-33000\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:10:51,640 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-33000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:10:51,799 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-33000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:10:51,804 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-33000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:10:51,808 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-33000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:10:52,190 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-32000] due to args.save_total_limit\n",
            "{'loss': 6.2484, 'learning_rate': 2.32e-05, 'epoch': 0.54}\n",
            " 54% 33500/62500 [21:39<17:53, 27.00it/s][INFO|trainer.py:1885] 2021-05-15 15:11:11,620 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-33500\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:11:11,625 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-33500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:11:11,786 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-33500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:11:11,792 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-33500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:11:11,807 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-33500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:11:12,284 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-32500] due to args.save_total_limit\n",
            "{'loss': 6.2547, 'learning_rate': 2.2800000000000002e-05, 'epoch': 0.54}\n",
            " 54% 34000/62500 [21:58<17:21, 27.37it/s][INFO|trainer.py:1885] 2021-05-15 15:11:31,245 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-34000\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:11:31,250 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-34000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:11:31,408 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-34000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:11:31,412 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-34000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:11:31,437 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-34000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:11:31,932 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-33000] due to args.save_total_limit\n",
            "{'loss': 6.2044, 'learning_rate': 2.2400000000000002e-05, 'epoch': 0.55}\n",
            " 55% 34500/62500 [22:18<18:10, 25.69it/s][INFO|trainer.py:1885] 2021-05-15 15:11:51,061 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-34500\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:11:51,066 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-34500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:11:51,224 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-34500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:11:51,229 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-34500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:11:51,233 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-34500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:11:51,770 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-33500] due to args.save_total_limit\n",
            "{'loss': 6.1816, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.56}\n",
            " 56% 35000/62500 [22:38<16:11, 28.32it/s][INFO|trainer.py:1885] 2021-05-15 15:12:10,664 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-35000\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:12:10,678 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-35000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:12:10,845 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-35000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:12:10,850 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-35000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:12:10,855 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-35000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:12:11,872 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-34000] due to args.save_total_limit\n",
            "{'loss': 6.2305, 'learning_rate': 2.16e-05, 'epoch': 0.57}\n",
            " 57% 35500/62500 [22:58<17:04, 26.37it/s][INFO|trainer.py:1885] 2021-05-15 15:12:31,084 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-35500\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:12:31,089 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-35500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:12:31,258 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-35500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:12:31,263 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-35500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:12:31,267 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-35500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:12:31,729 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-34500] due to args.save_total_limit\n",
            "{'loss': 6.1727, 'learning_rate': 2.12e-05, 'epoch': 0.58}\n",
            " 58% 36000/62500 [23:18<16:55, 26.10it/s][INFO|trainer.py:1885] 2021-05-15 15:12:50,992 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-36000\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:12:50,998 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-36000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:12:51,158 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-36000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:12:51,163 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-36000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:12:51,167 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-36000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:12:51,603 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-35000] due to args.save_total_limit\n",
            "{'loss': 6.196, 'learning_rate': 2.08e-05, 'epoch': 0.58}\n",
            " 58% 36500/62500 [23:38<17:03, 25.40it/s][INFO|trainer.py:1885] 2021-05-15 15:13:11,477 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-36500\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:13:11,484 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-36500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:13:11,655 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-36500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:13:11,660 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-36500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:13:11,664 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-36500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:13:12,087 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-35500] due to args.save_total_limit\n",
            "{'loss': 6.1527, 'learning_rate': 2.04e-05, 'epoch': 0.59}\n",
            " 59% 37000/62500 [23:58<17:08, 24.79it/s][INFO|trainer.py:1885] 2021-05-15 15:13:31,459 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-37000\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:13:31,465 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-37000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:13:31,657 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-37000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:13:31,663 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-37000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:13:31,667 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-37000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:13:32,045 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-36000] due to args.save_total_limit\n",
            "{'loss': 6.1978, 'learning_rate': 2e-05, 'epoch': 0.6}\n",
            " 60% 37500/62500 [24:18<15:00, 27.75it/s][INFO|trainer.py:1885] 2021-05-15 15:13:51,210 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-37500\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:13:51,216 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-37500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:13:51,371 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-37500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:13:51,376 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-37500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:13:51,380 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-37500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:13:52,081 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-36500] due to args.save_total_limit\n",
            "{'loss': 6.1023, 'learning_rate': 1.9600000000000002e-05, 'epoch': 0.61}\n",
            " 61% 38000/62500 [24:38<16:49, 24.27it/s][INFO|trainer.py:1885] 2021-05-15 15:14:11,581 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-38000\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:14:11,587 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-38000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:14:11,734 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-38000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:14:11,738 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-38000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:14:11,741 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-38000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:14:12,132 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-37000] due to args.save_total_limit\n",
            "{'loss': 6.0596, 'learning_rate': 1.9200000000000003e-05, 'epoch': 0.62}\n",
            " 62% 38500/62500 [24:59<15:25, 25.94it/s][INFO|trainer.py:1885] 2021-05-15 15:14:31,741 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-38500\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:14:31,746 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-38500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:14:31,915 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-38500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:14:31,919 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-38500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:14:31,923 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-38500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:14:32,296 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-37500] due to args.save_total_limit\n",
            "{'loss': 6.1837, 'learning_rate': 1.88e-05, 'epoch': 0.62}\n",
            " 62% 39000/62500 [25:18<14:36, 26.81it/s][INFO|trainer.py:1885] 2021-05-15 15:14:51,477 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-39000\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:14:51,482 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-39000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:14:51,631 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-39000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:14:51,636 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-39000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:14:51,640 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-39000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:14:52,066 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-38000] due to args.save_total_limit\n",
            "{'loss': 6.1466, 'learning_rate': 1.84e-05, 'epoch': 0.63}\n",
            " 63% 39500/62500 [25:39<14:09, 27.07it/s][INFO|trainer.py:1885] 2021-05-15 15:15:11,771 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-39500\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:15:11,777 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-39500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:15:11,922 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-39500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:15:11,927 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-39500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:15:11,931 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-39500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:15:12,308 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-38500] due to args.save_total_limit\n",
            "{'loss': 6.1687, 'learning_rate': 1.8e-05, 'epoch': 0.64}\n",
            " 64% 40000/62500 [25:58<14:09, 26.48it/s][INFO|trainer.py:1885] 2021-05-15 15:15:31,502 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-40000\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:15:31,507 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-40000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:15:31,665 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-40000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:15:31,673 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-40000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:15:31,679 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-40000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:15:32,075 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-39000] due to args.save_total_limit\n",
            "{'loss': 6.1048, 'learning_rate': 1.76e-05, 'epoch': 0.65}\n",
            " 65% 40500/62500 [26:18<13:06, 27.98it/s][INFO|trainer.py:1885] 2021-05-15 15:15:51,286 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-40500\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:15:51,291 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-40500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:15:51,453 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-40500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:15:51,457 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-40500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:15:51,477 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-40500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:15:51,861 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-39500] due to args.save_total_limit\n",
            "{'loss': 6.0791, 'learning_rate': 1.7199999999999998e-05, 'epoch': 0.66}\n",
            " 66% 41000/62500 [26:38<13:01, 27.51it/s][INFO|trainer.py:1885] 2021-05-15 15:16:10,736 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-41000\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:16:10,741 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-41000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:16:10,902 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-41000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:16:10,907 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-41000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:16:10,910 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-41000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:16:11,313 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-40000] due to args.save_total_limit\n",
            "{'loss': 6.0789, 'learning_rate': 1.6800000000000002e-05, 'epoch': 0.66}\n",
            " 66% 41500/62500 [26:57<13:17, 26.34it/s][INFO|trainer.py:1885] 2021-05-15 15:16:30,419 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-41500\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:16:30,425 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-41500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:16:30,576 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-41500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:16:30,581 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-41500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:16:30,584 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-41500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:16:30,980 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-40500] due to args.save_total_limit\n",
            "{'loss': 6.1121, 'learning_rate': 1.6400000000000002e-05, 'epoch': 0.67}\n",
            " 67% 42000/62500 [27:17<13:01, 26.24it/s][INFO|trainer.py:1885] 2021-05-15 15:16:50,283 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-42000\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:16:50,289 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-42000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:16:50,443 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-42000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:16:50,449 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-42000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:16:50,452 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-42000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:16:50,873 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-41000] due to args.save_total_limit\n",
            "{'loss': 6.0657, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.68}\n",
            " 68% 42500/62500 [27:37<12:50, 25.94it/s][INFO|trainer.py:1885] 2021-05-15 15:17:10,267 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-42500\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:17:10,272 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-42500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:17:10,431 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-42500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:17:10,436 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-42500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:17:10,439 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-42500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:17:10,822 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-41500] due to args.save_total_limit\n",
            "{'loss': 6.0766, 'learning_rate': 1.56e-05, 'epoch': 0.69}\n",
            " 69% 43000/62500 [27:57<13:14, 24.55it/s][INFO|trainer.py:1885] 2021-05-15 15:17:30,343 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-43000\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:17:30,348 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-43000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:17:30,502 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-43000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:17:30,507 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-43000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:17:30,511 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-43000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:17:30,898 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-42000] due to args.save_total_limit\n",
            "{'loss': 6.0473, 'learning_rate': 1.52e-05, 'epoch': 0.7}\n",
            " 70% 43500/62500 [28:17<11:30, 27.50it/s][INFO|trainer.py:1885] 2021-05-15 15:17:50,133 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-43500\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:17:50,138 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-43500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:17:50,310 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-43500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:17:50,315 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-43500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:17:50,318 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-43500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:17:50,708 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-42500] due to args.save_total_limit\n",
            "{'loss': 6.0641, 'learning_rate': 1.48e-05, 'epoch': 0.7}\n",
            " 70% 44000/62500 [28:37<12:07, 25.44it/s][INFO|trainer.py:1885] 2021-05-15 15:18:10,024 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-44000\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:18:10,029 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-44000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:18:10,176 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-44000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:18:10,180 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-44000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:18:10,184 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-44000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:18:10,591 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-43000] due to args.save_total_limit\n",
            "{'loss': 6.0587, 'learning_rate': 1.44e-05, 'epoch': 0.71}\n",
            " 71% 44500/62500 [28:57<11:52, 25.26it/s][INFO|trainer.py:1885] 2021-05-15 15:18:30,082 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-44500\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:18:30,087 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-44500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:18:30,250 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-44500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:18:30,254 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-44500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:18:30,260 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-44500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:18:30,694 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-43500] due to args.save_total_limit\n",
            "{'loss': 6.0449, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.72}\n",
            " 72% 45000/62500 [29:18<11:24, 25.55it/s][INFO|trainer.py:1885] 2021-05-15 15:18:50,725 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-45000\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:18:50,730 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-45000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:18:50,887 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-45000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:18:50,892 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-45000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:18:50,896 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-45000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:18:51,297 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-44000] due to args.save_total_limit\n",
            "{'loss': 5.9962, 'learning_rate': 1.3600000000000002e-05, 'epoch': 0.73}\n",
            " 73% 45500/62500 [29:38<10:57, 25.84it/s][INFO|trainer.py:1885] 2021-05-15 15:19:10,874 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-45500\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:19:10,880 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-45500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:19:11,036 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-45500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:19:11,041 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-45500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:19:11,046 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-45500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:19:11,428 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-44500] due to args.save_total_limit\n",
            "{'loss': 6.0489, 'learning_rate': 1.32e-05, 'epoch': 0.74}\n",
            " 74% 46000/62500 [29:58<11:00, 24.97it/s][INFO|trainer.py:1885] 2021-05-15 15:19:31,356 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-46000\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:19:31,361 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-46000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:19:31,520 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-46000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:19:31,525 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-46000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:19:31,528 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-46000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:19:31,912 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-45000] due to args.save_total_limit\n",
            "{'loss': 5.9863, 'learning_rate': 1.2800000000000001e-05, 'epoch': 0.74}\n",
            " 74% 46500/62500 [30:18<10:06, 26.39it/s][INFO|trainer.py:1885] 2021-05-15 15:19:51,431 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-46500\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:19:51,436 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-46500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:19:51,608 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-46500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:19:51,612 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-46500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:19:51,616 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-46500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:19:52,004 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-45500] due to args.save_total_limit\n",
            "{'loss': 6.022, 'learning_rate': 1.24e-05, 'epoch': 0.75}\n",
            " 75% 47000/62500 [30:39<10:09, 25.43it/s][INFO|trainer.py:1885] 2021-05-15 15:20:11,619 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-47000\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:20:11,625 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-47000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:20:11,797 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-47000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:20:11,803 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-47000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:20:11,807 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-47000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:20:12,175 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-46000] due to args.save_total_limit\n",
            "{'loss': 6.035, 'learning_rate': 1.2e-05, 'epoch': 0.76}\n",
            " 76% 47500/62500 [30:58<09:20, 26.74it/s][INFO|trainer.py:1885] 2021-05-15 15:20:31,216 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-47500\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:20:31,221 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-47500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:20:31,399 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-47500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:20:31,404 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-47500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:20:31,408 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-47500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:20:31,803 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-46500] due to args.save_total_limit\n",
            "{'loss': 6.0056, 'learning_rate': 1.16e-05, 'epoch': 0.77}\n",
            " 77% 48000/62500 [31:18<09:30, 25.43it/s][INFO|trainer.py:1885] 2021-05-15 15:20:51,104 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-48000\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:20:51,109 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-48000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:20:51,273 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-48000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:20:51,277 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-48000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:20:51,281 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-48000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:20:51,680 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-47000] due to args.save_total_limit\n",
            "{'loss': 5.923, 'learning_rate': 1.1200000000000001e-05, 'epoch': 0.78}\n",
            " 78% 48500/62500 [31:38<09:53, 23.60it/s][INFO|trainer.py:1885] 2021-05-15 15:21:11,007 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-48500\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:21:11,012 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-48500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:21:11,166 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-48500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:21:11,172 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-48500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:21:11,176 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-48500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:21:11,552 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-47500] due to args.save_total_limit\n",
            "{'loss': 6.0184, 'learning_rate': 1.08e-05, 'epoch': 0.78}\n",
            " 78% 49000/62500 [31:58<08:44, 25.72it/s][INFO|trainer.py:1885] 2021-05-15 15:21:30,905 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-49000\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:21:30,910 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-49000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:21:31,062 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-49000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:21:31,067 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-49000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:21:31,070 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-49000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:21:31,473 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-48000] due to args.save_total_limit\n",
            "{'loss': 5.9337, 'learning_rate': 1.04e-05, 'epoch': 0.79}\n",
            " 79% 49500/62500 [32:18<08:58, 24.16it/s][INFO|trainer.py:1885] 2021-05-15 15:21:51,087 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-49500\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:21:51,093 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-49500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:21:51,252 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-49500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:21:51,256 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-49500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:21:51,260 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-49500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:21:51,630 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-48500] due to args.save_total_limit\n",
            "{'loss': 5.9115, 'learning_rate': 1e-05, 'epoch': 0.8}\n",
            " 80% 50000/62500 [32:38<07:52, 26.47it/s][INFO|trainer.py:1885] 2021-05-15 15:22:11,154 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-50000\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:22:11,173 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-50000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:22:11,346 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-50000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:22:11,350 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-50000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:22:11,354 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-50000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:22:11,734 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-49000] due to args.save_total_limit\n",
            "{'loss': 5.8907, 'learning_rate': 9.600000000000001e-06, 'epoch': 0.81}\n",
            " 81% 50500/62500 [32:58<07:44, 25.83it/s][INFO|trainer.py:1885] 2021-05-15 15:22:30,817 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-50500\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:22:30,822 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-50500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:22:30,983 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-50500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:22:30,988 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-50500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:22:31,010 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-50500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:22:31,397 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-49500] due to args.save_total_limit\n",
            "{'loss': 5.9288, 'learning_rate': 9.2e-06, 'epoch': 0.82}\n",
            " 82% 51000/62500 [33:18<07:09, 26.77it/s][INFO|trainer.py:1885] 2021-05-15 15:22:50,698 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-51000\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:22:50,704 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-51000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:22:50,858 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-51000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:22:50,863 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-51000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:22:50,867 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-51000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:22:51,260 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-50000] due to args.save_total_limit\n",
            "{'loss': 5.9429, 'learning_rate': 8.8e-06, 'epoch': 0.82}\n",
            " 82% 51500/62500 [33:37<07:26, 24.66it/s][INFO|trainer.py:1885] 2021-05-15 15:23:10,131 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-51500\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:23:10,136 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-51500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:23:10,294 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-51500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:23:10,299 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-51500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:23:10,304 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-51500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:23:10,698 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-50500] due to args.save_total_limit\n",
            "{'loss': 5.94, 'learning_rate': 8.400000000000001e-06, 'epoch': 0.83}\n",
            " 83% 52000/62500 [33:57<06:47, 25.77it/s][INFO|trainer.py:1885] 2021-05-15 15:23:29,793 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-52000\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:23:29,797 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-52000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:23:29,942 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-52000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:23:29,946 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-52000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:23:29,949 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-52000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:23:30,352 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-51000] due to args.save_total_limit\n",
            "{'loss': 5.9205, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.84}\n",
            " 84% 52500/62500 [34:17<06:06, 27.32it/s][INFO|trainer.py:1885] 2021-05-15 15:23:49,993 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-52500\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:23:49,998 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-52500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:23:50,151 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-52500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:23:50,156 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-52500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:23:50,159 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-52500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:23:50,524 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-51500] due to args.save_total_limit\n",
            "{'loss': 5.9346, 'learning_rate': 7.6e-06, 'epoch': 0.85}\n",
            " 85% 53000/62500 [34:36<06:13, 25.41it/s][INFO|trainer.py:1885] 2021-05-15 15:24:09,512 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-53000\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:24:09,517 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-53000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:24:09,670 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-53000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:24:09,675 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-53000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:24:09,678 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-53000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:24:10,041 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-52000] due to args.save_total_limit\n",
            "{'loss': 5.9051, 'learning_rate': 7.2e-06, 'epoch': 0.86}\n",
            " 86% 53500/62500 [34:56<05:52, 25.57it/s][INFO|trainer.py:1885] 2021-05-15 15:24:29,031 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-53500\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:24:29,036 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-53500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:24:29,191 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-53500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:24:29,211 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-53500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:24:29,215 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-53500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:24:29,582 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-52500] due to args.save_total_limit\n",
            "{'loss': 5.9218, 'learning_rate': 6.800000000000001e-06, 'epoch': 0.86}\n",
            " 86% 54000/62500 [35:15<05:13, 27.15it/s][INFO|trainer.py:1885] 2021-05-15 15:24:48,434 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-54000\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:24:48,440 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-54000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:24:48,588 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-54000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:24:48,593 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-54000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:24:48,596 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-54000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:24:48,982 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-53000] due to args.save_total_limit\n",
            "{'loss': 5.9239, 'learning_rate': 6.4000000000000006e-06, 'epoch': 0.87}\n",
            " 87% 54500/62500 [35:35<04:38, 28.72it/s][INFO|trainer.py:1885] 2021-05-15 15:25:07,749 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-54500\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:25:07,754 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-54500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:25:07,903 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-54500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:25:07,907 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-54500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:25:07,914 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-54500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:25:08,582 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-53500] due to args.save_total_limit\n",
            "{'loss': 5.9365, 'learning_rate': 6e-06, 'epoch': 0.88}\n",
            " 88% 55000/62500 [35:54<04:22, 28.55it/s][INFO|trainer.py:1885] 2021-05-15 15:25:27,520 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-55000\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:25:27,526 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-55000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:25:27,673 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-55000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:25:27,678 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-55000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:25:27,681 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-55000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:25:28,069 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-54000] due to args.save_total_limit\n",
            "{'loss': 5.8333, 'learning_rate': 5.600000000000001e-06, 'epoch': 0.89}\n",
            " 89% 55500/62500 [36:14<04:16, 27.30it/s][INFO|trainer.py:1885] 2021-05-15 15:25:46,894 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-55500\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:25:46,899 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-55500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:25:47,048 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-55500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:25:47,053 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-55500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:25:47,056 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-55500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:25:47,428 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-54500] due to args.save_total_limit\n",
            "{'loss': 5.9289, 'learning_rate': 5.2e-06, 'epoch': 0.9}\n",
            " 90% 56000/62500 [36:33<04:01, 26.87it/s][INFO|trainer.py:1885] 2021-05-15 15:26:06,413 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-56000\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:26:06,418 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-56000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:26:06,565 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-56000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:26:06,570 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-56000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:26:06,574 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-56000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:26:06,954 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-55000] due to args.save_total_limit\n",
            "{'loss': 5.8953, 'learning_rate': 4.800000000000001e-06, 'epoch': 0.9}\n",
            " 90% 56500/62500 [36:53<03:40, 27.22it/s][INFO|trainer.py:1885] 2021-05-15 15:26:26,013 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-56500\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:26:26,021 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-56500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:26:26,181 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-56500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:26:26,185 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-56500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:26:26,189 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-56500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:26:26,559 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-55500] due to args.save_total_limit\n",
            "{'loss': 5.8833, 'learning_rate': 4.4e-06, 'epoch': 0.91}\n",
            " 91% 57000/62500 [37:12<03:20, 27.36it/s][INFO|trainer.py:1885] 2021-05-15 15:26:45,272 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-57000\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:26:45,277 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-57000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:26:45,778 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-57000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:26:45,784 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-57000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:26:45,843 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-57000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:26:46,204 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-56000] due to args.save_total_limit\n",
            "{'loss': 5.8692, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.92}\n",
            " 92% 57500/62500 [37:32<03:06, 26.74it/s][INFO|trainer.py:1885] 2021-05-15 15:27:05,110 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-57500\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:27:05,116 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-57500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:27:05,275 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-57500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:27:05,280 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-57500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:27:05,285 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-57500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:27:05,663 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-56500] due to args.save_total_limit\n",
            "{'loss': 5.8759, 'learning_rate': 3.6e-06, 'epoch': 0.93}\n",
            " 93% 58000/62500 [37:52<02:44, 27.42it/s][INFO|trainer.py:1885] 2021-05-15 15:27:24,748 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-58000\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:27:24,753 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-58000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:27:24,905 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-58000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:27:24,909 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-58000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:27:24,913 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-58000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:27:25,314 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-57000] due to args.save_total_limit\n",
            "{'loss': 5.8594, 'learning_rate': 3.2000000000000003e-06, 'epoch': 0.94}\n",
            " 94% 58500/62500 [38:11<02:28, 26.96it/s][INFO|trainer.py:1885] 2021-05-15 15:27:44,355 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-58500\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:27:44,361 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-58500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:27:44,514 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-58500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:27:44,521 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-58500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:27:44,524 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-58500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:27:45,225 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-57500] due to args.save_total_limit\n",
            "{'loss': 5.8682, 'learning_rate': 2.8000000000000003e-06, 'epoch': 0.94}\n",
            " 94% 59000/62500 [38:31<02:08, 27.24it/s][INFO|trainer.py:1885] 2021-05-15 15:28:04,100 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-59000\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:28:04,106 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-59000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:28:04,254 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-59000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:28:04,258 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-59000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:28:04,262 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-59000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:28:04,633 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-58000] due to args.save_total_limit\n",
            "{'loss': 5.806, 'learning_rate': 2.4000000000000003e-06, 'epoch': 0.95}\n",
            " 95% 59500/62500 [38:51<01:58, 25.31it/s][INFO|trainer.py:1885] 2021-05-15 15:28:23,624 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-59500\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:28:23,630 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-59500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:28:23,784 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-59500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:28:23,789 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-59500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:28:23,793 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-59500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:28:24,174 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-58500] due to args.save_total_limit\n",
            "{'loss': 5.8556, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.96}\n",
            " 96% 60000/62500 [39:10<01:36, 25.92it/s][INFO|trainer.py:1885] 2021-05-15 15:28:43,193 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-60000\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:28:43,199 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-60000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:28:43,354 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-60000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:28:43,388 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-60000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:28:43,396 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-60000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:28:43,777 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-59000] due to args.save_total_limit\n",
            "{'loss': 5.8841, 'learning_rate': 1.6000000000000001e-06, 'epoch': 0.97}\n",
            " 97% 60500/62500 [39:30<01:16, 26.12it/s][INFO|trainer.py:1885] 2021-05-15 15:29:03,042 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-60500\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:29:03,048 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-60500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:29:03,214 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-60500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:29:03,219 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-60500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:29:03,223 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-60500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:29:03,606 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-59500] due to args.save_total_limit\n",
            "{'loss': 5.8364, 'learning_rate': 1.2000000000000002e-06, 'epoch': 0.98}\n",
            " 98% 61000/62500 [39:50<01:00, 24.91it/s][INFO|trainer.py:1885] 2021-05-15 15:29:23,093 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-61000\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:29:23,098 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-61000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:29:23,265 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-61000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:29:23,270 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-61000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:29:23,273 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-61000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:29:23,677 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-60000] due to args.save_total_limit\n",
            "{'loss': 5.8724, 'learning_rate': 8.000000000000001e-07, 'epoch': 0.98}\n",
            " 98% 61500/62500 [40:10<00:34, 28.59it/s][INFO|trainer.py:1885] 2021-05-15 15:29:42,858 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-61500\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:29:42,864 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-61500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:29:43,033 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-61500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:29:43,037 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-61500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:29:43,041 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-61500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:29:43,409 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-60500] due to args.save_total_limit\n",
            "{'loss': 5.8785, 'learning_rate': 4.0000000000000003e-07, 'epoch': 0.99}\n",
            " 99% 62000/62500 [40:29<00:17, 27.87it/s][INFO|trainer.py:1885] 2021-05-15 15:30:02,538 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-62000\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:30:02,543 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-62000/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:30:02,702 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-62000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:30:02,708 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-62000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:30:02,711 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-62000/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:30:03,081 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-61000] due to args.save_total_limit\n",
            "{'loss': 5.8737, 'learning_rate': 0.0, 'epoch': 1.0}\n",
            "100% 62500/62500 [40:49<00:00, 27.20it/s][INFO|trainer.py:1885] 2021-05-15 15:30:21,918 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-62500\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:30:21,923 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-62500/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:30:22,071 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-62500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:30:22,076 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-62500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:30:22,079 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-62500/special_tokens_map.json\n",
            "[INFO|trainer.py:1953] 2021-05-15 15:30:22,471 >> Deleting older checkpoint [/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/checkpoint-61500] due to args.save_total_limit\n",
            "[INFO|trainer.py:1341] 2021-05-15 15:30:22,513 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 2449.9588, 'train_samples_per_second': 25.511, 'epoch': 1.0}\n",
            "100% 62500/62500 [40:49<00:00, 25.51it/s]\n",
            "[INFO|trainer.py:1885] 2021-05-15 15:30:22,764 >> Saving model checkpoint to /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model\n",
            "[INFO|configuration_utils.py:351] 2021-05-15 15:30:22,992 >> Configuration saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-05-15 15:30:23,118 >> Model weights saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-15 15:30:23,122 >> tokenizer config file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-15 15:30:23,125 >> Special tokens file saved in /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special2/model/special_tokens_map.json\n",
            "[INFO|trainer_pt_utils.py:907] 2021-05-15 15:30:23,177 >> ***** train metrics *****\n",
            "[INFO|trainer_pt_utils.py:912] 2021-05-15 15:30:23,178 >>   epoch                      =        1.0\n",
            "[INFO|trainer_pt_utils.py:912] 2021-05-15 15:30:23,178 >>   init_mem_cpu_alloc_delta   =     2598MB\n",
            "[INFO|trainer_pt_utils.py:912] 2021-05-15 15:30:23,178 >>   init_mem_cpu_peaked_delta  =        0MB\n",
            "[INFO|trainer_pt_utils.py:912] 2021-05-15 15:30:23,178 >>   init_mem_gpu_alloc_delta   =       44MB\n",
            "[INFO|trainer_pt_utils.py:912] 2021-05-15 15:30:23,178 >>   init_mem_gpu_peaked_delta  =        0MB\n",
            "[INFO|trainer_pt_utils.py:912] 2021-05-15 15:30:23,178 >>   train_mem_cpu_alloc_delta  =       13MB\n",
            "[INFO|trainer_pt_utils.py:912] 2021-05-15 15:30:23,178 >>   train_mem_cpu_peaked_delta =       45MB\n",
            "[INFO|trainer_pt_utils.py:912] 2021-05-15 15:30:23,178 >>   train_mem_gpu_alloc_delta  =      132MB\n",
            "[INFO|trainer_pt_utils.py:912] 2021-05-15 15:30:23,178 >>   train_mem_gpu_peaked_delta =      847MB\n",
            "[INFO|trainer_pt_utils.py:912] 2021-05-15 15:30:23,178 >>   train_runtime              = 0:40:49.95\n",
            "[INFO|trainer_pt_utils.py:912] 2021-05-15 15:30:23,178 >>   train_samples              =     500000\n",
            "[INFO|trainer_pt_utils.py:912] 2021-05-15 15:30:23,178 >>   train_samples_per_second   =     25.511\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6R8TLMISRaPA"
      },
      "source": [
        "tokenizer = AlbertTokenizerFast.from_pretrained(albet_tokenizer_model+'_special2/model')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jEPdRLmRbGT",
        "outputId": "4551a694-b955-43fa-ad7c-3cf034e92bbe"
      },
      "source": [
        "op = tokenizer(\"나는 오늘 학교에 간다.\", return_tensors=\"pt\")\n",
        "print(op)\n",
        "print(\"Tokens (str)      : {}\".format([tokenizer.convert_ids_to_tokens(s) for s in op['input_ids'].tolist()[0]]))\n",
        "print(\"Tokens (int)      : {}\".format(op['input_ids'].tolist()[0]))\n",
        "print(\"Tokens (attn_mask): {}\\n\".format(op['attention_mask'].tolist()[0]))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'input_ids': tensor([[   5,  172,  274,  402,  720,  368, 1870, 2802,    6]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
            "Tokens (str)      : ['[CLS]', '▁나', '는', '▁오늘', '▁학교', '에', '▁간다', '.', '[SEP]']\n",
            "Tokens (int)      : [5, 172, 274, 402, 720, 368, 1870, 2802, 6]\n",
            "Tokens (attn_mask): [1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4twnD81qaRR",
        "outputId": "9f12a97d-6fe4-481d-e5b8-bc3cfe0887e3"
      },
      "source": [
        "tokenizer.save_pretrained('/content/drive/MyDrive/korean_albert')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/korean_albert/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/korean_albert/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/korean_albert/spiece.model',\n",
              " '/content/drive/MyDrive/korean_albert/added_tokens.json',\n",
              " '/content/drive/MyDrive/korean_albert/tokenizer.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "To8JmvtkqwTN"
      },
      "source": [
        "tokenizer = AlbertTokenizerFast.from_pretrained('/content/drive/MyDrive/korean_albert')"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4ghZhMYuO38"
      },
      "source": [
        "# 기존 albert-kor-base 사용 \n",
        "\n",
        "https://huggingface.co/kykim/albert-kor-base"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245,
          "referenced_widgets": [
            "44757a3a7afe4e4eb95e95f11e94baed",
            "fee20f84e6204e7bad616f22de2b85ba",
            "673b5f7827d7437f92e1f3240161aa5a",
            "9170fc91ba304ad8b6a4c2d2ccb31355",
            "e561f9e07090434cb77090aeddc4789a",
            "de0381976d7c4fc68b06665cbae12834",
            "6d81c8e6bbaa47139a9161791c49e526",
            "db5a8e04fa0949de9cfc76eb7494f9ca",
            "f6fb86e6f6914670968ad70d09ab5852",
            "bd03d7a79e02416ebcdb5f6193414fe3",
            "b656b044f5964b18a69be602cf391abf",
            "3bb580a2181641c48ca8a14da6e7c2a0",
            "4ceff003d619437d8ecae1bf080db110",
            "c807b711d5e84e2fb60db9716f57a004",
            "6c478a8fd1214967a49ba63b7a5f5c2d",
            "2a15c90485f04cdab63dcd9704bc68d7",
            "cc2d8144b76b471a824dddd49b6c5889",
            "51dca6c50af5412994758f1c854a78de",
            "2448a43b9a3645cebcabe6a5c21b0d6f",
            "0c511bccc20b4815a1ff1a54d40f94da",
            "508cd332718045e290ac9a43cf950af3",
            "42119f7a708b411b93a877db42feae49",
            "5dad0c302f7d4533ada1482eebc97809",
            "73e88e9c09f44e86a9ae023ec7f6ed21"
          ]
        },
        "id": "OCTh5ViFuWUs",
        "outputId": "f56a2dd5-3949-42b5-b5c2-64f8ca9009f5"
      },
      "source": [
        "from transformers import BertTokenizerFast, AlbertModel\n",
        "\n",
        "tokenizer = BertTokenizerFast.from_pretrained('kykim/albert-kor-base')\n",
        "model = AlbertModel.from_pretrained('kykim/albert-kor-base')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "44757a3a7afe4e4eb95e95f11e94baed",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=344259.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f6fb86e6f6914670968ad70d09ab5852",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=684.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cc2d8144b76b471a824dddd49b6c5889",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=53325832.0, style=ProgressStyle(descrip…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at kykim/albert-kor-base were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'sop_classifier.classifier.bias', 'sop_classifier.classifier.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight']\n",
            "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfSy-90kvAqH",
        "outputId": "a771a469-dcab-4393-9de7-45c47785d1c7"
      },
      "source": [
        "tokenizer.save_pretrained('/content/drive/MyDrive/Tokenizer_train/albert-kor-base')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/Tokenizer_train/albert-kor-base/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/Tokenizer_train/albert-kor-base/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/Tokenizer_train/albert-kor-base/vocab.txt',\n",
              " '/content/drive/MyDrive/Tokenizer_train/albert-kor-base/added_tokens.json',\n",
              " '/content/drive/MyDrive/Tokenizer_train/albert-kor-base/tokenizer.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEb3ISnMvXnA"
      },
      "source": [
        "model.save_pretrained('/content/drive/MyDrive/Tokenizer_train/albert-kor-base')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elahPhO3vijH",
        "outputId": "5d1af88a-f870-4628-a379-06682e86c397"
      },
      "source": [
        "from transformers import AlbertTokenizerFast, AlbertForQuestionAnswering\n",
        "\n",
        "tokenizer = AlbertTokenizerFast.from_pretrained('/content/drive/MyDrive/Tokenizer_train/albert-kor-base')\n",
        "model = AlbertForQuestionAnswering.from_pretrained('/content/drive/MyDrive/Tokenizer_train/albert-kor-base')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of AlbertForQuestionAnswering were not initialized from the model checkpoint at /content/drive/MyDrive/Tokenizer_train/albert-kor-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JiwgEyUiv3E1",
        "outputId": "0084515b-307e-4952-8786-74d46f7943dd"
      },
      "source": [
        "op = tokenizer(\"나는 오늘 학교에 간다.\", return_tensors=\"pt\")\n",
        "print(op)\n",
        "print(\"Tokens (str)      : {}\".format([tokenizer.convert_ids_to_tokens(s) for s in op['input_ids'].tolist()[0]]))\n",
        "print(\"Tokens (int)      : {}\".format(op['input_ids'].tolist()[0]))\n",
        "print(\"Tokens (attn_mask): {}\\n\".format(op['attention_mask'].tolist()[0]))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'input_ids': tensor([[    2, 14264, 14098, 25439, 21096,  2016,     3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n",
            "Tokens (str)      : ['[CLS]', '나는', '오늘', '학교에', '간다', '.', '[SEP]']\n",
            "Tokens (int)      : [2, 14264, 14098, 25439, 21096, 2016, 3]\n",
            "Tokens (attn_mask): [1, 1, 1, 1, 1, 1, 1]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SN_gSqSrJQAL"
      },
      "source": [
        "# KoQuAD1.0 데이터 Load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277,
          "referenced_widgets": [
            "e80484b7bf7e42d8955a783527efa449",
            "8594184c30fd4f42884018665132071c",
            "bff33c907fae4468992a75d52c9b45cb",
            "bdbd5317006d4cc988f2be327b60b9cc",
            "1e0dea7098b14f9aaa9c05c45506771f",
            "27cf9f0a2ee1454e87c7f29425996149",
            "ba6b60c555d54ef7a5340884d7402a1e",
            "3deff3b92bbd4d5d90a9892b9bbf9b2b",
            "37032dfb4c0447c99452bc2e1ed224cf",
            "6b87adb4e2ad44e58eac98160c435707",
            "b05a6f92b18846fdaef17065384a4f6d",
            "5a2b32d20b684cfbad69671b1096d735",
            "6a77122558a04f71adb0ef07660f001f",
            "adff7f3201f44b959c075871f1e3763f",
            "158279eddc0b4daf95694635fb1e9316",
            "bb89231a6fab474db83927011f0799a3",
            "b47b97389a99431cbae5685a6b1dad88",
            "67e36562ff6e4f10875a151f38de1849",
            "b7d7104883a24660845acba3f4e1be67",
            "831402e822244e3dbe4859edd8a1e7e2",
            "33e67f2c219d4cee8cfc55b62da5c447",
            "ab065fb1a10b4a079a5db50ca171184c",
            "62d20f0a59fc4ef5866b5b318fca0774",
            "6fc869f1b5c9456994968eeb848bcaeb",
            "5ab973ee2a8b4c7487679b5749c5d988",
            "b79cb0d6551f4e6da16addea54df8089",
            "76273585da6e465e87b43791649e7c14",
            "2df0d84d9ec542e5b59dc17772440871",
            "1094cb85854f4804bec39b414d8c346a",
            "a9c85025461c44a0adec75f6b77c1c3e",
            "2c7b2acabfbf4cd8892560b657aca898",
            "617ee24100854a0fb804092c7ae73c72",
            "a2663bcd7d2b46e89ed6239c56a8c0c8",
            "2a815e7963e347ac9e9a94d8bc8bc261",
            "64ecb12825ab420ca3958762c024481c",
            "d6c45b4eca264116a3fa09c56735988e",
            "9184d682db724db1bfaf8be7641df92e",
            "adcec87047594835a0cb4af50a148f03",
            "c9b1770e22b14fdca79c58c221ccede9",
            "8e9a855880254e3b8622b6605077ba04",
            "9640d999963b4e85b39e1b16d36e55f9",
            "c18d2b23f4e942b1b4e35801e165819f",
            "6967c92c15c84363a7fcba6f9e4dae8e",
            "02cc2b431c9c4bd2a29d3c84d1c08706",
            "c9f49e619a414fa0848ff3eff55224ee",
            "43c00307ffe84f6db6a09ce92d83c0ff",
            "eca6b0e7275e433eb5198045de51121d",
            "11f0f543b5204d33992272bdb4817545"
          ]
        },
        "id": "GxHkhgjRJVFv",
        "outputId": "6acb1064-86c5-4717-f45c-5f5c7045e113"
      },
      "source": [
        "from datasets import list_datasets, load_dataset, list_metrics, load_metric, load_from_disk\n",
        "\n",
        "squad_dataset = load_dataset('squad_kor_v1') "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e80484b7bf7e42d8955a783527efa449",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1710.0, style=ProgressStyle(description…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "37032dfb4c0447c99452bc2e1ed224cf",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=962.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Downloading and preparing dataset squad_kor_v1/squad_kor_v1 (download: 40.44 MiB, generated: 87.40 MiB, post-processed: Unknown size, total: 127.84 MiB) to /root/.cache/huggingface/datasets/squad_kor_v1/squad_kor_v1/1.0.0/92f88eedc7d67b3f38389e8682eabe68caa450442cc4f7370a27873dbc045fe4...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b47b97389a99431cbae5685a6b1dad88",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=7568316.0, style=ProgressStyle(descript…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5ab973ee2a8b4c7487679b5749c5d988",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=770480.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a2663bcd7d2b46e89ed6239c56a8c0c8",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9640d999963b4e85b39e1b16d36e55f9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\rDataset squad_kor_v1 downloaded and prepared to /root/.cache/huggingface/datasets/squad_kor_v1/squad_kor_v1/1.0.0/92f88eedc7d67b3f38389e8682eabe68caa450442cc4f7370a27873dbc045fe4. Subsequent calls will reuse this data.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqU8Q1p2JkJL",
        "outputId": "4026ef33-2940-42cb-81d7-944cf0b6e86f"
      },
      "source": [
        "%%time\n",
        "\n",
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "\n",
        "squad_dataset_train = {}\n",
        "squad_dataset_train['id'] = []\n",
        "squad_dataset_train['title'] = []\n",
        "squad_dataset_train['context'] = []\n",
        "squad_dataset_train['question'] = []\n",
        "squad_dataset_train['answers'] = []\n",
        "\n",
        "\n",
        "for sdt in squad_dataset['train']:\n",
        "    if  len(sdt['answers']['answer_start']) > 0:\n",
        "        squad_dataset_train['id'].append(sdt['id'])\n",
        "        squad_dataset_train['title'].append(sdt['title'])\n",
        "        squad_dataset_train['context'].append(sdt['context'])\n",
        "        squad_dataset_train['question'].append(sdt['question'])\n",
        "        squad_dataset_train['answers'].append(sdt['answers'])\n",
        "\n",
        "dataset_train = Dataset.from_dict(squad_dataset_train)\n",
        "\n",
        "squad_dataset_val = {}\n",
        "squad_dataset_val['id'] = []\n",
        "squad_dataset_val['title'] = []\n",
        "squad_dataset_val['context'] = []\n",
        "squad_dataset_val['question'] = []\n",
        "squad_dataset_val['answers'] = []\n",
        "\n",
        "for sdt in squad_dataset['validation']:\n",
        "    if  len(sdt['answers']['answer_start']) > 0:\n",
        "        squad_dataset_val['id'].append(sdt['id'])\n",
        "        squad_dataset_val['title'].append(sdt['title'])\n",
        "        squad_dataset_val['context'].append(sdt['context'])\n",
        "        squad_dataset_val['question'].append(sdt['question'])\n",
        "        squad_dataset_val['answers'].append(sdt['answers'])\n",
        "\n",
        "dataset_val = Dataset.from_dict(squad_dataset_val)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 8.84 s, sys: 350 ms, total: 9.19 s\n",
            "Wall time: 8.98 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohLyW4TgJv1r",
        "outputId": "be22ebbe-5372-4793-e04d-a1658a9f5b32"
      },
      "source": [
        "%%time\n",
        "\n",
        "train_contexts = dataset_train['context']\n",
        "train_questions = dataset_train['question']\n",
        "train_answers = dataset_train['answers']\n",
        "\n",
        "val_contexts = dataset_val['context']\n",
        "val_questions = dataset_val['question']\n",
        "val_answers = dataset_val['answers']"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1.34 s, sys: 66.5 ms, total: 1.4 s\n",
            "Wall time: 1.39 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqvCK2x9J6TG"
      },
      "source": [
        "def add_end_idx(answers, contexts):\n",
        "    for answer, context in zip(answers, contexts):\n",
        "        try:\n",
        "        \n",
        "            gold_text = answer['text']\n",
        "            if isinstance(gold_text,list):\n",
        "                gold_text = gold_text[0]\n",
        "\n",
        "            start_idx = answer['answer_start']\n",
        "            if isinstance(start_idx,list):\n",
        "                start_idx = start_idx[0]\n",
        "\n",
        "            end_idx = start_idx + len(gold_text)\n",
        "\n",
        "            # sometimes squad answers are off by a character or two – fix this\n",
        "            if context[start_idx:end_idx] == gold_text:\n",
        "                answer['answer_end'] = [end_idx]\n",
        "            elif context[start_idx-1:end_idx-1] == gold_text:\n",
        "                answer['answer_start'] = [start_idx - 1]\n",
        "                answer['answer_end'] = [end_idx - 1]     # When the gold label is off by one character\n",
        "            elif context[start_idx-2:end_idx-2] == gold_text:\n",
        "                answer['answer_start'] = [start_idx - 2]\n",
        "                answer['answer_end'] = [end_idx - 2]     # When the gold label is off by two characters\n",
        "        except Exception as e:\n",
        "            pass\n",
        "            #print('context',context)\n",
        "            #print('gold_text',gold_text)\n",
        "\n",
        "\n",
        "add_end_idx(train_answers, train_contexts)\n",
        "add_end_idx(val_answers, val_contexts)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8hTx2G5J7jD"
      },
      "source": [
        "\n",
        "train_encodings = tokenizer(train_contexts, train_questions, max_length=256, truncation=True, padding=True)\n",
        "val_encodings = tokenizer(val_contexts, val_questions, max_length=256, truncation=True, padding=True)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2631889UxcA"
      },
      "source": [
        "squad_dataset = None"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfWPKZFDKFPr"
      },
      "source": [
        "def add_token_positions(encodings, answers):\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "    for i in range(len(answers)):\n",
        "        start_positions.append(encodings.char_to_token(i, answers[i]['answer_start'][0]))\n",
        "        end_positions.append(encodings.char_to_token(i, answers[i]['answer_end'][0] - 1))\n",
        "\n",
        "        # if start position is None, the answer passage has been truncated\n",
        "        if start_positions[-1] is None:\n",
        "            start_positions[-1] = tokenizer.model_max_length\n",
        "        if end_positions[-1] is None:\n",
        "            end_positions[-1] = tokenizer.model_max_length\n",
        "\n",
        "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
        "\n",
        "add_token_positions(train_encodings, train_answers)\n",
        "add_token_positions(val_encodings, val_answers)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVuhMvRgKIBO"
      },
      "source": [
        "import torch\n",
        "\n",
        "class SquadDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "    #def __getitem__(self, idx):\n",
        "    #    return {key: val[idx] for key, val in self.encodings.items()}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings.input_ids)\n",
        "\n",
        "train_dataset = SquadDataset(train_encodings)\n",
        "val_dataset = SquadDataset(val_encodings)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6ja7OlvLLeW"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from transformers import AdamW\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "optim = AdamW(model.parameters(), lr=5e-5)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdTGtftBLPpI"
      },
      "source": [
        "# Print iterations progress\n",
        "class ProgressBar:\n",
        "\n",
        "  def __init__(self,total=20, prefix = '', suffix = '', decimals = 1, length = 20, fill = '█', printEnd = \"\\r\"):\n",
        "    self.total = total\n",
        "    self.prefix = prefix\n",
        "    self.suffix = suffix\n",
        "    self.decimals = decimals\n",
        "    self.length = length\n",
        "    self.fill = fill\n",
        "    self.printEnd = printEnd\n",
        "    self.ite = 0\n",
        "\n",
        "  def printProgress(self,iteration, text):\n",
        "      self.ite += iteration\n",
        "      percent = (\"{0:.\" + str(self.decimals) + \"f}\").format(100 * (self.ite / float(self.total)))\n",
        "\n",
        "      filledLength = int(self.length * self.ite // self.total)\n",
        "      bar = self.fill * filledLength + '-' * (self.length - filledLength)\n",
        "      print(f'\\r{self.prefix} |{bar}| {percent}% {self.suffix}  {text}', end=\"\", flush=True)\n",
        "      # Print New Line on Complete\n",
        "      if self.ite == self.total: \n",
        "          print()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ePbOGWILUjQ",
        "outputId": "35a9f5e5-0edf-4e02-e025-0c5ac333cda2"
      },
      "source": [
        "\n",
        "batch_count = len(train_loader)\n",
        "model.to(device)\n",
        "model.train()\n",
        "#loss_graph = []\n",
        "epochs = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    #seeding(1024+epoch)\n",
        "    pb = ProgressBar(total=batch_count,prefix='Epoch '+ str(epoch+1) + '/' + str(epochs))\n",
        "    current_batch = 0\n",
        "    for batch in train_loader:\n",
        "        current_batch+=1\n",
        "        #seeding(1024+epoch+current_batch)\n",
        "        optim.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        #print('input_ids',len(input_ids))\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        #print('attention_mask',len(attention_mask))\n",
        "        start_positions = batch['start_positions'].to(device)\n",
        "        #print('start_positions',len(start_positions))\n",
        "        end_positions = batch['end_positions'].to(device)\n",
        "        #print('end_positions',len(end_positions))\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
        "        loss = outputs[0]\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        pb.printProgress(+1,'loss:' + str(loss))\n",
        "        #loss_graph.append(loss.item())\n",
        "    model.save_pretrained('/content/drive/MyDrive/korQuAD1.0/model')\n",
        "\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10 |--------------------| 2.9%   loss:tensor(0.4774, device='cuda:0', grad_fn=<DivBackward0>)"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCnkYdpoLeh1"
      },
      "source": [
        "model = AlbertForQuestionAnswering.from_pretrained('/content/drive/MyDrive/korQuAD1.0/model')\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)\n",
        "\n",
        "examples = dataset_val\n",
        "\n",
        "def display_example(example):    \n",
        "    from pprint import pprint\n",
        "\n",
        "    #idx = qid_to_example_index[qid]\n",
        "    q = example['question']\n",
        "    c = example['context']\n",
        "    a = example['answers']['text']\n",
        "    \n",
        "    print(f'Example {example[\"id\"]} of {len(examples)}\\n---------------------')\n",
        "    print(f\"Q: {q}\\n\")\n",
        "    print(\"Context:\")\n",
        "    pprint(c)\n",
        "    print(f\"\\nTrue Answers:\\n{a}\")\n",
        "\n",
        "from pprint import pprint\n",
        "\n",
        "def get_prediction(example,display=False):\n",
        "\n",
        "    if display:\n",
        "        display_example(example)\n",
        "    # given a question id (qas_id or qid), load the example, get the model outputs and generate an answer\n",
        "    question = example['question'] #examples[qid_to_example_index[qid]].question_text\n",
        "    context = example['context'] #examples[qid_to_example_index[qid]].context_text\n",
        "\n",
        "    #print(f\"Q: {question}\\n\")\n",
        "    #print(\"Context:\")\n",
        "    #pprint(context)\n",
        "\n",
        "    inputs = tokenizer.encode_plus(question, context, max_length=256,return_tensors='pt')\n",
        "\n",
        "    inputs.to(device)\n",
        "    \n",
        "    outputs = model(**inputs)\n",
        "\n",
        "    #print(outputs)\n",
        "    #print(outputs)    \n",
        "    answer_start = torch.argmax(outputs[0])  # get the most likely beginning of answer with the argmax of the score\n",
        "    answer_end = torch.argmax(outputs[1]) + 1 \n",
        "\n",
        "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][answer_start:answer_end]))\n",
        "    if display:\n",
        "        print('')\n",
        "        print('Predict answer:',answer)\n",
        "    return answer\n",
        "\n",
        " # these functions are heavily influenced by the HF squad_metrics.py script\n",
        "def normalize_text(s):\n",
        "    \"\"\"Removing articles and punctuation, and standardizing whitespace are all typical text processing steps.\"\"\"\n",
        "    import string, re\n",
        "\n",
        "    def remove_articles(text):\n",
        "        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
        "        return re.sub(regex, \" \", text)\n",
        "\n",
        "    def white_space_fix(text):\n",
        "        return \" \".join(text.split())\n",
        "\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return \"\".join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "def compute_exact_match(prediction, truth):\n",
        "    return int(normalize_text(prediction) == normalize_text(truth))\n",
        "\n",
        "def compute_f1(prediction, truth):\n",
        "    pred_tokens = normalize_text(prediction).split()\n",
        "    truth_tokens = normalize_text(truth).split()\n",
        "    \n",
        "    # if either the prediction or the truth is no-answer then f1 = 1 if they agree, 0 otherwise\n",
        "    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
        "        return int(pred_tokens == truth_tokens)\n",
        "    \n",
        "    common_tokens = set(pred_tokens) & set(truth_tokens)\n",
        "    \n",
        "    # if there are no common tokens then f1 = 0\n",
        "    if len(common_tokens) == 0:\n",
        "        return 0\n",
        "    \n",
        "    prec = len(common_tokens) / len(pred_tokens)\n",
        "    rec = len(common_tokens) / len(truth_tokens)\n",
        "    \n",
        "    return 2 * (prec * rec) / (prec + rec)\n",
        "\n",
        "def get_gold_answers(example):\n",
        "    \"\"\"helper function that retrieves all possible true answers from a squad2.0 example\"\"\"\n",
        "    \n",
        "    gold_answers = example['answers']['text'] #[answer[\"text\"] for answer in example['answers'] if answer[\"text\"]]\n",
        "\n",
        "    # if gold_answers doesn't exist it's because this is a negative example - \n",
        "    # the only correct answer is an empty string\n",
        "    if not gold_answers:\n",
        "        gold_answers = [\"\"]\n",
        "        \n",
        "    return gold_answers\n",
        "\n",
        "def evaluate_example(example,display=False):\n",
        "    gold_answers = get_gold_answers(example)\n",
        "    prediction = get_prediction(example)\n",
        "    em_score = max((compute_exact_match(prediction, answer)) for answer in gold_answers)\n",
        "    f1_score = max((compute_f1(prediction, answer)) for answer in gold_answers)\n",
        "    if display:\n",
        "        print(f\"Question: {example['question']}\")\n",
        "        print(f\"Prediction: {prediction}\")\n",
        "        print(f\"True Answers: {gold_answers}\")\n",
        "        print(f\"EM: {em_score} \\t F1: {f1_score}\")\n",
        "    return em_score,f1_score"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYMzvq5-LlXC",
        "outputId": "7df3d167-1c2a-45bd-ce8e-379ec360136b"
      },
      "source": [
        "em,f1 = evaluate_example(examples[0],display=True)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question: 임종석이 여의도 농민 폭력 시위를 주도한 혐의로 지명수배 된 날은?\n",
            "Prediction: 1989년 2월 15일\n",
            "True Answers: ['1989년 2월 15일']\n",
            "EM: 1 \t F1: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mF_CW4-uLob1",
        "outputId": "cde7ae95-692a-4820-8355-ed65a6a871a1"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "ems = []\n",
        "f1s = []\n",
        "pb = ProgressBar(total=len(examples),prefix='Evaluate... ')\n",
        "for example in examples:\n",
        "    em,f1 = evaluate_example(example)\n",
        "    ems.append(em)\n",
        "    f1s.append(f1)\n",
        "    pb.printProgress(+1,'EM:' + str(em) + '  F1:' + str(f1))\n",
        "print('EM:',np.mean(ems))\n",
        "print('F1:',np.mean(f1s))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluate...  |████████████████████| 100.0%   EM:0  F1:0.5\n",
            "EM: 0.48423969518531346\n",
            "F1: 0.5736512706343756\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}