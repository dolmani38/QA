{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyONFfeQYVQFjz7a4jGrsAhi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e0a234238a20442188bffeb50b8a82e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_508da04576294734b60f3fa90d40b2f5",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_31dda3106cc647fda699a60bf34a320c",
              "IPY_MODEL_7269bb94559540d8a2ad15890118c9a0"
            ]
          }
        },
        "508da04576294734b60f3fa90d40b2f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "31dda3106cc647fda699a60bf34a320c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4bbddc49854648ee809cab2744f19f03",
            "_dom_classes": [],
            "description": "Dl Completed...: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a6b91a068a2445da88b086a99123cca1"
          }
        },
        "7269bb94559540d8a2ad15890118c9a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a0c5df4e61074ded90aadad9d9123604",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 5/5 [01:57&lt;00:00, 23.44s/ url]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ffaa2c46811545579645138631dfcfb7"
          }
        },
        "4bbddc49854648ee809cab2744f19f03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a6b91a068a2445da88b086a99123cca1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a0c5df4e61074ded90aadad9d9123604": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ffaa2c46811545579645138631dfcfb7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b07bff8f8c0242cb9d14c4e9200fe065": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c138a58915804d3a82cdab0711e6ee13",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b2f387970f2b453db3beaedc6aaea0a0",
              "IPY_MODEL_10d6cdeac7824a1da0e7ee23a9857162"
            ]
          }
        },
        "c138a58915804d3a82cdab0711e6ee13": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b2f387970f2b453db3beaedc6aaea0a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8cde191a1d694cac973a9ff53c680c6f",
            "_dom_classes": [],
            "description": "Dl Size...: ",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f715a113aec4444f91d26969ea994d30"
          }
        },
        "10d6cdeac7824a1da0e7ee23a9857162": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8fad692c0da84f67b2981b9dc5085798",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 557/? [01:57&lt;00:00,  4.75 MiB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d94643c7f32d4c679260bb0abf305de3"
          }
        },
        "8cde191a1d694cac973a9ff53c680c6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f715a113aec4444f91d26969ea994d30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8fad692c0da84f67b2981b9dc5085798": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d94643c7f32d4c679260bb0abf305de3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a94f18110c324dc780e714e50491e9fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_be9e86e7e0b54aa1a8cceb4a7c44fbd9",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d6c60b10cea943e2b0fdba4f546ec849",
              "IPY_MODEL_d8100564205c4989b0cc9512ffdef942"
            ]
          }
        },
        "be9e86e7e0b54aa1a8cceb4a7c44fbd9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d6c60b10cea943e2b0fdba4f546ec849": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_83c6a479d4594efdaa7c8fdf758f86ef",
            "_dom_classes": [],
            "description": "Extraction completed...: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_edb7645bd9cf4ebcb0472ab8413f15ec"
          }
        },
        "d8100564205c4989b0cc9512ffdef942": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6dd9062c334a4755ba7b0a1bd86468ca",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2/2 [01:57&lt;00:00, 58.56s/ file]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0ecf99f564724d79b28843833d2e9c6b"
          }
        },
        "83c6a479d4594efdaa7c8fdf758f86ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "edb7645bd9cf4ebcb0472ab8413f15ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6dd9062c334a4755ba7b0a1bd86468ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0ecf99f564724d79b28843833d2e9c6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "141567d835724105af5415594e9a7f9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2293ab6f8444436d826a597626e581f1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e7df796c569f4432b158b7d360cea766",
              "IPY_MODEL_dc0e96d7d05e4ed0ac616431ffc17104"
            ]
          }
        },
        "2293ab6f8444436d826a597626e581f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e7df796c569f4432b158b7d360cea766": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_845e048e05db4a1ba2fbe08d84aefa40",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_15172d9909fd47dc98f1666eb3ab3102"
          }
        },
        "dc0e96d7d05e4ed0ac616431ffc17104": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_058be0a299de4e408294b8551c55bc63",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 222894/0 [15:10&lt;00:00, 231.40 examples/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6ecd6166ed4b41b9b09da5d04d26ada4"
          }
        },
        "845e048e05db4a1ba2fbe08d84aefa40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "15172d9909fd47dc98f1666eb3ab3102": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "058be0a299de4e408294b8551c55bc63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6ecd6166ed4b41b9b09da5d04d26ada4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "aa2bb3c703cd47bbae4c3725ebbe8607": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6d0cb13c40b94435b8cc28c53e04be9f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a82eba20d147451fae82808feebe2ee6",
              "IPY_MODEL_f8338f63e39e4297960120064bcd6cfa"
            ]
          }
        },
        "6d0cb13c40b94435b8cc28c53e04be9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a82eba20d147451fae82808feebe2ee6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3c771b594639409980363ad629509820",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 570,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 570,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_97618f3b65be4d0dbc736b7f5d57bbbc"
          }
        },
        "f8338f63e39e4297960120064bcd6cfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_aeeb3800b6e54c43947521d2b1db8bac",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 570/570 [00:15&lt;00:00, 36.9B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6765b2b669cb4ccc9b95023fd76b36b9"
          }
        },
        "3c771b594639409980363ad629509820": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "97618f3b65be4d0dbc736b7f5d57bbbc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "aeeb3800b6e54c43947521d2b1db8bac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6765b2b669cb4ccc9b95023fd76b36b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a5f744a372114cd1aa1b205d7fe38a78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_bd8b2b2d73e1470ba0c082828d499ef3",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6c79c2c1bee44c5487cdd7d54e4974ca",
              "IPY_MODEL_1b0c3cf5a34d4a23a50378e3111fbaab"
            ]
          }
        },
        "bd8b2b2d73e1470ba0c082828d499ef3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6c79c2c1bee44c5487cdd7d54e4974ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0cd1019099024c8bb465eb84371dd313",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 526681800,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 526681800,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_266830fdd6bf499698715a415a909773"
          }
        },
        "1b0c3cf5a34d4a23a50378e3111fbaab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_378f1fde5736470094c345f994541a65",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 527M/527M [00:14&lt;00:00, 36.3MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_17250d596553432d9a0c313cd3be22e8"
          }
        },
        "0cd1019099024c8bb465eb84371dd313": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "266830fdd6bf499698715a415a909773": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "378f1fde5736470094c345f994541a65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "17250d596553432d9a0c313cd3be22e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ad87e8b4d9dd4c50a623bc0c5b3231b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6ee83dcf024a4fc59dedf8ed6793c961",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ba3a7416f39b48ad98221f3445345c39",
              "IPY_MODEL_23a939dca94b4400a25b84a8f17feeca"
            ]
          }
        },
        "6ee83dcf024a4fc59dedf8ed6793c961": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ba3a7416f39b48ad98221f3445345c39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_03d48499e49a4b1588cf99006f1ee2ca",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 435779157,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 435779157,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8d0c6df303274deb900ec2e4e60645fc"
          }
        },
        "23a939dca94b4400a25b84a8f17feeca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_66de0432831d40a9b58728f36d6469e7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 436M/436M [07:32&lt;00:00, 963kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_86ccde2a1f0f44ea9c079baa0d7ccc88"
          }
        },
        "03d48499e49a4b1588cf99006f1ee2ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8d0c6df303274deb900ec2e4e60645fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "66de0432831d40a9b58728f36d6469e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "86ccde2a1f0f44ea9c079baa0d7ccc88": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dolmani38/QA/blob/main/koAlbert_Tokenizer_0515.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21tlIhtCjJia"
      },
      "source": [
        "# 한국어 Albert Tokenizer 학습!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zc1weyK_jPwQ",
        "outputId": "a9341d1d-61f1-466f-a310-c0475cdda6eb"
      },
      "source": [
        "\n",
        "if True:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5DRavVNRey3"
      },
      "source": [
        "## 참조\n",
        "\n",
        "https://colab.research.google.com/github/parmarsuraj99/suraj-parmar/blob/master/_notebooks/2020-05-02-SanskritALBERT.ipynb#scrollTo=VNAOMXjpMHZD\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "038IYm33Muol"
      },
      "source": [
        "import os\n",
        "import gc\n",
        "import glob\n",
        "import torch\n",
        "import pickle\n",
        "import joblib\n",
        "from tqdm.auto import tqdm"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVCIQF4UMmyF",
        "outputId": "fb01dbd4-f3b7-4f36-ad17-489e036fbe4d"
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers\n",
        "!pip install transformers/.\n",
        "!pip install sentencepiece==0.1.95\n",
        "!pip install datasets==1.6.2"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 72664, done.\u001b[K\n",
            "remote: Counting objects: 100% (700/700), done.\u001b[K\n",
            "remote: Compressing objects: 100% (366/366), done.\u001b[K\n",
            "remote: Total 72664 (delta 389), reused 498 (delta 294), pack-reused 71964\u001b[K\n",
            "Receiving objects: 100% (72664/72664), 56.16 MiB | 22.23 MiB/s, done.\n",
            "Resolving deltas: 100% (51525/51525), done.\n",
            "Processing ./transformers\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting huggingface-hub==0.0.8\n",
            "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 13.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (4.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (20.9)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (4.41.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 35.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.7.0.dev0) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.7.0.dev0) (3.4.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.7.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.7.0.dev0) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.7.0.dev0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.7.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.7.0.dev0) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.7.0.dev0) (8.0.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.7.0.dev0) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.7.0.dev0) (1.15.0)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.7.0.dev0-cp37-none-any.whl size=2259809 sha256=29e64cf8d568fdca5cc5a2a8702bf4c64f327d8b03d1a9107308ab2b67c84eff\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ofqkyy0j/wheels/23/19/dd/2561a4e47240cf6b307729d58e56f8077dd0c698f5992216cf\n",
            "Successfully built transformers\n",
            "Installing collected packages: huggingface-hub, tokenizers, sacremoses, transformers\n",
            "Successfully installed huggingface-hub-0.0.8 sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.7.0.dev0\n",
            "Collecting sentencepiece==0.1.95\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 6.7MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.95\n",
            "Collecting datasets==1.6.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/1a/b9f9b3bfef624686ae81c070f0a6bb635047b17cdb3698c7ad01281e6f9a/datasets-1.6.2-py3-none-any.whl (221kB)\n",
            "\u001b[K     |████████████████████████████████| 225kB 8.3MB/s \n",
            "\u001b[?25hCollecting fsspec\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/52/816d1a3a599176057bf29dfacb1f8fadb61d35fbd96cb1bab4aaa7df83c0/fsspec-2021.5.0-py3-none-any.whl (111kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 37.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets==1.6.2) (1.19.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets==1.6.2) (0.3.3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets==1.6.2) (4.0.1)\n",
            "Collecting xxhash\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/4f/0a862cad26aa2ed7a7cd87178cbbfa824fc1383e472d63596a0d018374e7/xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 31.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub<0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets==1.6.2) (0.0.8)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets==1.6.2) (0.70.11.1)\n",
            "Requirement already satisfied: pyarrow>=1.0.0<4.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets==1.6.2) (3.0.0)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets==1.6.2) (4.41.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets==1.6.2) (1.1.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets==1.6.2) (20.9)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets==1.6.2) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets==1.6.2) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets==1.6.2) (3.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets==1.6.2) (3.0.12)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.6.2) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.6.2) (2.8.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets==1.6.2) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.6.2) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.6.2) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.6.2) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.6.2) (2020.12.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets==1.6.2) (1.15.0)\n",
            "Installing collected packages: fsspec, xxhash, datasets\n",
            "Successfully installed datasets-1.6.2 fsspec-2021.5.0 xxhash-2.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQt6ukg-M5p9"
      },
      "source": [
        "## mecab 설치 참조\n",
        "\n",
        "https://somjang.tistory.com/entry/Google-Colab%EC%97%90%EC%84%9C-Mecab-koMecab-ko-dic-%EC%89%BD%EA%B2%8C-%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7E1YDPFdMslI",
        "outputId": "7eddce76-d321-49ee-b688-21ccba51ba2d"
      },
      "source": [
        "!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Mecab-ko-for-Google-Colab'...\n",
            "remote: Enumerating objects: 91, done.\u001b[K\n",
            "remote: Counting objects:   1% (1/91)\u001b[K\rremote: Counting objects:   2% (2/91)\u001b[K\rremote: Counting objects:   3% (3/91)\u001b[K\rremote: Counting objects:   4% (4/91)\u001b[K\rremote: Counting objects:   5% (5/91)\u001b[K\rremote: Counting objects:   6% (6/91)\u001b[K\rremote: Counting objects:   7% (7/91)\u001b[K\rremote: Counting objects:   8% (8/91)\u001b[K\rremote: Counting objects:   9% (9/91)\u001b[K\rremote: Counting objects:  10% (10/91)\u001b[K\rremote: Counting objects:  12% (11/91)\u001b[K\rremote: Counting objects:  13% (12/91)\u001b[K\rremote: Counting objects:  14% (13/91)\u001b[K\rremote: Counting objects:  15% (14/91)\u001b[K\rremote: Counting objects:  16% (15/91)\u001b[K\rremote: Counting objects:  17% (16/91)\u001b[K\rremote: Counting objects:  18% (17/91)\u001b[K\rremote: Counting objects:  19% (18/91)\u001b[K\rremote: Counting objects:  20% (19/91)\u001b[K\rremote: Counting objects:  21% (20/91)\u001b[K\rremote: Counting objects:  23% (21/91)\u001b[K\rremote: Counting objects:  24% (22/91)\u001b[K\rremote: Counting objects:  25% (23/91)\u001b[K\rremote: Counting objects:  26% (24/91)\u001b[K\rremote: Counting objects:  27% (25/91)\u001b[K\rremote: Counting objects:  28% (26/91)\u001b[K\rremote: Counting objects:  29% (27/91)\u001b[K\rremote: Counting objects:  30% (28/91)\u001b[K\rremote: Counting objects:  31% (29/91)\u001b[K\rremote: Counting objects:  32% (30/91)\u001b[K\rremote: Counting objects:  34% (31/91)\u001b[K\rremote: Counting objects:  35% (32/91)\u001b[K\rremote: Counting objects:  36% (33/91)\u001b[K\rremote: Counting objects:  37% (34/91)\u001b[K\rremote: Counting objects:  38% (35/91)\u001b[K\rremote: Counting objects:  39% (36/91)\u001b[K\rremote: Counting objects:  40% (37/91)\u001b[K\rremote: Counting objects:  41% (38/91)\u001b[K\rremote: Counting objects:  42% (39/91)\u001b[K\rremote: Counting objects:  43% (40/91)\u001b[K\rremote: Counting objects:  45% (41/91)\u001b[K\rremote: Counting objects:  46% (42/91)\u001b[K\rremote: Counting objects:  47% (43/91)\u001b[K\rremote: Counting objects:  48% (44/91)\u001b[K\rremote: Counting objects:  49% (45/91)\u001b[K\rremote: Counting objects:  50% (46/91)\u001b[K\rremote: Counting objects:  51% (47/91)\u001b[K\rremote: Counting objects:  52% (48/91)\u001b[K\rremote: Counting objects:  53% (49/91)\u001b[K\rremote: Counting objects:  54% (50/91)\u001b[K\rremote: Counting objects:  56% (51/91)\u001b[K\rremote: Counting objects:  57% (52/91)\u001b[K\rremote: Counting objects:  58% (53/91)\u001b[K\rremote: Counting objects:  59% (54/91)\u001b[K\rremote: Counting objects:  60% (55/91)\u001b[K\rremote: Counting objects:  61% (56/91)\u001b[K\rremote: Counting objects:  62% (57/91)\u001b[K\rremote: Counting objects:  63% (58/91)\u001b[K\rremote: Counting objects:  64% (59/91)\u001b[K\rremote: Counting objects:  65% (60/91)\u001b[K\rremote: Counting objects:  67% (61/91)\u001b[K\rremote: Counting objects:  68% (62/91)\u001b[K\rremote: Counting objects:  69% (63/91)\u001b[K\rremote: Counting objects:  70% (64/91)\u001b[K\rremote: Counting objects:  71% (65/91)\u001b[K\rremote: Counting objects:  72% (66/91)\u001b[K\rremote: Counting objects:  73% (67/91)\u001b[K\rremote: Counting objects:  74% (68/91)\u001b[K\rremote: Counting objects:  75% (69/91)\u001b[K\rremote: Counting objects:  76% (70/91)\u001b[K\rremote: Counting objects:  78% (71/91)\u001b[K\rremote: Counting objects:  79% (72/91)\u001b[K\rremote: Counting objects:  80% (73/91)\u001b[K\rremote: Counting objects:  81% (74/91)\u001b[K\rremote: Counting objects:  82% (75/91)\u001b[K\rremote: Counting objects:  83% (76/91)\u001b[K\rremote: Counting objects:  84% (77/91)\u001b[K\rremote: Counting objects:  85% (78/91)\u001b[K\rremote: Counting objects:  86% (79/91)\u001b[K\rremote: Counting objects:  87% (80/91)\u001b[K\rremote: Counting objects:  89% (81/91)\u001b[K\rremote: Counting objects:  90% (82/91)\u001b[K\rremote: Counting objects:  91% (83/91)\u001b[K\rremote: Counting objects:  92% (84/91)\u001b[K\rremote: Counting objects:  93% (85/91)\u001b[K\rremote: Counting objects:  94% (86/91)\u001b[K\rremote: Counting objects:  95% (87/91)\u001b[K\rremote: Counting objects:  96% (88/91)\u001b[K\rremote: Counting objects:  97% (89/91)\u001b[K\rremote: Counting objects:  98% (90/91)\u001b[K\rremote: Counting objects: 100% (91/91)\u001b[K\rremote: Counting objects: 100% (91/91), done.\u001b[K\n",
            "remote: Compressing objects:   1% (1/85)\u001b[K\rremote: Compressing objects:   2% (2/85)\u001b[K\rremote: Compressing objects:   3% (3/85)\u001b[K\rremote: Compressing objects:   4% (4/85)\u001b[K\rremote: Compressing objects:   5% (5/85)\u001b[K\rremote: Compressing objects:   7% (6/85)\u001b[K\rremote: Compressing objects:   8% (7/85)\u001b[K\rremote: Compressing objects:   9% (8/85)\u001b[K\rremote: Compressing objects:  10% (9/85)\u001b[K\rremote: Compressing objects:  11% (10/85)\u001b[K\rremote: Compressing objects:  12% (11/85)\u001b[K\rremote: Compressing objects:  14% (12/85)\u001b[K\rremote: Compressing objects:  15% (13/85)\u001b[K\rremote: Compressing objects:  16% (14/85)\u001b[K\rremote: Compressing objects:  17% (15/85)\u001b[K\rremote: Compressing objects:  18% (16/85)\u001b[K\rremote: Compressing objects:  20% (17/85)\u001b[K\rremote: Compressing objects:  21% (18/85)\u001b[K\rremote: Compressing objects:  22% (19/85)\u001b[K\rremote: Compressing objects:  23% (20/85)\u001b[K\rremote: Compressing objects:  24% (21/85)\u001b[K\rremote: Compressing objects:  25% (22/85)\u001b[K\rremote: Compressing objects:  27% (23/85)\u001b[K\rremote: Compressing objects:  28% (24/85)\u001b[K\rremote: Compressing objects:  29% (25/85)\u001b[K\rremote: Compressing objects:  30% (26/85)\u001b[K\rremote: Compressing objects:  31% (27/85)\u001b[K\rremote: Compressing objects:  32% (28/85)\u001b[K\rremote: Compressing objects:  34% (29/85)\u001b[K\rremote: Compressing objects:  35% (30/85)\u001b[K\rremote: Compressing objects:  36% (31/85)\u001b[K\rremote: Compressing objects:  37% (32/85)\u001b[K\rremote: Compressing objects:  38% (33/85)\u001b[K\rremote: Compressing objects:  40% (34/85)\u001b[K\rremote: Compressing objects:  41% (35/85)\u001b[K\rremote: Compressing objects:  42% (36/85)\u001b[K\rremote: Compressing objects:  43% (37/85)\u001b[K\rremote: Compressing objects:  44% (38/85)\u001b[K\rremote: Compressing objects:  45% (39/85)\u001b[K\rremote: Compressing objects:  47% (40/85)\u001b[K\rremote: Compressing objects:  48% (41/85)\u001b[K\rremote: Compressing objects:  49% (42/85)\u001b[K\rremote: Compressing objects:  50% (43/85)\u001b[K\rremote: Compressing objects:  51% (44/85)\u001b[K\rremote: Compressing objects:  52% (45/85)\u001b[K\rremote: Compressing objects:  54% (46/85)\u001b[K\rremote: Compressing objects:  55% (47/85)\u001b[K\rremote: Compressing objects:  56% (48/85)\u001b[K\rremote: Compressing objects:  57% (49/85)\u001b[K\rremote: Compressing objects:  58% (50/85)\u001b[K\rremote: Compressing objects:  60% (51/85)\u001b[K\rremote: Compressing objects:  61% (52/85)\u001b[K\rremote: Compressing objects:  62% (53/85)\u001b[K\rremote: Compressing objects:  63% (54/85)\u001b[K\rremote: Compressing objects:  64% (55/85)\u001b[K\rremote: Compressing objects:  65% (56/85)\u001b[K\rremote: Compressing objects:  67% (57/85)\u001b[K\rremote: Compressing objects:  68% (58/85)\u001b[K\rremote: Compressing objects:  69% (59/85)\u001b[K\rremote: Compressing objects:  70% (60/85)\u001b[K\rremote: Compressing objects:  71% (61/85)\u001b[K\rremote: Compressing objects:  72% (62/85)\u001b[K\rremote: Compressing objects:  74% (63/85)\u001b[K\rremote: Compressing objects:  75% (64/85)\u001b[K\rremote: Compressing objects:  76% (65/85)\u001b[K\rremote: Compressing objects:  77% (66/85)\u001b[K\rremote: Compressing objects:  78% (67/85)\u001b[K\rremote: Compressing objects:  80% (68/85)\u001b[K\rremote: Compressing objects:  81% (69/85)\u001b[K\rremote: Compressing objects:  82% (70/85)\u001b[K\rremote: Compressing objects:  83% (71/85)\u001b[K\rremote: Compressing objects:  84% (72/85)\u001b[K\rremote: Compressing objects:  85% (73/85)\u001b[K\rremote: Compressing objects:  87% (74/85)\u001b[K\rremote: Compressing objects:  88% (75/85)\u001b[K\rremote: Compressing objects:  89% (76/85)\u001b[K\rremote: Compressing objects:  90% (77/85)\u001b[K\rremote: Compressing objects:  91% (78/85)\u001b[K\rremote: Compressing objects:  92% (79/85)\u001b[K\rremote: Compressing objects:  94% (80/85)\u001b[K\rremote: Compressing objects:  95% (81/85)\u001b[K\rremote: Compressing objects:  96% (82/85)\u001b[K\rremote: Compressing objects:  97% (83/85)\u001b[K\rremote: Compressing objects:  98% (84/85)\u001b[K\rremote: Compressing objects: 100% (85/85)\u001b[K\rremote: Compressing objects: 100% (85/85), done.\u001b[K\n",
            "Unpacking objects:   1% (1/91)   \rUnpacking objects:   2% (2/91)   \rUnpacking objects:   3% (3/91)   \rUnpacking objects:   4% (4/91)   \rUnpacking objects:   5% (5/91)   \rUnpacking objects:   6% (6/91)   \rUnpacking objects:   7% (7/91)   \rUnpacking objects:   8% (8/91)   \rUnpacking objects:   9% (9/91)   \rUnpacking objects:  10% (10/91)   \rUnpacking objects:  12% (11/91)   \rUnpacking objects:  13% (12/91)   \rUnpacking objects:  14% (13/91)   \rUnpacking objects:  15% (14/91)   \rUnpacking objects:  16% (15/91)   \rUnpacking objects:  17% (16/91)   \rUnpacking objects:  18% (17/91)   \rUnpacking objects:  19% (18/91)   \rUnpacking objects:  20% (19/91)   \rUnpacking objects:  21% (20/91)   \rUnpacking objects:  23% (21/91)   \rUnpacking objects:  24% (22/91)   \rUnpacking objects:  25% (23/91)   \rUnpacking objects:  26% (24/91)   \rUnpacking objects:  27% (25/91)   \rUnpacking objects:  28% (26/91)   \rUnpacking objects:  29% (27/91)   \rUnpacking objects:  30% (28/91)   \rUnpacking objects:  31% (29/91)   \rUnpacking objects:  32% (30/91)   \rUnpacking objects:  34% (31/91)   \rUnpacking objects:  35% (32/91)   \rUnpacking objects:  36% (33/91)   \rUnpacking objects:  37% (34/91)   \rUnpacking objects:  38% (35/91)   \rUnpacking objects:  39% (36/91)   \rUnpacking objects:  40% (37/91)   \rUnpacking objects:  41% (38/91)   \rUnpacking objects:  42% (39/91)   \rUnpacking objects:  43% (40/91)   \rUnpacking objects:  45% (41/91)   \rUnpacking objects:  46% (42/91)   \rremote: Total 91 (delta 43), reused 22 (delta 6), pack-reused 0\u001b[K\n",
            "Unpacking objects:  47% (43/91)   \rUnpacking objects:  48% (44/91)   \rUnpacking objects:  49% (45/91)   \rUnpacking objects:  50% (46/91)   \rUnpacking objects:  51% (47/91)   \rUnpacking objects:  52% (48/91)   \rUnpacking objects:  53% (49/91)   \rUnpacking objects:  54% (50/91)   \rUnpacking objects:  56% (51/91)   \rUnpacking objects:  57% (52/91)   \rUnpacking objects:  58% (53/91)   \rUnpacking objects:  59% (54/91)   \rUnpacking objects:  60% (55/91)   \rUnpacking objects:  61% (56/91)   \rUnpacking objects:  62% (57/91)   \rUnpacking objects:  63% (58/91)   \rUnpacking objects:  64% (59/91)   \rUnpacking objects:  65% (60/91)   \rUnpacking objects:  67% (61/91)   \rUnpacking objects:  68% (62/91)   \rUnpacking objects:  69% (63/91)   \rUnpacking objects:  70% (64/91)   \rUnpacking objects:  71% (65/91)   \rUnpacking objects:  72% (66/91)   \rUnpacking objects:  73% (67/91)   \rUnpacking objects:  74% (68/91)   \rUnpacking objects:  75% (69/91)   \rUnpacking objects:  76% (70/91)   \rUnpacking objects:  78% (71/91)   \rUnpacking objects:  79% (72/91)   \rUnpacking objects:  80% (73/91)   \rUnpacking objects:  81% (74/91)   \rUnpacking objects:  82% (75/91)   \rUnpacking objects:  83% (76/91)   \rUnpacking objects:  84% (77/91)   \rUnpacking objects:  85% (78/91)   \rUnpacking objects:  86% (79/91)   \rUnpacking objects:  87% (80/91)   \rUnpacking objects:  89% (81/91)   \rUnpacking objects:  90% (82/91)   \rUnpacking objects:  91% (83/91)   \rUnpacking objects:  92% (84/91)   \rUnpacking objects:  93% (85/91)   \rUnpacking objects:  94% (86/91)   \rUnpacking objects:  95% (87/91)   \rUnpacking objects:  96% (88/91)   \rUnpacking objects:  97% (89/91)   \rUnpacking objects:  98% (90/91)   \rUnpacking objects: 100% (91/91)   \rUnpacking objects: 100% (91/91), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1XdrdGGM9ky",
        "outputId": "4c785985-f752-4e7a-a824-8656e2190560"
      },
      "source": [
        "!bash ./Mecab-ko-for-Google-Colab/install_mecab-ko_on_colab190912.sh"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Installing konlpy.....\n",
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 18.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 11.2MB/s \n",
            "\u001b[?25hCollecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Collecting JPype1>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/a5/9781e2ef4ca92d09912c4794642c1653aea7607f473e156cf4d423a881a1/JPype1-1.2.1-cp37-cp37m-manylinux2010_x86_64.whl (457kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 30.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Installing collected packages: beautifulsoup4, colorama, JPype1, konlpy\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-1.2.1 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2\n",
            "Done\n",
            "Installing mecab-0.996-ko-0.9.2.tar.gz.....\n",
            "Downloading mecab-0.996-ko-0.9.2.tar.gz.......\n",
            "from https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz\n",
            "--2021-05-15 05:44:43--  https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz\n",
            "Resolving bitbucket.org (bitbucket.org)... 104.192.141.1, 2406:da00:ff00::22c2:513, 2406:da00:ff00::22cd:e0db, ...\n",
            "Connecting to bitbucket.org (bitbucket.org)|104.192.141.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://bbuseruploads.s3.amazonaws.com/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz?Signature=Uk44Vx4uApIAENpmDG12Df%2B0DEA%3D&Expires=1621058695&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=null&response-content-disposition=attachment%3B%20filename%3D%22mecab-0.996-ko-0.9.2.tar.gz%22&response-content-encoding=None [following]\n",
            "--2021-05-15 05:44:43--  https://bbuseruploads.s3.amazonaws.com/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz?Signature=Uk44Vx4uApIAENpmDG12Df%2B0DEA%3D&Expires=1621058695&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=null&response-content-disposition=attachment%3B%20filename%3D%22mecab-0.996-ko-0.9.2.tar.gz%22&response-content-encoding=None\n",
            "Resolving bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)... 52.217.138.113\n",
            "Connecting to bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)|52.217.138.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1414979 (1.3M) [application/x-tar]\n",
            "Saving to: ‘mecab-0.996-ko-0.9.2.tar.gz’\n",
            "\n",
            "mecab-0.996-ko-0.9. 100%[===================>]   1.35M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2021-05-15 05:44:43 (29.5 MB/s) - ‘mecab-0.996-ko-0.9.2.tar.gz’ saved [1414979/1414979]\n",
            "\n",
            "Done\n",
            "Unpacking mecab-0.996-ko-0.9.2.tar.gz.......\n",
            "Done\n",
            "Change Directory to mecab-0.996-ko-0.9.2.......\n",
            "installing mecab-0.996-ko-0.9.2.tar.gz........\n",
            "configure\n",
            "make\n",
            "make check\n",
            "make install\n",
            "ldconfig\n",
            "Done\n",
            "Change Directory to /content\n",
            "Downloading mecab-ko-dic-2.1.1-20180720.tar.gz.......\n",
            "from https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz\n",
            "--2021-05-15 05:46:12--  https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz\n",
            "Resolving bitbucket.org (bitbucket.org)... 18.205.93.0, 18.205.93.1, 18.205.93.2, ...\n",
            "Connecting to bitbucket.org (bitbucket.org)|18.205.93.0|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://bbuseruploads.s3.amazonaws.com/a4fcd83e-34f1-454e-a6ac-c242c7d434d3/downloads/b5a0c703-7b64-45ed-a2d7-180e962710b6/mecab-ko-dic-2.1.1-20180720.tar.gz?Signature=3IIoVirT64wRtZasSICG2JsJ7Hg%3D&Expires=1621059025&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=tzyxc1TtnZU_zEuaaQDGN4F76hPDpyFq&response-content-disposition=attachment%3B%20filename%3D%22mecab-ko-dic-2.1.1-20180720.tar.gz%22&response-content-encoding=None [following]\n",
            "--2021-05-15 05:46:12--  https://bbuseruploads.s3.amazonaws.com/a4fcd83e-34f1-454e-a6ac-c242c7d434d3/downloads/b5a0c703-7b64-45ed-a2d7-180e962710b6/mecab-ko-dic-2.1.1-20180720.tar.gz?Signature=3IIoVirT64wRtZasSICG2JsJ7Hg%3D&Expires=1621059025&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=tzyxc1TtnZU_zEuaaQDGN4F76hPDpyFq&response-content-disposition=attachment%3B%20filename%3D%22mecab-ko-dic-2.1.1-20180720.tar.gz%22&response-content-encoding=None\n",
            "Resolving bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)... 52.217.18.100\n",
            "Connecting to bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)|52.217.18.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 49775061 (47M) [application/x-tar]\n",
            "Saving to: ‘mecab-ko-dic-2.1.1-20180720.tar.gz’\n",
            "\n",
            "mecab-ko-dic-2.1.1- 100%[===================>]  47.47M   100MB/s    in 0.5s    \n",
            "\n",
            "2021-05-15 05:46:12 (100 MB/s) - ‘mecab-ko-dic-2.1.1-20180720.tar.gz’ saved [49775061/49775061]\n",
            "\n",
            "Done\n",
            "Unpacking  mecab-ko-dic-2.1.1-20180720.tar.gz.......\n",
            "Done\n",
            "Change Directory to mecab-ko-dic-2.1.1-20180720\n",
            "Done\n",
            "installing........\n",
            "configure\n",
            "make\n",
            "make install\n",
            "apt-get update\n",
            "apt-get upgrade\n",
            "apt install curl\n",
            "apt install git\n",
            "bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\n",
            "Done\n",
            "Successfully Installed\n",
            "Now you can use Mecab\n",
            "from konlpy.tag import Mecab\n",
            "mecab = Mecab()\n",
            "사용자 사전 추가 방법 : https://bit.ly/3k0ZH53\n",
            "NameError: name 'Tagger' is not defined 오류 발생 시 런타임을 재실행 해주세요\n",
            "블로그에 해결 방법을 남겨주신 tana님 감사합니다.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecJdu9LKxh2z"
      },
      "source": [
        "# 한국어 Tokenizer 학습"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaP5MmCejnq_"
      },
      "source": [
        "## dataset 만들기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqRjnbbJlpBl"
      },
      "source": [
        "### 네이버 뉴스.... 평화 평가 댓글... 에서 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUJMmmyZmpqX",
        "outputId": "0a3126f9-b89f-470c-9639-d3362a5b1426"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ea31IIPRm2cy"
      },
      "source": [
        "\n",
        "# 간단한 전처리\n",
        "def clean_text(txt):\n",
        "    txt = txt.replace('\\n',' ')\n",
        "    txt = txt.replace('\\r',' ')    \n",
        "    txt = txt.replace('=','')\n",
        "    txt = txt.replace('\\\"','')   \n",
        "    txt = txt.replace('\\'','')\n",
        "    #txt = txt.replace(',','')\n",
        "    txt = txt.replace('..','')\n",
        "    txt = txt.replace('...','')\n",
        "    #txt = txt.replace('.','. ')\n",
        "    txt = txt.replace('.','. ')\n",
        "    txt = txt.replace('  ',' ')\n",
        "    txt = txt.replace('  ',' ')    \n",
        "    txt = txt.replace('  ',' ')   \n",
        "    txt = txt.replace('  ',' ')           \n",
        "    txt = txt.replace('  ',' ')\n",
        "    txt = txt.replace('  ',' ')    \n",
        "    txt = txt.replace('  ',' ')   \n",
        "    txt = txt.replace('  ',' ')             \n",
        "    return txt.strip()\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RijcRl1ukG3Z",
        "outputId": "dae0f266-f09f-40a0-b13a-081719b9d7e8"
      },
      "source": [
        "%%time\n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "import sys\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/summary/korean_news_corpus.csv')\n",
        "# 검사...\n",
        "pattens = [\"[34569][0-9]{3}[\\;.\\;-\\; ][0-9]{4}[\\;.\\;-\\; ][0-9]{4}[\\;.\\;-\\; ][0-9]{4}\",\n",
        "           \"[0-9]{2,3}[\\:\\s\\;.\\;,\\;-;)][0-9]{3,4}[\\:\\s\\;.\\;,\\;-][0-9]{4}\",\n",
        "           \"[0-9]{1}[0-9]{1}[\\W]?[0-1]{1}[0-9]{1}[\\W]?[0-3]{1}[\\W]?[0-9]{1}[\\W]?[1-4]{1}[\\W]?[0-9]{1}[\\W]?[0-9]{1}[\\W]?[0-9]{1}[\\W]?[0-9]{1}[\\W]?[0-9]{1}[\\W]?[0-9]{1}\",\n",
        "           \"[0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{6}|[0-9]{3}[\\:\\s\\;.\\;,\\;-]([0-9]{5,6}[\\:\\s\\;.\\;,\\;-][0-9]{3}|[0-9]{6}[\\:\\s\\;.\\;,\\;-][0-9]{5}|[0-9]{2,3}[\\:\\s\\;.\\;,\\;-][0-9]{6}|[0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{7}|[0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{4,6}[\\:\\s\\;.\\;,\\;-][0-9]|[0-9]{5}[\\:\\s\\;.\\;,\\;-][0-9]{3}[\\:\\s\\;.\\;,\\;-][0-9]{2}|[0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{5}[\\:\\s\\;.\\;,\\;-][0-9]{3}|[0-9]{4}[\\:\\s\\;.\\;,\\;-][0-9]{4}[\\:\\s\\;.\\;,\\;-][0-9]{3}|[0-9]{6}[\\:\\s\\;.\\;,\\;-][0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{3}|[0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{7})|[0-9]{4}[\\:\\s\\;.\\;,\\;-]([0-9]{3}[\\:\\s\\;.\\;,\\;-][0-9]{6}|[0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{6}[\\:\\s\\;.\\;,\\;-][0-9])|[0-9]{5}[\\:\\s\\;.\\;,\\;-][0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{6}|[0-9]{6}[\\:\\s\\;.\\;,\\;-][0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{5,6}\"\n",
        "           ]\n",
        "\n",
        "filters = []\n",
        "for p in pattens:\n",
        "    filters.append(re.compile(p))\n",
        "\n",
        "sentences = []\n",
        "df = df.dropna(axis=0)\n",
        "cnt = df['contents'].count()\n",
        "print('Total row count:',cnt)\n",
        "i=0\n",
        "for raw_text in df['contents']:\n",
        "    i=i+1\n",
        "    try:\n",
        "        if i%100 == 0:\n",
        "            percent = (\"{0:.2f}\").format(100 * (i / float(cnt)))\n",
        "            print(f'\\r {percent}% {i}/{str(cnt)}', end=\"\", flush=True)\n",
        "\n",
        "        docs = nltk.sent_tokenize(clean_text(raw_text))\n",
        "        for txt in docs:\n",
        "            if txt.find('▶') > -1 or txt.find('@') > -1 or txt.find('ⓒ') > -1: \n",
        "                pass\n",
        "            else:\n",
        "                txt = txt.strip()\n",
        "                if any(chr.isdigit() for chr in txt) :\n",
        "                    pass\n",
        "                else:\n",
        "                    sentences.append(txt)\n",
        "    except KeyboardInterrupt as ki:\n",
        "        raise ki        \n",
        "    except:\n",
        "        pass #print(\"Unexpected error:\", sys.exc_info()[0])\n",
        "\n",
        "print('')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total row count: 140536\n",
            " 99.97% 140500/140536\n",
            "CPU times: user 3min 20s, sys: 2.5 s, total: 3min 22s\n",
            "Wall time: 3min 24s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-U-syo92kQis",
        "outputId": "30e3feac-3c7d-4b90-95e8-9fc499b38c4e"
      },
      "source": [
        "len(sentences)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2967202"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FcekjFG6nvX9",
        "outputId": "83f36837-f043-4381-dc46-f0f99d643337"
      },
      "source": [
        "sentences[:10]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['연합뉴스 문재인 대통령이 변창흠 국토교통부 장관의 사의 표명을 사실상 수용했다.',\n",
              " '문 대통령은 변 장관의 사의 표명에 책임지는 모습을 보일 수밖에 없다고 밝혔다.',\n",
              " '이는 문 대통령이 사실상 변 장관의 사의를 수용한 것으로 해석된다.',\n",
              " '이에 앞서 변창흠 국토교통부 장관은 LH 땅 투기 의혹 사건과 관련한 책임론에 대해 “자리에 연연하지 않는다”면서 “(청와대의) 결정에 따르겠다”고 말했다.',\n",
              " '변 장관은 이날 국회 국토교통위원회 전체회의에 참석해 “LH 사태로 국민들이 걱정하는 부분을 해소할 수 있게 최대한 대안을 만들고 LH가 근본적으로 다시 태어날 수 있도록 책임지고 추진하겠다”고 언급하고 “그 역할이 충분하다고 평가되지 못했을 때 언제든지 자리에 연연하지 않고 결정에 따르겠다”고 말했다.',\n",
              " 'kr ) 무단 전재 및 재배포 금지',\n",
              " 'kr ) 무단 전재 및 재배포 금지',\n",
              " 'kr ) 무단 전재 및 재배포 금지',\n",
              " '인재 양성·소외 계층 지원 등 계획 “부친 질병·가난 악순환 끊기 원해 국내 최고 넘어 세계적 병원 되길 정몽구 현대차그룹 명예회장.',\n",
              " '그는 “질병과 가난이 악순환되는 고리를 끊기 위해 아산재단과 서울아산병원을 설립했던 아버님의 뜻을 이어 우리 사회의 어려운 이웃을 돕는 데 보탬이 되기를 바란다”고 기부 취지를 밝혔다.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGheAmmIkVQ4"
      },
      "source": [
        "\n",
        "import re\n",
        "import sys\n",
        "import io\n",
        "\n",
        "#텍스트 정제(전처리)\n",
        "def cleanText(readData):\n",
        "    #텍스트에 포함되어 있는 특수 문자 제거\n",
        "    text = re.sub('[-=+#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》◆◇●🎧○▲\\t―△━▷]', '', readData)\n",
        "    return text"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb6oBQFrkV_3",
        "outputId": "688560b1-4287-484b-8d3b-78d6df5423af"
      },
      "source": [
        "%%time\n",
        "c_sentences = []\n",
        "for sentence in sentences:\n",
        "    s = cleanText(sentence)\n",
        "    c = len(s.split())\n",
        "    if c >= 3 and c < 10 and s.find('재배포') < 0 and s.find('기자') < 0  and s.find('유투브') < 0 and s.find('www') < 0 and s.find('com') < 0 and s.find('접속하기') < 0 and s.find('http') < 0 and s.find('뉴스') < 0 and s.find('일보') < 0 :\n",
        "        if s.endswith(('다','요')):\n",
        "            c_sentences.append(s.strip())\n",
        "\n",
        "\n",
        "# 한국어 영화 리뷰를 Load.\n",
        "ds = pd.read_csv(\"https://raw.githubusercontent.com/dolmani38/similarly/master/ratings_test.txt\",sep='\\t')\n",
        "for row in ds.iterrows():\n",
        "    doc_id = row[1][0]\n",
        "    doc_cont = str(row[1][1])\n",
        "    c_sentences.append(doc_cont)\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "코퍼스 평균/총 단어 갯수 : 6.6 / 6029075\n",
            "CPU times: user 17.2 s, sys: 155 ms, total: 17.3 s\n",
            "Wall time: 17.6 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDANhUJ9qDel"
      },
      "source": [
        "import random\n",
        "\n",
        "random.shuffle(c_sentences)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugNi7PH5qtw2"
      },
      "source": [
        "### 한국어... 교착어 처리..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMRu1dDHqre-",
        "outputId": "e615fd1f-0d60-46b2-b815-a98a7ab0b5b2"
      },
      "source": [
        "%%time\n",
        "# mecab for window는 아래 코드 사용\n",
        "from konlpy.tag import Mecab  # install mecab for window: https://hong-yp-ml-records.tistory.com/91\n",
        "mecab_tokenizer = Mecab().morphs\n",
        "print('mecab check :', mecab_tokenizer('어릴때보고 지금다시봐도 재밌어요ㅋㅋ'))\n",
        "\n",
        "ko_sentences = []\n",
        "# 50만개의 문장만 사용...\n",
        "for sentence in c_sentences[:500000]:\n",
        "    # 문장단위 mecab 적용\n",
        "    morph_sentence= mecab_tokenizer(sentence)\n",
        "    # 문장단위 저장\n",
        "    ko_sentences.append( (' '.join(morph_sentence)).strip())\n",
        "        \n",
        "num_word_list = [len(sentence.split()) for sentence in ko_sentences]\n",
        "print('\\n힌국어 코퍼스 평균/총 단어 갯수 : %.1f / %d' % (sum(num_word_list)/len(num_word_list), sum(num_word_list)))       "
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mecab check : ['어릴', '때', '보', '고', '지금', '다시', '봐도', '재밌', '어요', 'ㅋㅋ']\n",
            "\n",
            "힌국어 코퍼스 평균/총 단어 갯수 : 13.2 / 6584031\n",
            "CPU times: user 40.2 s, sys: 192 ms, total: 40.4 s\n",
            "Wall time: 40.5 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sZ4yn-toGXw"
      },
      "source": [
        "### CNN/Daily main Dataset에서 영어... 문장 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 647,
          "referenced_widgets": [
            "e0a234238a20442188bffeb50b8a82e0",
            "508da04576294734b60f3fa90d40b2f5",
            "31dda3106cc647fda699a60bf34a320c",
            "7269bb94559540d8a2ad15890118c9a0",
            "4bbddc49854648ee809cab2744f19f03",
            "a6b91a068a2445da88b086a99123cca1",
            "a0c5df4e61074ded90aadad9d9123604",
            "ffaa2c46811545579645138631dfcfb7",
            "b07bff8f8c0242cb9d14c4e9200fe065",
            "c138a58915804d3a82cdab0711e6ee13",
            "b2f387970f2b453db3beaedc6aaea0a0",
            "10d6cdeac7824a1da0e7ee23a9857162",
            "8cde191a1d694cac973a9ff53c680c6f",
            "f715a113aec4444f91d26969ea994d30",
            "8fad692c0da84f67b2981b9dc5085798",
            "d94643c7f32d4c679260bb0abf305de3",
            "a94f18110c324dc780e714e50491e9fa",
            "be9e86e7e0b54aa1a8cceb4a7c44fbd9",
            "d6c60b10cea943e2b0fdba4f546ec849",
            "d8100564205c4989b0cc9512ffdef942",
            "83c6a479d4594efdaa7c8fdf758f86ef",
            "edb7645bd9cf4ebcb0472ab8413f15ec",
            "6dd9062c334a4755ba7b0a1bd86468ca",
            "0ecf99f564724d79b28843833d2e9c6b",
            "141567d835724105af5415594e9a7f9a",
            "2293ab6f8444436d826a597626e581f1",
            "e7df796c569f4432b158b7d360cea766",
            "dc0e96d7d05e4ed0ac616431ffc17104",
            "845e048e05db4a1ba2fbe08d84aefa40",
            "15172d9909fd47dc98f1666eb3ab3102",
            "058be0a299de4e408294b8551c55bc63",
            "6ecd6166ed4b41b9b09da5d04d26ada4"
          ]
        },
        "id": "gpv2vn9Bom28",
        "outputId": "c0512baf-6051-4174-90f4-ec6cbecbca8a"
      },
      "source": [
        "\n",
        "import tensorflow_datasets as tfds\n",
        "train_data, test_data = tfds.load(name=\"cnn_dailymail\",split=(tfds.Split.TRAIN,tfds.Split.TEST),with_info=True)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mDownloading and preparing dataset cnn_dailymail/plain_text/3.0.0 (download: 558.32 MiB, generated: 1.27 GiB, total: 1.82 GiB) to /root/tensorflow_datasets/cnn_dailymail/plain_text/3.0.0...\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e0a234238a20442188bffeb50b8a82e0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Completed...', max=1.0, style=Progre…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b07bff8f8c0242cb9d14c4e9200fe065",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Size...', max=1.0, style=ProgressSty…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a94f18110c324dc780e714e50491e9fa",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Extraction completed...', max=1.0, styl…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "141567d835724105af5415594e9a7f9a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-7ea54526221a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_datasets\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cnn_dailymail\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSplit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtfds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSplit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTEST\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwith_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/load.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, split, data_dir, batch_size, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)\u001b[0m\n\u001b[1;32m    342\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[0mdownload_and_prepare_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_and_prepare_kwargs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m     \u001b[0mdbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_and_prepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdownload_and_prepare_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mas_dataset_kwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/dataset_builder.py\u001b[0m in \u001b[0;36mdownload_and_prepare\u001b[0;34m(self, download_dir, download_config)\u001b[0m\n\u001b[1;32m    385\u001b[0m           self._download_and_prepare(\n\u001b[1;32m    386\u001b[0m               \u001b[0mdl_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdl_manager\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m               download_config=download_config)\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m           \u001b[0;31m# NOTE: If modifying the lines below to put additional information in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/dataset_builder.py\u001b[0m in \u001b[0;36m_download_and_prepare\u001b[0;34m(self, dl_manager, download_config)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     super(GeneratorBasedBuilder, self)._download_and_prepare(\n\u001b[1;32m   1023\u001b[0m         \u001b[0mdl_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdl_manager\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m         \u001b[0mmax_examples_per_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_examples_per_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m     )\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/dataset_builder.py\u001b[0m in \u001b[0;36m_download_and_prepare\u001b[0;34m(self, dl_manager, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m    972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m       \u001b[0;31m# Prepare split will record examples associated to the split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 974\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mprepare_split_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    975\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m     \u001b[0;31m# Update the info object with the splits.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/dataset_builder.py\u001b[0m in \u001b[0;36m_prepare_split\u001b[0;34m(self, split_generator, max_examples_per_split)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                      hash_salt=split_generator.name)\n\u001b[1;32m   1038\u001b[0m     for key, record in utils.tqdm(generator, unit=\" examples\",\n\u001b[0;32m-> 1039\u001b[0;31m                                   total=split_info.num_examples, leave=False):\n\u001b[0m\u001b[1;32m   1040\u001b[0m       \u001b[0mexample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[1;32m   1103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1104\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1105\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m             \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/summarization/cnn_dailymail.py\u001b[0m in \u001b[0;36m_generate_examples\u001b[0;34m(self, files)\u001b[0m\n\u001b[1;32m    306\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_generate_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m       \u001b[0marticle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhighlights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_art_abs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0marticle\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhighlights\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/summarization/cnn_dailymail.py\u001b[0m in \u001b[0;36m_get_art_abs\u001b[0;34m(story_file, tfds_version)\u001b[0m\n\u001b[1;32m    187\u001b[0m   \u001b[0;31m#     make_datafiles.py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m   \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_text_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstory_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m   \u001b[0;31m# The github code lowercase the text and we removed it in 3.0.0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/summarization/cnn_dailymail.py\u001b[0m in \u001b[0;36m_read_text_file\u001b[0;34m(text_file)\u001b[0m\n\u001b[1;32m    177\u001b[0m   \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m       \u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m     \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mreadline\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;34mr\"\"\"Reads the next line, keeping \\n. At EOF, returns ''.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preread_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_buf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xc7bauWlov2n"
      },
      "source": [
        "%%time\n",
        "en_sentences = []\n",
        "iterator = iter(test_data[0])\n",
        "for data in iterator:\n",
        "    article = nltk.sent_tokenize(data['article'].numpy().decode('UTF-8'))\n",
        "    for sent in article:    \n",
        "        en_sentences.append(sent)\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VhwqYk_o32m"
      },
      "source": [
        "num_word_list = [len(sentence.split()) for sentence in en_sentences]\n",
        "print('\\n영어 코퍼스 평균/총 단어 갯수 : %.1f / %d' % (sum(num_word_list)/len(num_word_list), sum(num_word_list)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3V3QEInEkhEl",
        "outputId": "899d9a85-fd37-466d-b87c-2999f9f7d720"
      },
      "source": [
        "%%time\n",
        "# subword 학습을 위해 문장만 따로 저장\n",
        "with open('/content/drive/MyDrive/Tokenizer_train/data/train_tokenizer.txt', 'w', encoding='utf-8') as f:\n",
        "    for line in ko_sentences:\n",
        "        f.write(line+'\\n')\n",
        "\n",
        "f.close()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 288 ms, sys: 40.3 ms, total: 328 ms\n",
            "Wall time: 1.35 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGBDbPNwy1X-",
        "outputId": "5f6dc492-69e2-4069-ad8e-df44617726f8"
      },
      "source": [
        "%%time\n",
        "import sentencepiece as spm\n",
        "import os\n",
        "# spm_train --input=data/train_tokenizer.txt  --model_prefix=sentencepiece/sp --vocab_size=32000 character_coverage=1.0 --model_type=\"unigram\"\n",
        "\n",
        "input_file = '/content/drive/MyDrive/Tokenizer_train/data/train_tokenizer.txt'\n",
        "vocab_size = 30015\n",
        "\n",
        "sp_model_root='/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model'\n",
        "if not os.path.isdir(sp_model_root):\n",
        "    os.mkdir(sp_model_root)\n",
        "sp_model_name = 'spiece'\n",
        "sp_model_path = os.path.join(sp_model_root, sp_model_name)\n",
        "model_type = 'unigram'  # 학습할 모델 선택, unigram이 더 성능이 좋음'bpe'\n",
        "character_coverage  = 1.0  # 전체를 cover 하기 위해, default=0.9995\n",
        "user_defined_symbols = '[PAD],[UNK],[CLS],[SEP],[MASK],[BOS],[EOS],[UNK0],[UNK1],[UNK2],[UNK3],[UNK4],[UNK5],[UNK6],[UNK7],[UNK8],[UNK9],[unused0],[unused1],[unused2],[unused3],[unused4],[unused5],[unused6],[unused7],[unused8],[unused9],[unused10],[unused11],[unused12],[unused13],[unused14],[unused15],[unused16],[unused17],[unused18],[unused19],[unused20],[unused21],[unused22],[unused23],[unused24],[unused25],[unused26],[unused27],[unused28],[unused29],[unused30],[unused31],[unused32],[unused33],[unused34],[unused35],[unused36],[unused37],[unused38],[unused39],[unused40],[unused41],[unused42],[unused43],[unused44],[unused45],[unused46],[unused47],[unused48],[unused49],[unused50],[unused51],[unused52],[unused53],[unused54],[unused55],[unused56],[unused57],[unused58],[unused59],[unused60],[unused61],[unused62],[unused63],[unused64],[unused65],[unused66],[unused67],[unused68],[unused69],[unused70],[unused71],[unused72],[unused73],[unused74],[unused75],[unused76],[unused77],[unused78],[unused79],[unused80],[unused81],[unused82],[unused83],[unused84],[unused85],[unused86],[unused87],[unused88],[unused89],[unused90],[unused91],[unused92],[unused93],[unused94],[unused95],[unused96],[unused97],[unused98],[unused99]'\n",
        "\n",
        "input_argument = '--input=%s --model_prefix=%s --vocab_size=%s --user_defined_symbols=%s --model_type=%s --character_coverage=%s'\n",
        "cmd = input_argument%(input_file, sp_model_path, vocab_size,user_defined_symbols, model_type, character_coverage)\n",
        "\n",
        "spm.SentencePieceTrainer.Train(cmd)\n",
        "print('train done')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train done\n",
            "CPU times: user 19 s, sys: 479 ms, total: 19.5 s\n",
            "Wall time: 19.4 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkA1gOfTRr6q"
      },
      "source": [
        "# pass"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m23ogFfIztOR",
        "outputId": "1609b5bf-1c5d-4e76-b9ef-29da86783ed9"
      },
      "source": [
        "## check\n",
        "import sentencepiece as spm\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.Load('{}.model'.format(sp_model_path))\n",
        "\n",
        "tokens = sp.encode_as_pieces('나는 오늘 아침밥을 먹었다.')\n",
        "ids = sp.encode_as_ids('나는 오늘 아침밥을 먹었다.')\n",
        "\n",
        "print(ids)\n",
        "print(tokens)\n",
        "\n",
        "tokens = sp.decode_pieces(tokens)\n",
        "ids = sp.decode_ids(ids)\n",
        "\n",
        "print(ids)\n",
        "print(tokens)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[422, 668, 5753, 5305, 131, 121, 9622, 120]\n",
            "['▁나는', '▁오늘', '▁아침', '밥', '을', '▁', '먹었다', '.']\n",
            "나는 오늘 아침밥을 먹었다.\n",
            "나는 오늘 아침밥을 먹었다.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSNUOU1i0J2k",
        "outputId": "33abd03b-03ad-4e4d-fbb5-be697ebe32a1"
      },
      "source": [
        "!pip install transformers==4.4.1\n",
        "!pip install tokenizers==0.10.1"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers==4.4.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2c/d8/5144b0712f7f82229a8da5983a8fbb8d30cec5fbd5f8d12ffe1854dcea67/transformers-4.4.1-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 4.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.1) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.1) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.1) (2.23.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 21.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.1) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.1) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.1) (4.0.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.1) (20.9)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 38.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.4.1) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.4.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.4.1) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.4.1) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.4.1) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.4.1) (8.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.4.1) (1.15.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.4.1) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.4.1) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.4.1) (2.4.7)\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.4.1\n",
            "Collecting tokenizers==0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 4.1MB/s \n",
            "\u001b[?25hInstalling collected packages: tokenizers\n",
            "  Found existing installation: tokenizers 0.10.2\n",
            "    Uninstalling tokenizers-0.10.2:\n",
            "      Successfully uninstalled tokenizers-0.10.2\n",
            "Successfully installed tokenizers-0.10.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPYF4fkL1X2G"
      },
      "source": [
        "## mecab 설치 참조\n",
        "\n",
        "https://somjang.tistory.com/entry/Google-Colab%EC%97%90%EC%84%9C-Mecab-koMecab-ko-dic-%EC%89%BD%EA%B2%8C-%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEMkMuYe1mMy",
        "outputId": "24f5206b-52ea-4a58-8381-8f8454c68690"
      },
      "source": [
        "!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Mecab-ko-for-Google-Colab'...\n",
            "remote: Enumerating objects: 91, done.\u001b[K\n",
            "remote: Counting objects: 100% (91/91), done.\u001b[K\n",
            "remote: Compressing objects: 100% (85/85), done.\u001b[K\n",
            "remote: Total 91 (delta 43), reused 22 (delta 6), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (91/91), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9we-LODi1stV",
        "outputId": "c5568883-f3b6-4f20-88fe-b7784164ca18"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive  Mecab-ko-for-Google-Colab  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQS-tyl_115Q",
        "outputId": "95bf7cfb-b07f-41bd-da51-2d2263fa96b9"
      },
      "source": [
        "cd Mecab-ko-for-Google-Colab"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Mecab-ko-for-Google-Colab\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7zG-vWR18UD",
        "outputId": "18ef6572-5d85-4111-ab0d-56c7e609e5d6"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "images\t\t\t\t\t   LICENSE\n",
            "install_mecab-ko_on_colab190912.sh\t   README.md\n",
            "install_mecab-ko_on_colab_light_210108.sh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Myd_HnLC2TkH",
        "outputId": "e194847c-dca2-4552-8df9-a9470a0cb4a8"
      },
      "source": [
        "!bash ./Mecab-ko-for-Google-Colab/install_mecab-ko_on_colab190912.sh"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Installing konlpy.....\n",
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 4.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 10.3MB/s \n",
            "\u001b[?25hCollecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Collecting JPype1>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/a5/9781e2ef4ca92d09912c4794642c1653aea7607f473e156cf4d423a881a1/JPype1-1.2.1-cp37-cp37m-manylinux2010_x86_64.whl (457kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 35.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Installing collected packages: beautifulsoup4, colorama, JPype1, konlpy\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-1.2.1 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2\n",
            "Done\n",
            "Installing mecab-0.996-ko-0.9.2.tar.gz.....\n",
            "Downloading mecab-0.996-ko-0.9.2.tar.gz.......\n",
            "from https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz\n",
            "--2021-05-15 02:38:28--  https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz\n",
            "Resolving bitbucket.org (bitbucket.org)... 104.192.141.1, 2406:da00:ff00::6b17:d1f5, 2406:da00:ff00::22c2:513, ...\n",
            "Connecting to bitbucket.org (bitbucket.org)|104.192.141.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://bbuseruploads.s3.amazonaws.com/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz?Signature=3MsI8SU4yWB1Uvr8Nz7Jp%2F6txLI%3D&Expires=1621047452&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=null&response-content-disposition=attachment%3B%20filename%3D%22mecab-0.996-ko-0.9.2.tar.gz%22&response-content-encoding=None [following]\n",
            "--2021-05-15 02:38:29--  https://bbuseruploads.s3.amazonaws.com/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz?Signature=3MsI8SU4yWB1Uvr8Nz7Jp%2F6txLI%3D&Expires=1621047452&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=null&response-content-disposition=attachment%3B%20filename%3D%22mecab-0.996-ko-0.9.2.tar.gz%22&response-content-encoding=None\n",
            "Resolving bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)... 52.216.20.203\n",
            "Connecting to bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)|52.216.20.203|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1414979 (1.3M) [application/x-tar]\n",
            "Saving to: ‘mecab-0.996-ko-0.9.2.tar.gz’\n",
            "\n",
            "mecab-0.996-ko-0.9. 100%[===================>]   1.35M  1.08MB/s    in 1.3s    \n",
            "\n",
            "2021-05-15 02:38:31 (1.08 MB/s) - ‘mecab-0.996-ko-0.9.2.tar.gz’ saved [1414979/1414979]\n",
            "\n",
            "Done\n",
            "Unpacking mecab-0.996-ko-0.9.2.tar.gz.......\n",
            "Done\n",
            "Change Directory to mecab-0.996-ko-0.9.2.......\n",
            "installing mecab-0.996-ko-0.9.2.tar.gz........\n",
            "configure\n",
            "make\n",
            "make check\n",
            "make install\n",
            "ldconfig\n",
            "Done\n",
            "Change Directory to /content\n",
            "Downloading mecab-ko-dic-2.1.1-20180720.tar.gz.......\n",
            "from https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz\n",
            "--2021-05-15 02:40:01--  https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz\n",
            "Resolving bitbucket.org (bitbucket.org)... 104.192.141.1, 2406:da00:ff00::6b17:d1f5, 2406:da00:ff00::22cd:e0db, ...\n",
            "Connecting to bitbucket.org (bitbucket.org)|104.192.141.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://bbuseruploads.s3.amazonaws.com/a4fcd83e-34f1-454e-a6ac-c242c7d434d3/downloads/b5a0c703-7b64-45ed-a2d7-180e962710b6/mecab-ko-dic-2.1.1-20180720.tar.gz?Signature=zT%2B5khLv3A4fh9LrGoc2lUtGAus%3D&Expires=1621047547&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=tzyxc1TtnZU_zEuaaQDGN4F76hPDpyFq&response-content-disposition=attachment%3B%20filename%3D%22mecab-ko-dic-2.1.1-20180720.tar.gz%22&response-content-encoding=None [following]\n",
            "--2021-05-15 02:40:02--  https://bbuseruploads.s3.amazonaws.com/a4fcd83e-34f1-454e-a6ac-c242c7d434d3/downloads/b5a0c703-7b64-45ed-a2d7-180e962710b6/mecab-ko-dic-2.1.1-20180720.tar.gz?Signature=zT%2B5khLv3A4fh9LrGoc2lUtGAus%3D&Expires=1621047547&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=tzyxc1TtnZU_zEuaaQDGN4F76hPDpyFq&response-content-disposition=attachment%3B%20filename%3D%22mecab-ko-dic-2.1.1-20180720.tar.gz%22&response-content-encoding=None\n",
            "Resolving bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)... 52.217.168.73\n",
            "Connecting to bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)|52.217.168.73|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 49775061 (47M) [application/x-tar]\n",
            "Saving to: ‘mecab-ko-dic-2.1.1-20180720.tar.gz’\n",
            "\n",
            "mecab-ko-dic-2.1.1- 100%[===================>]  47.47M  12.4MB/s    in 5.0s    \n",
            "\n",
            "2021-05-15 02:40:08 (9.50 MB/s) - ‘mecab-ko-dic-2.1.1-20180720.tar.gz’ saved [49775061/49775061]\n",
            "\n",
            "Done\n",
            "Unpacking  mecab-ko-dic-2.1.1-20180720.tar.gz.......\n",
            "Done\n",
            "Change Directory to mecab-ko-dic-2.1.1-20180720\n",
            "Done\n",
            "installing........\n",
            "configure\n",
            "make\n",
            "make install\n",
            "apt-get update\n",
            "apt-get upgrade\n",
            "apt install curl\n",
            "apt install git\n",
            "bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\n",
            "Done\n",
            "Successfully Installed\n",
            "Now you can use Mecab\n",
            "from konlpy.tag import Mecab\n",
            "mecab = Mecab()\n",
            "사용자 사전 추가 방법 : https://bit.ly/3k0ZH53\n",
            "NameError: name 'Tagger' is not defined 오류 발생 시 런타임을 재실행 해주세요\n",
            "블로그에 해결 방법을 남겨주신 tana님 감사합니다.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAan_c2I49-S",
        "outputId": "b7e536ef-cb8e-42bb-afd8-6c7a9ebb47a5"
      },
      "source": [
        "cd .."
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dq7J8XOV4Ic-"
      },
      "source": [
        "from konlpy.tag import Mecab\n",
        "mecab = Mecab()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1ia4Y8E0Nw7",
        "outputId": "aa7aaf41-f539-430e-d558-9cceb812b15d"
      },
      "source": [
        "%%time\n",
        "\n",
        "# load korean corpus for tokenizer training\n",
        "with open(input_file, 'r', encoding='utf-8') as f:\n",
        "    data = f.read().split('\\n')\n",
        "print(data[:3])\n",
        "\n",
        "# mecab for window는 아래 코드 사용\n",
        "from konlpy.tag import Mecab  # install mecab for window: https://hong-yp-ml-records.tistory.com/91\n",
        "mecab_tokenizer = Mecab().morphs\n",
        "print('mecab check :', mecab_tokenizer('어릴때보고 지금다시봐도 재밌어요ㅋㅋ'))\n",
        "\n",
        "for_generation = False # or normal\n",
        "\n",
        "if for_generation:\n",
        "    # 1: '어릴때' -> '어릴, ##때' for generation model\n",
        "    total_morph=[]\n",
        "    for sentence in data:\n",
        "        # 문장단위 mecab 적용\n",
        "        morph_sentence= []\n",
        "        count = 0\n",
        "        for token_mecab in mecab_tokenizer(sentence):\n",
        "            token_mecab_save = token_mecab\n",
        "            if count > 0:\n",
        "                token_mecab_save = \"##\" + token_mecab_save  # 앞에 ##를 부친다\n",
        "                morph_sentence.append(token_mecab_save)\n",
        "            else:\n",
        "                morph_sentence.append(token_mecab_save)\n",
        "                count += 1\n",
        "        # 문장단위 저장\n",
        "        total_morph.append(morph_sentence)\n",
        "\n",
        "else:\n",
        "    # 2: '어릴때' -> '어릴, 때'   for normal case\n",
        "    total_morph=[]\n",
        "    for sentence in data:\n",
        "        # 문장단위 mecab 적용\n",
        "        morph_sentence= mecab_tokenizer(sentence)\n",
        "        # 문장단위 저장\n",
        "        total_morph.append(morph_sentence)\n",
        "                        \n",
        "print(total_morph[:3])\n",
        "print(len(total_morph))\n",
        "\n",
        "# mecab 적용한 데이터 저장\n",
        "# ex) 1 line: '어릴 때 보 고 지금 다시 봐도 재밌 어요 ㅋㅋ'\n",
        "with open('/content/drive/MyDrive/Tokenizer_train/data/after_mecab.txt', 'w', encoding='utf-8') as f:\n",
        "    for line in total_morph:\n",
        "        f.write(' '.join(line)+'\\n')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['굳 ㅋ', 'GDNTOPCLASSINTHECLUB', '뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아']\n",
            "mecab check : ['어릴', '때', '보', '고', '지금', '다시', '봐도', '재밌', '어요', 'ㅋㅋ']\n",
            "[['굳', 'ㅋ'], ['GDNTOPCLASSINTHECLUB'], ['뭐', '야', '이', '평점', '들', '은', '.', '...', '나쁘', '진', '않', '지만', '10', '점', '짜리', '는', '더더욱', '아니', '잖아']]\n",
            "49998\n",
            "CPU times: user 5.15 s, sys: 244 ms, total: 5.4 s\n",
            "Wall time: 6.19 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jr_uwLua66Dj",
        "outputId": "ee88d651-170c-4200-8497-d5d598276ef9"
      },
      "source": [
        "## 1) define special tokens\n",
        "user_defined_symbols = ['[BOS]','[EOS]','[UNK0]','[UNK1]','[UNK2]','[UNK3]','[UNK4]','[UNK5]','[UNK6]','[UNK7]','[UNK8]','[UNK9]']\n",
        "unused_token_num = 200\n",
        "unused_list = ['[unused{}]'.format(n) for n in range(unused_token_num)]\n",
        "user_defined_symbols = user_defined_symbols + unused_list\n",
        "\n",
        "print(user_defined_symbols)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[BOS]', '[EOS]', '[UNK0]', '[UNK1]', '[UNK2]', '[UNK3]', '[UNK4]', '[UNK5]', '[UNK6]', '[UNK7]', '[UNK8]', '[UNK9]', '[unused0]', '[unused1]', '[unused2]', '[unused3]', '[unused4]', '[unused5]', '[unused6]', '[unused7]', '[unused8]', '[unused9]', '[unused10]', '[unused11]', '[unused12]', '[unused13]', '[unused14]', '[unused15]', '[unused16]', '[unused17]', '[unused18]', '[unused19]', '[unused20]', '[unused21]', '[unused22]', '[unused23]', '[unused24]', '[unused25]', '[unused26]', '[unused27]', '[unused28]', '[unused29]', '[unused30]', '[unused31]', '[unused32]', '[unused33]', '[unused34]', '[unused35]', '[unused36]', '[unused37]', '[unused38]', '[unused39]', '[unused40]', '[unused41]', '[unused42]', '[unused43]', '[unused44]', '[unused45]', '[unused46]', '[unused47]', '[unused48]', '[unused49]', '[unused50]', '[unused51]', '[unused52]', '[unused53]', '[unused54]', '[unused55]', '[unused56]', '[unused57]', '[unused58]', '[unused59]', '[unused60]', '[unused61]', '[unused62]', '[unused63]', '[unused64]', '[unused65]', '[unused66]', '[unused67]', '[unused68]', '[unused69]', '[unused70]', '[unused71]', '[unused72]', '[unused73]', '[unused74]', '[unused75]', '[unused76]', '[unused77]', '[unused78]', '[unused79]', '[unused80]', '[unused81]', '[unused82]', '[unused83]', '[unused84]', '[unused85]', '[unused86]', '[unused87]', '[unused88]', '[unused89]', '[unused90]', '[unused91]', '[unused92]', '[unused93]', '[unused94]', '[unused95]', '[unused96]', '[unused97]', '[unused98]', '[unused99]', '[unused100]', '[unused101]', '[unused102]', '[unused103]', '[unused104]', '[unused105]', '[unused106]', '[unused107]', '[unused108]', '[unused109]', '[unused110]', '[unused111]', '[unused112]', '[unused113]', '[unused114]', '[unused115]', '[unused116]', '[unused117]', '[unused118]', '[unused119]', '[unused120]', '[unused121]', '[unused122]', '[unused123]', '[unused124]', '[unused125]', '[unused126]', '[unused127]', '[unused128]', '[unused129]', '[unused130]', '[unused131]', '[unused132]', '[unused133]', '[unused134]', '[unused135]', '[unused136]', '[unused137]', '[unused138]', '[unused139]', '[unused140]', '[unused141]', '[unused142]', '[unused143]', '[unused144]', '[unused145]', '[unused146]', '[unused147]', '[unused148]', '[unused149]', '[unused150]', '[unused151]', '[unused152]', '[unused153]', '[unused154]', '[unused155]', '[unused156]', '[unused157]', '[unused158]', '[unused159]', '[unused160]', '[unused161]', '[unused162]', '[unused163]', '[unused164]', '[unused165]', '[unused166]', '[unused167]', '[unused168]', '[unused169]', '[unused170]', '[unused171]', '[unused172]', '[unused173]', '[unused174]', '[unused175]', '[unused176]', '[unused177]', '[unused178]', '[unused179]', '[unused180]', '[unused181]', '[unused182]', '[unused183]', '[unused184]', '[unused185]', '[unused186]', '[unused187]', '[unused188]', '[unused189]', '[unused190]', '[unused191]', '[unused192]', '[unused193]', '[unused194]', '[unused195]', '[unused196]', '[unused197]', '[unused198]', '[unused199]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BrNgVd067cd",
        "outputId": "3bdf23cd-262b-489f-aace-8b6de97262dd"
      },
      "source": [
        "%%time\n",
        "## 2) train\n",
        "import os\n",
        "from tokenizers import BertWordPieceTokenizer, SentencePieceBPETokenizer, CharBPETokenizer, ByteLevelBPETokenizer\n",
        "\n",
        "# 4가지중 tokenizer 선택\n",
        "# how_to_tokenize = BertWordPieceTokenizer  # The famous Bert tokenizer, using WordPiece\n",
        "# how_to_tokenize = SentencePieceBPETokenizer  # A BPE implementation compatible with the one used by SentencePiece\n",
        "# how_to_tokenize = CharBPETokenizer  # The original BPE\n",
        "# how_to_tokenize = ByteLevelBPETokenizer  # The byte level version of the BPE\n",
        "#tokenizer = BertWordPieceTokenizer(strip_accents=False,  # Must be False if cased model\n",
        "#                                       lowercase=False)\n",
        "\n",
        "\n",
        "tokenizer = BertWordPieceTokenizer()\n",
        "\n",
        "corpus_file   = ['/content/drive/MyDrive/Tokenizer_train/data/after_mecab.txt']  # data path\n",
        "vocab_size    = 32000\n",
        "limit_alphabet= 6000\n",
        "#output_path   = '/content/drive/MyDrive/Tokenizer_train/hugging_%d'%(vocab_size)\n",
        "min_frequency = 5\n",
        "\n",
        "# Then train it!\n",
        "tokenizer.train(files=corpus_file,\n",
        "               vocab_size=vocab_size,\n",
        "               min_frequency=min_frequency,  # 단어의 최소 발생 빈도, 5\n",
        "               limit_alphabet=limit_alphabet,  # ByteLevelBPETokenizer 학습시엔 주석처리 필요\n",
        "               show_progress=True)\n",
        "print('train complete')\n",
        "\n",
        "sentence = '나는 오늘 아침밥을 먹었다.'\n",
        "output = tokenizer.encode(sentence)\n",
        "print(sentence)\n",
        "print('=>idx   : %s'%output.ids)\n",
        "print('=>tokens: %s'%output.tokens)\n",
        "print('=>offset: %s'%output.offsets)\n",
        "print('=>decode: %s\\n'%tokenizer.decode(output.ids))\n",
        "\n",
        "sentence = 'I want to go my hometown'\n",
        "output = tokenizer.encode(sentence)\n",
        "print(sentence)\n",
        "print('=>idx   : %s'%output.ids)\n",
        "print('=>tokens: %s'%output.tokens)\n",
        "print('=>offset: %s'%output.offsets)\n",
        "print('=>decode: %s\\n'%tokenizer.decode(output.ids))\n",
        "\n",
        "# save tokenizer\n",
        "hf_model_path='/content/drive/MyDrive/Tokenizer_train/tokenizer_model'\n",
        "if not os.path.isdir(hf_model_path):\n",
        "    os.mkdir(hf_model_path)\n",
        "tokenizer.save_model(hf_model_path)  # vocab.txt 파일 한개가 만들어진다"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train complete\n",
            "나는 오늘 아침밥을 먹었다.\n",
            "=>idx   : [4768, 539, 1658, 2742, 3950, 1828, 1167, 1324, 566, 18]\n",
            "=>tokens: ['나ᄂ', '##ᅳᆫ', '오늘', '아침', '##밥', '##을', '먹', '##었', '##다', '.']\n",
            "=>offset: [(0, 2), (1, 2), (3, 5), (6, 8), (8, 9), (9, 10), (11, 12), (12, 13), (13, 14), (14, 15)]\n",
            "=>decode: 나는 오늘 아침밥을 먹었다.\n",
            "\n",
            "I want to go my hometown\n",
            "=>idx   : [51, 65, 10620, 3576, 6994, 5482, 50, 4275, 418, 414, 4438, 419]\n",
            "=>tokens: ['i', 'w', '##ant', 'to', 'go', 'my', 'h', '##om', '##e', '##t', '##ow', '##n']\n",
            "=>offset: [(0, 1), (2, 3), (3, 6), (7, 9), (10, 12), (13, 15), (16, 17), (17, 19), (19, 20), (20, 21), (21, 23), (23, 24)]\n",
            "=>decode: i want to go my hometown\n",
            "\n",
            "CPU times: user 5.1 s, sys: 267 ms, total: 5.37 s\n",
            "Wall time: 2.94 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 475
        },
        "id": "Z6jIMEkX8UAB",
        "outputId": "91389575-b571-4069-b6ba-07aa45154e38"
      },
      "source": [
        "from transformers import BertTokenizerFast, AlbertTokenizerFast\n",
        "\n",
        "tokenizer_for_load = AlbertTokenizerFast.from_pretrained(hf_model_path,\n",
        "                                                       strip_accents=False,  # Must be False if cased model\n",
        "                                                       lowercase=False)  # 로드\n",
        "\n",
        "print('vocab size : %d' % tokenizer_for_load.vocab_size)\n",
        "# tokenized_input_for_pytorch = tokenizer_for_load(\"i am very hungry\", return_tensors=\"pt\")\n",
        "tokenized_input_for_pytorch = tokenizer_for_load(\"나는 오늘 아침밥을 먹었다.\", return_tensors=\"pt\")\n",
        "tokenized_input_for_tensorflow = tokenizer_for_load(\"나는 오늘 아침밥을 먹었다.\", return_tensors=\"tf\")\n",
        "\n",
        "print(\"Tokens (str)      : {}\".format([tokenizer_for_load.convert_ids_to_tokens(s) for s in tokenized_input_for_pytorch['input_ids'].tolist()[0]]))\n",
        "print(\"Tokens (int)      : {}\".format(tokenized_input_for_pytorch['input_ids'].tolist()[0]))\n",
        "print(\"Tokens (attn_mask): {}\\n\".format(tokenized_input_for_pytorch['attention_mask'].tolist()[0]))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-071d2ecc36f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m tokenizer_for_load = AlbertTokenizerFast.from_pretrained(hf_model_path,\n\u001b[1;32m      4\u001b[0m                                                        \u001b[0mstrip_accents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Must be False if cased model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                                                        lowercase=False)  # 로드\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vocab size : %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtokenizer_for_load\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1691\u001b[0m                 \u001b[0;34mf\"- or '{pretrained_model_name_or_path}' is the correct path to a directory containing relevant tokenizer files\\n\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1692\u001b[0m             )\n\u001b[0;32m-> 1693\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1695\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfile_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvocab_files\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for '/content/drive/MyDrive/Tokenizer_train/tokenizer_model'. Make sure that:\n\n- '/content/drive/MyDrive/Tokenizer_train/tokenizer_model' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or '/content/drive/MyDrive/Tokenizer_train/tokenizer_model' is the correct path to a directory containing relevant tokenizer files\n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzGw-0CB8lmK",
        "outputId": "b4563c37-7442-4d40-8ef5-88d6db0f86f8"
      },
      "source": [
        "# special token check\n",
        "tokenizer_for_load.all_special_tokens # 추가하기 전 기본적인 special token"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBUrh8sM8xuX",
        "outputId": "6d630fd3-3da0-4b21-aeb4-f10f01b2e84e"
      },
      "source": [
        "# tokenizer에 special token 추가\n",
        "special_tokens_dict = {'additional_special_tokens': user_defined_symbols}\n",
        "tokenizer_for_load.add_special_tokens(special_tokens_dict)\n",
        "\n",
        "# check tokenizer vocab with special tokens\n",
        "print('check special tokens : %s'%tokenizer_for_load.all_special_tokens[:20])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "check special tokens : ['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]', '[BOS]', '[EOS]', '[UNK0]', '[UNK1]', '[UNK2]', '[UNK3]', '[UNK4]', '[UNK5]', '[UNK6]', '[UNK7]', '[UNK8]', '[UNK9]', '[unused0]', '[unused1]', '[unused2]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYV69C2e9DfA",
        "outputId": "512a7131-96b3-4baf-9fb7-24ddbf999b01"
      },
      "source": [
        "# save tokenizer model with special tokens\n",
        "tokenizer_for_load.save_pretrained(hf_model_path+'_special')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/Tokenizer_train/tokenizer_model_special/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/Tokenizer_train/tokenizer_model_special/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/Tokenizer_train/tokenizer_model_special/vocab.txt',\n",
              " '/content/drive/MyDrive/Tokenizer_train/tokenizer_model_special/added_tokens.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LE6N8QdS9Nqk",
        "outputId": "c2e8f1f0-d524-423e-9bb3-32ad0a4e2918"
      },
      "source": [
        "# check special tokens\n",
        "from transformers import BertTokenizerFast, AlbertTokenizerFast\n",
        "tokenizer_check = BertTokenizerFast.from_pretrained(hf_model_path+'_special')\n",
        "\n",
        "print('check special tokens : %s'%tokenizer_check.all_special_tokens[:20])\n",
        "\n",
        "print('vocab size : %d' % tokenizer_check.vocab_size)\n",
        "tokenized_input_for_pytorch = tokenizer_check(\"나는 오늘 아침밥을 먹었다.\", return_tensors=\"pt\")\n",
        "tokenized_input_for_tensorflow = tokenizer_check(\"나는 오늘 아침밥을 먹었다.\", return_tensors=\"tf\")\n",
        "\n",
        "print(\"Tokens (str)      : {}\".format([tokenizer_check.convert_ids_to_tokens(s) for s in tokenized_input_for_pytorch['input_ids'].tolist()[0]]))\n",
        "print(\"Tokens (int)      : {}\".format(tokenized_input_for_pytorch['input_ids'].tolist()[0]))\n",
        "print(\"Tokens (attn_mask): {}\\n\".format(tokenized_input_for_pytorch['attention_mask'].tolist()[0]))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "check special tokens : ['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]', '[BOS]', '[EOS]', '[UNK0]', '[UNK1]', '[UNK2]', '[UNK3]', '[UNK4]', '[UNK5]', '[UNK6]', '[UNK7]', '[UNK8]', '[UNK9]', '[unused0]', '[unused1]', '[unused2]']\n",
            "vocab size : 13201\n",
            "Tokens (str)      : ['[CLS]', '나', '##는', '오늘', '아침', '##밥', '##을', '먹', '##었', '##다', '.', '[SEP]']\n",
            "Tokens (int)      : [2, 555, 2425, 4213, 4895, 2793, 2639, 1014, 2521, 2426, 18, 3]\n",
            "Tokens (attn_mask): [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283,
          "referenced_widgets": [
            "aa2bb3c703cd47bbae4c3725ebbe8607",
            "6d0cb13c40b94435b8cc28c53e04be9f",
            "a82eba20d147451fae82808feebe2ee6",
            "f8338f63e39e4297960120064bcd6cfa",
            "3c771b594639409980363ad629509820",
            "97618f3b65be4d0dbc736b7f5d57bbbc",
            "aeeb3800b6e54c43947521d2b1db8bac",
            "6765b2b669cb4ccc9b95023fd76b36b9",
            "a5f744a372114cd1aa1b205d7fe38a78",
            "bd8b2b2d73e1470ba0c082828d499ef3",
            "6c79c2c1bee44c5487cdd7d54e4974ca",
            "1b0c3cf5a34d4a23a50378e3111fbaab",
            "0cd1019099024c8bb465eb84371dd313",
            "266830fdd6bf499698715a415a909773",
            "378f1fde5736470094c345f994541a65",
            "17250d596553432d9a0c313cd3be22e8",
            "ad87e8b4d9dd4c50a623bc0c5b3231b9",
            "6ee83dcf024a4fc59dedf8ed6793c961",
            "ba3a7416f39b48ad98221f3445345c39",
            "23a939dca94b4400a25b84a8f17feeca",
            "03d48499e49a4b1588cf99006f1ee2ca",
            "8d0c6df303274deb900ec2e4e60645fc",
            "66de0432831d40a9b58728f36d6469e7",
            "86ccde2a1f0f44ea9c079baa0d7ccc88"
          ]
        },
        "id": "S7CzjOER-SbQ",
        "outputId": "ce743ac8-13bb-4ba8-cbfb-21960275f140"
      },
      "source": [
        "# test to tf&pytorch bert model\n",
        "from transformers import TFBertModel, BertModel\n",
        "\n",
        "# load a BERT model for TensorFlow and PyTorch\n",
        "model_tf = TFBertModel.from_pretrained('bert-base-cased')\n",
        "model_pt = BertModel.from_pretrained('bert-base-cased')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aa2bb3c703cd47bbae4c3725ebbe8607",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=570.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a5f744a372114cd1aa1b205d7fe38a78",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=526681800.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ad87e8b4d9dd4c50a623bc0c5b3231b9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=435779157.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYeMzWLq-aAc",
        "outputId": "9b385d3c-6a30-4ebd-c378-d5bf4c8613fe"
      },
      "source": [
        "## tf vs torch bert output\n",
        "# transformers generates a ready to use dictionary with all the required parameters for the specific framework.\n",
        "input_tf = tokenizer_check(\"나는 오늘 아침밥을 먹었다.\", return_tensors=\"tf\")\n",
        "input_pt = tokenizer_check(\"나는 오늘 아침밥을 먹었다.\", return_tensors=\"pt\")\n",
        "\n",
        "# Let's compare the outputs\n",
        "output_tf, output_pt = model_tf(input_tf), model_pt(**input_pt)\n",
        "\n",
        "print('final layer output shape : %s'%(output_pt['last_hidden_state'].shape,))\n",
        "\n",
        "# Models outputs 2 values (The value for each tokens, the pooled representation of the input sentence)\n",
        "# Here we compare the output differences between PyTorch and TensorFlow.\n",
        "\n",
        "print('\\ntorch vs tf 결과차이')\n",
        "for name in [\"last_hidden_state\", \"pooler_output\"]:\n",
        "    print(\"   => {} differences: {:.5}\".format(name, (output_tf[name].numpy() - output_pt[name].detach().numpy()).sum()))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "final layer output shape : torch.Size([1, 12, 768])\n",
            "\n",
            "torch vs tf 결과차이\n",
            "   => last_hidden_state differences: 2.5626e-06\n",
            "   => pooler_output differences: -3.0823e-06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNXWYH5FRyu1"
      },
      "source": [
        "# 여기서부터 다시"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55T0J0MDFtrm",
        "outputId": "c82e9761-d16b-4668-97c8-0a5c5349f69f"
      },
      "source": [
        "## 1) define special tokens\n",
        "user_defined_symbols = ['[BOS]','[EOS]','[UNK0]','[UNK1]','[UNK2]','[UNK3]','[UNK4]','[UNK5]','[UNK6]','[UNK7]','[UNK8]','[UNK9]']\n",
        "unused_token_num = 200\n",
        "unused_list = ['[unused{}]'.format(n) for n in range(unused_token_num)]\n",
        "user_defined_symbols = user_defined_symbols + unused_list\n",
        "\n",
        "print(user_defined_symbols)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[BOS]', '[EOS]', '[UNK0]', '[UNK1]', '[UNK2]', '[UNK3]', '[UNK4]', '[UNK5]', '[UNK6]', '[UNK7]', '[UNK8]', '[UNK9]', '[unused0]', '[unused1]', '[unused2]', '[unused3]', '[unused4]', '[unused5]', '[unused6]', '[unused7]', '[unused8]', '[unused9]', '[unused10]', '[unused11]', '[unused12]', '[unused13]', '[unused14]', '[unused15]', '[unused16]', '[unused17]', '[unused18]', '[unused19]', '[unused20]', '[unused21]', '[unused22]', '[unused23]', '[unused24]', '[unused25]', '[unused26]', '[unused27]', '[unused28]', '[unused29]', '[unused30]', '[unused31]', '[unused32]', '[unused33]', '[unused34]', '[unused35]', '[unused36]', '[unused37]', '[unused38]', '[unused39]', '[unused40]', '[unused41]', '[unused42]', '[unused43]', '[unused44]', '[unused45]', '[unused46]', '[unused47]', '[unused48]', '[unused49]', '[unused50]', '[unused51]', '[unused52]', '[unused53]', '[unused54]', '[unused55]', '[unused56]', '[unused57]', '[unused58]', '[unused59]', '[unused60]', '[unused61]', '[unused62]', '[unused63]', '[unused64]', '[unused65]', '[unused66]', '[unused67]', '[unused68]', '[unused69]', '[unused70]', '[unused71]', '[unused72]', '[unused73]', '[unused74]', '[unused75]', '[unused76]', '[unused77]', '[unused78]', '[unused79]', '[unused80]', '[unused81]', '[unused82]', '[unused83]', '[unused84]', '[unused85]', '[unused86]', '[unused87]', '[unused88]', '[unused89]', '[unused90]', '[unused91]', '[unused92]', '[unused93]', '[unused94]', '[unused95]', '[unused96]', '[unused97]', '[unused98]', '[unused99]', '[unused100]', '[unused101]', '[unused102]', '[unused103]', '[unused104]', '[unused105]', '[unused106]', '[unused107]', '[unused108]', '[unused109]', '[unused110]', '[unused111]', '[unused112]', '[unused113]', '[unused114]', '[unused115]', '[unused116]', '[unused117]', '[unused118]', '[unused119]', '[unused120]', '[unused121]', '[unused122]', '[unused123]', '[unused124]', '[unused125]', '[unused126]', '[unused127]', '[unused128]', '[unused129]', '[unused130]', '[unused131]', '[unused132]', '[unused133]', '[unused134]', '[unused135]', '[unused136]', '[unused137]', '[unused138]', '[unused139]', '[unused140]', '[unused141]', '[unused142]', '[unused143]', '[unused144]', '[unused145]', '[unused146]', '[unused147]', '[unused148]', '[unused149]', '[unused150]', '[unused151]', '[unused152]', '[unused153]', '[unused154]', '[unused155]', '[unused156]', '[unused157]', '[unused158]', '[unused159]', '[unused160]', '[unused161]', '[unused162]', '[unused163]', '[unused164]', '[unused165]', '[unused166]', '[unused167]', '[unused168]', '[unused169]', '[unused170]', '[unused171]', '[unused172]', '[unused173]', '[unused174]', '[unused175]', '[unused176]', '[unused177]', '[unused178]', '[unused179]', '[unused180]', '[unused181]', '[unused182]', '[unused183]', '[unused184]', '[unused185]', '[unused186]', '[unused187]', '[unused188]', '[unused189]', '[unused190]', '[unused191]', '[unused192]', '[unused193]', '[unused194]', '[unused195]', '[unused196]', '[unused197]', '[unused198]', '[unused199]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XF_Q_0VTFzPv",
        "outputId": "8dd91e2a-0960-4b17-8397-3afcf2e97ca7"
      },
      "source": [
        "#'/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model'\n",
        "\n",
        "from transformers import AlbertTokenizer, AlbertTokenizerFast\n",
        "\n",
        "albet_tokenizer_model = '/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model'\n",
        "\n",
        "tokenizer = AlbertTokenizerFast.from_pretrained(albet_tokenizer_model)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "QB0SlhA4GMQQ",
        "outputId": "1fec8be5-79c0-4515-d19d-9513e478c0f6"
      },
      "source": [
        "op = tokenizer.encode(\"나는 오늘 학교에 간다.\")\n",
        "print(op)\n",
        "tokenizer.decode(op)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[5, 168, 459, 397, 707, 473, 1823, 17709, 6]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[CLS] 나는 오늘 학교에 간다.[SEP]'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMDfH8GZF_mK",
        "outputId": "63a81265-41d2-4ec8-dfc2-0846dff817a7"
      },
      "source": [
        "# tokenizer에 special token 추가\n",
        "special_tokens_dict = {'additional_special_tokens': user_defined_symbols}\n",
        "tokenizer.add_special_tokens(special_tokens_dict)\n",
        "\n",
        "# check tokenizer vocab with special tokens\n",
        "print('check special tokens : %s'%tokenizer.all_special_tokens[:20])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "check special tokens : ['[CLS]', '[SEP]', '<unk>', '<pad>', '[MASK]', '[BOS]', '[EOS]', '[UNK0]', '[UNK1]', '[UNK2]', '[UNK3]', '[UNK4]', '[UNK5]', '[UNK6]', '[UNK7]', '[UNK8]', '[UNK9]', '[unused0]', '[unused1]', '[unused2]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Inr3cDwQG6Fs",
        "outputId": "3369ca3c-e891-45ac-a9ef-bded384ea97d"
      },
      "source": [
        "# save tokenizer model with special tokens\n",
        "tokenizer.save_pretrained(albet_tokenizer_model+'_special')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special/spiece.model',\n",
              " '/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special/added_tokens.json',\n",
              " '/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model_special/tokenizer.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwhlWrXXHr6L"
      },
      "source": [
        "tokenizer = AlbertTokenizerFast.from_pretrained(albet_tokenizer_model+'_special')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgHhYsZbH3zr",
        "outputId": "638d9a0e-9db5-4dff-ee64-c783a3ee4de4"
      },
      "source": [
        "op = tokenizer(\"나는 오늘 학교에 간다.\", return_tensors=\"pt\")\n",
        "print(op)\n",
        "print(\"Tokens (str)      : {}\".format([tokenizer.convert_ids_to_tokens(s) for s in op['input_ids'].tolist()[0]]))\n",
        "print(\"Tokens (int)      : {}\".format(op['input_ids'].tolist()[0]))\n",
        "print(\"Tokens (attn_mask): {}\\n\".format(op['attention_mask'].tolist()[0]))\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'input_ids': tensor([[    5,   168,   459,   397,   707,   473,  1823, 17709,     6]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
            "Tokens (str)      : ['[CLS]', '▁나', '는', '▁오늘', '▁학교', '에', '▁간다', '.', '[SEP]']\n",
            "Tokens (int)      : [5, 168, 459, 397, 707, 473, 1823, 17709, 6]\n",
            "Tokens (attn_mask): [1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "paElX_hhJAoh",
        "outputId": "4eab35da-0b2d-451d-833d-bad7c66754f9"
      },
      "source": [
        "#Checking vocabulary size\n",
        "vocab_size=tokenizer.vocab_size ; vocab_size"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30015"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXtO2BQaGdlJ",
        "outputId": "e9c4401d-aae4-4d27-96e9-e986639844c6"
      },
      "source": [
        "#Checking vocabulary size\n",
        "vocab_size=tokenizer.vocab_size ; vocab_size"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30015"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBitBQ_TGgo9"
      },
      "source": [
        "import json\n",
        "\n",
        "config = {\n",
        "    \"architectures\": [\n",
        "        \"AlbertModel\"\n",
        "    ],\n",
        "\t\"attention_probs_dropout_prob\": 0.1,\n",
        "\t\"hidden_act\": \"gelu\",\n",
        "\t\"hidden_dropout_prob\": 0.1,\n",
        "\t\"hidden_size\": 768,\n",
        "\t\"initializer_range\": 0.02,\n",
        "\t\"intermediate_size\": 3072,\n",
        "\t\"layer_norm_eps\": 1e-05,\n",
        "\t\"max_position_embeddings\": 514,\n",
        "\t\"model_type\": \"albert\",\n",
        "\t\"num_attention_heads\": 12,\n",
        "\t\"num_hidden_layers\": 6,\n",
        "\t\"type_vocab_size\": 1,\n",
        "\t\"vocab_size\": vocab_size\n",
        "}\n",
        "with open(albet_tokenizer_model + \"/config.json\", 'w') as fp:\n",
        "    json.dump(config, fp)\n",
        "\n",
        "\n",
        "#Configuration for tokenizer.\n",
        "#Note: I set do_lower_case: False, and keep_accents:True\n",
        "\n",
        "tokenizer_config = {\n",
        "\t\"max_len\": 512,\n",
        "\t\"model_type\": \"albert\",\n",
        "\t\"do_lower_case\":False, \n",
        "\t\"keep_accents\":True\n",
        "}\n",
        "with open(albet_tokenizer_model+ \"/tokenizer_config.json\", 'w') as fp:\n",
        "    json.dump(tokenizer_config, fp)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwPbX9lEHJFX",
        "outputId": "17e50c9b-7f51-4876-ed6a-6fb616d17419"
      },
      "source": [
        "import torch\n",
        "import gc\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1941"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmhyZHUlH9YP",
        "outputId": "d4f8ee88-23b4-43fd-f928-3e3da01652b5"
      },
      "source": [
        "#To train from scratch\n",
        "!python /content/transformers/examples/pytorch/language-modeling/run_mlm.py \\\n",
        "        --model_name_or_path albert-base-v2 \\\n",
        "        --config_name /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model/ \\\n",
        "        --tokenizer_name /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model/ \\\n",
        "        --train_file /content/drive/MyDrive/Tokenizer_train/data/train_tokenizer.txt \\\n",
        "        --output_dir /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model/model \\\n",
        "        --use_fast_tokenizer 1 \\\n",
        "        --do_train 1 \\\n",
        "        --line_by_line 1 \\\n",
        "        --save_steps 500 \\\n",
        "        --logging_steps 500 \\\n",
        "        --save_total_limit 2 \\\n",
        "        --num_train_epochs 1 \\\n",
        "        --seed 108 \\\n",
        "        --logging_dir /content/drive/MyDrive/Tokenizer_train/logs"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-15 08:08:38.675014: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "05/15/2021 08:08:40 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\n",
            "05/15/2021 08:08:40 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model/model, overwrite_output_dir=False, do_train=True, do_eval=False, do_predict=False, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=/content/drive/MyDrive/Tokenizer_train/logs, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.STEPS, save_steps=500, save_total_limit=2, no_cuda=False, seed=108, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model/model, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, _n_gpu=0, mp_parameters=)\n",
            "05/15/2021 08:08:40 - WARNING - datasets.builder -   Using custom data configuration default-87331c72d1317c34\n",
            "05/15/2021 08:08:40 - WARNING - datasets.builder -   Reusing dataset text (/root/.cache/huggingface/datasets/text/default-87331c72d1317c34/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5)\n",
            "[INFO|configuration_utils.py:515] 2021-05-15 08:08:40,722 >> loading configuration file /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model/config.json\n",
            "[INFO|configuration_utils.py:553] 2021-05-15 08:08:40,723 >> Model config AlbertConfig {\n",
            "  \"architectures\": [\n",
            "    \"AlbertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.7.0.dev0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"vocab_size\": 30015\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:515] 2021-05-15 08:08:40,724 >> loading configuration file /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model/config.json\n",
            "[INFO|configuration_utils.py:553] 2021-05-15 08:08:40,725 >> Model config AlbertConfig {\n",
            "  \"architectures\": [\n",
            "    \"AlbertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.7.0.dev0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"vocab_size\": 30015\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1651] 2021-05-15 08:08:40,726 >> Didn't find file /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model/tokenizer.json. We won't load it.\n",
            "[INFO|tokenization_utils_base.py:1651] 2021-05-15 08:08:40,726 >> Didn't find file /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model/added_tokens.json. We won't load it.\n",
            "[INFO|tokenization_utils_base.py:1651] 2021-05-15 08:08:40,727 >> Didn't find file /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model/special_tokens_map.json. We won't load it.\n",
            "[INFO|tokenization_utils_base.py:1715] 2021-05-15 08:08:40,728 >> loading file /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model/spiece.model\n",
            "[INFO|tokenization_utils_base.py:1715] 2021-05-15 08:08:40,728 >> loading file None\n",
            "[INFO|tokenization_utils_base.py:1715] 2021-05-15 08:08:40,728 >> loading file None\n",
            "[INFO|tokenization_utils_base.py:1715] 2021-05-15 08:08:40,728 >> loading file None\n",
            "[INFO|tokenization_utils_base.py:1715] 2021-05-15 08:08:40,728 >> loading file /content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model/tokenizer_config.json\n",
            "[INFO|tokenization_utils.py:207] 2021-05-15 08:08:40,778 >> Adding <pad> to the vocabulary\n",
            "[WARNING|tokenization_utils_base.py:1847] 2021-05-15 08:08:40,779 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[WARNING|tokenization_utils_base.py:1847] 2021-05-15 08:08:40,879 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|modeling_utils.py:1155] 2021-05-15 08:08:40,912 >> loading weights file https://huggingface.co/albert-base-v2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/bf1986d976e9a8320cbd3a0597e610bf299d639ce31b7ca581cbf54be3aaa6d3.d6d54047dfe6ae844e3bf6e7a7d0aff71cb598d3df019361e076ba7639b1da9b\n",
            "[INFO|modeling_utils.py:1339] 2021-05-15 08:08:41,097 >> All model checkpoint weights were used when initializing AlbertForMaskedLM.\n",
            "\n",
            "[INFO|modeling_utils.py:1348] 2021-05-15 08:08:41,097 >> All the weights of AlbertForMaskedLM were initialized from the model checkpoint at albert-base-v2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertForMaskedLM for predictions without further training.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/transformers/examples/pytorch/language-modeling/run_mlm.py\", line 497, in <module>\n",
            "    main()\n",
            "  File \"/content/transformers/examples/pytorch/language-modeling/run_mlm.py\", line 310, in main\n",
            "    use_auth_token=True if model_args.use_auth_token else None,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/auto/auto_factory.py\", line 381, in from_pretrained\n",
            "    return model_class.from_pretrained(pretrained_model_name_or_path, *model_args, config=config, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py\", line 1214, in from_pretrained\n",
            "    model, state_dict, pretrained_model_name_or_path, _fast_init=_fast_init\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py\", line 1354, in _load_state_dict_into_model\n",
            "    raise RuntimeError(f\"Error(s) in loading state_dict for {model.__class__.__name__}:\\n\\t{error_msg}\")\n",
            "RuntimeError: Error(s) in loading state_dict for AlbertForMaskedLM:\n",
            "\tsize mismatch for albert.embeddings.word_embeddings.weight: copying a param with shape torch.Size([30000, 128]) from checkpoint, the shape in current model is torch.Size([30015, 128]).\n",
            "\tsize mismatch for albert.embeddings.position_embeddings.weight: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([514, 128]).\n",
            "\tsize mismatch for albert.embeddings.token_type_embeddings.weight: copying a param with shape torch.Size([2, 128]) from checkpoint, the shape in current model is torch.Size([1, 128]).\n",
            "\tsize mismatch for predictions.bias: copying a param with shape torch.Size([30000]) from checkpoint, the shape in current model is torch.Size([30015]).\n",
            "\tsize mismatch for predictions.decoder.weight: copying a param with shape torch.Size([30000, 128]) from checkpoint, the shape in current model is torch.Size([30015, 128]).\n",
            "\tsize mismatch for predictions.decoder.bias: copying a param with shape torch.Size([30000]) from checkpoint, the shape in current model is torch.Size([30015]).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXpdZtduA23J",
        "outputId": "0e5eac42-4de7-488a-bbdc-b99e96db333e"
      },
      "source": [
        "!python /content/transformers/examples/pytorch/language-modeling/run_mlm.py --help"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-15 07:53:03.897640: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "usage: run_mlm.py [-h] [--model_name_or_path MODEL_NAME_OR_PATH]\n",
            "                  [--model_type MODEL_TYPE] [--config_name CONFIG_NAME]\n",
            "                  [--tokenizer_name TOKENIZER_NAME] [--cache_dir CACHE_DIR]\n",
            "                  [--no_use_fast_tokenizer]\n",
            "                  [--use_fast_tokenizer [USE_FAST_TOKENIZER]]\n",
            "                  [--model_revision MODEL_REVISION]\n",
            "                  [--use_auth_token [USE_AUTH_TOKEN]]\n",
            "                  [--dataset_name DATASET_NAME]\n",
            "                  [--dataset_config_name DATASET_CONFIG_NAME]\n",
            "                  [--train_file TRAIN_FILE]\n",
            "                  [--validation_file VALIDATION_FILE]\n",
            "                  [--overwrite_cache [OVERWRITE_CACHE]]\n",
            "                  [--validation_split_percentage VALIDATION_SPLIT_PERCENTAGE]\n",
            "                  [--max_seq_length MAX_SEQ_LENGTH]\n",
            "                  [--preprocessing_num_workers PREPROCESSING_NUM_WORKERS]\n",
            "                  [--mlm_probability MLM_PROBABILITY]\n",
            "                  [--line_by_line [LINE_BY_LINE]]\n",
            "                  [--pad_to_max_length [PAD_TO_MAX_LENGTH]]\n",
            "                  [--max_train_samples MAX_TRAIN_SAMPLES]\n",
            "                  [--max_eval_samples MAX_EVAL_SAMPLES] --output_dir\n",
            "                  OUTPUT_DIR [--overwrite_output_dir [OVERWRITE_OUTPUT_DIR]]\n",
            "                  [--do_train [DO_TRAIN]] [--do_eval [DO_EVAL]]\n",
            "                  [--do_predict [DO_PREDICT]]\n",
            "                  [--evaluation_strategy {no,steps,epoch}]\n",
            "                  [--prediction_loss_only [PREDICTION_LOSS_ONLY]]\n",
            "                  [--per_device_train_batch_size PER_DEVICE_TRAIN_BATCH_SIZE]\n",
            "                  [--per_device_eval_batch_size PER_DEVICE_EVAL_BATCH_SIZE]\n",
            "                  [--per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE]\n",
            "                  [--per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE]\n",
            "                  [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n",
            "                  [--eval_accumulation_steps EVAL_ACCUMULATION_STEPS]\n",
            "                  [--learning_rate LEARNING_RATE]\n",
            "                  [--weight_decay WEIGHT_DECAY] [--adam_beta1 ADAM_BETA1]\n",
            "                  [--adam_beta2 ADAM_BETA2] [--adam_epsilon ADAM_EPSILON]\n",
            "                  [--max_grad_norm MAX_GRAD_NORM]\n",
            "                  [--num_train_epochs NUM_TRAIN_EPOCHS]\n",
            "                  [--max_steps MAX_STEPS]\n",
            "                  [--lr_scheduler_type {linear,cosine,cosine_with_restarts,polynomial,constant,constant_with_warmup}]\n",
            "                  [--warmup_ratio WARMUP_RATIO] [--warmup_steps WARMUP_STEPS]\n",
            "                  [--logging_dir LOGGING_DIR]\n",
            "                  [--logging_strategy {no,steps,epoch}]\n",
            "                  [--logging_first_step [LOGGING_FIRST_STEP]]\n",
            "                  [--logging_steps LOGGING_STEPS]\n",
            "                  [--save_strategy {no,steps,epoch}] [--save_steps SAVE_STEPS]\n",
            "                  [--save_total_limit SAVE_TOTAL_LIMIT] [--no_cuda [NO_CUDA]]\n",
            "                  [--seed SEED] [--fp16 [FP16]]\n",
            "                  [--fp16_opt_level FP16_OPT_LEVEL]\n",
            "                  [--fp16_backend {auto,amp,apex}]\n",
            "                  [--fp16_full_eval [FP16_FULL_EVAL]]\n",
            "                  [--local_rank LOCAL_RANK] [--tpu_num_cores TPU_NUM_CORES]\n",
            "                  [--tpu_metrics_debug [TPU_METRICS_DEBUG]] [--debug DEBUG]\n",
            "                  [--dataloader_drop_last [DATALOADER_DROP_LAST]]\n",
            "                  [--eval_steps EVAL_STEPS]\n",
            "                  [--dataloader_num_workers DATALOADER_NUM_WORKERS]\n",
            "                  [--past_index PAST_INDEX] [--run_name RUN_NAME]\n",
            "                  [--disable_tqdm DISABLE_TQDM] [--no_remove_unused_columns]\n",
            "                  [--remove_unused_columns [REMOVE_UNUSED_COLUMNS]]\n",
            "                  [--label_names LABEL_NAMES [LABEL_NAMES ...]]\n",
            "                  [--load_best_model_at_end [LOAD_BEST_MODEL_AT_END]]\n",
            "                  [--metric_for_best_model METRIC_FOR_BEST_MODEL]\n",
            "                  [--greater_is_better GREATER_IS_BETTER]\n",
            "                  [--ignore_data_skip [IGNORE_DATA_SKIP]]\n",
            "                  [--sharded_ddp SHARDED_DDP] [--deepspeed DEEPSPEED]\n",
            "                  [--label_smoothing_factor LABEL_SMOOTHING_FACTOR]\n",
            "                  [--adafactor [ADAFACTOR]]\n",
            "                  [--group_by_length [GROUP_BY_LENGTH]]\n",
            "                  [--length_column_name LENGTH_COLUMN_NAME]\n",
            "                  [--report_to REPORT_TO [REPORT_TO ...]]\n",
            "                  [--ddp_find_unused_parameters DDP_FIND_UNUSED_PARAMETERS]\n",
            "                  [--no_dataloader_pin_memory]\n",
            "                  [--dataloader_pin_memory [DATALOADER_PIN_MEMORY]]\n",
            "                  [--skip_memory_metrics [SKIP_MEMORY_METRICS]]\n",
            "                  [--use_legacy_prediction_loop [USE_LEGACY_PREDICTION_LOOP]]\n",
            "                  [--push_to_hub [PUSH_TO_HUB]]\n",
            "                  [--resume_from_checkpoint RESUME_FROM_CHECKPOINT]\n",
            "                  [--mp_parameters MP_PARAMETERS]\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  --model_name_or_path MODEL_NAME_OR_PATH\n",
            "                        The model checkpoint for weights initialization.Don't\n",
            "                        set if you want to train a model from scratch.\n",
            "  --model_type MODEL_TYPE\n",
            "                        If training from scratch, pass a model type from the\n",
            "                        list: big_bird, wav2vec2, convbert, layoutlm,\n",
            "                        distilbert, albert, bart, mbart, camembert, xlm-\n",
            "                        roberta, longformer, roberta, squeezebert, bert,\n",
            "                        megatron-bert, mobilebert, flaubert, xlm, electra,\n",
            "                        reformer, funnel, mpnet, tapas, deberta, deberta-v2,\n",
            "                        ibert\n",
            "  --config_name CONFIG_NAME\n",
            "                        Pretrained config name or path if not the same as\n",
            "                        model_name\n",
            "  --tokenizer_name TOKENIZER_NAME\n",
            "                        Pretrained tokenizer name or path if not the same as\n",
            "                        model_name\n",
            "  --cache_dir CACHE_DIR\n",
            "                        Where do you want to store the pretrained models\n",
            "                        downloaded from huggingface.co\n",
            "  --no_use_fast_tokenizer\n",
            "                        Whether to use one of the fast tokenizer (backed by\n",
            "                        the tokenizers library) or not.\n",
            "  --use_fast_tokenizer [USE_FAST_TOKENIZER]\n",
            "                        Whether to use one of the fast tokenizer (backed by\n",
            "                        the tokenizers library) or not.\n",
            "  --model_revision MODEL_REVISION\n",
            "                        The specific model version to use (can be a branch\n",
            "                        name, tag name or commit id).\n",
            "  --use_auth_token [USE_AUTH_TOKEN]\n",
            "                        Will use the token generated when running\n",
            "                        `transformers-cli login` (necessary to use this script\n",
            "                        with private models).\n",
            "  --dataset_name DATASET_NAME\n",
            "                        The name of the dataset to use (via the datasets\n",
            "                        library).\n",
            "  --dataset_config_name DATASET_CONFIG_NAME\n",
            "                        The configuration name of the dataset to use (via the\n",
            "                        datasets library).\n",
            "  --train_file TRAIN_FILE\n",
            "                        The input training data file (a text file).\n",
            "  --validation_file VALIDATION_FILE\n",
            "                        An optional input evaluation data file to evaluate the\n",
            "                        perplexity on (a text file).\n",
            "  --overwrite_cache [OVERWRITE_CACHE]\n",
            "                        Overwrite the cached training and evaluation sets\n",
            "  --validation_split_percentage VALIDATION_SPLIT_PERCENTAGE\n",
            "                        The percentage of the train set used as validation set\n",
            "                        in case there's no validation split\n",
            "  --max_seq_length MAX_SEQ_LENGTH\n",
            "                        The maximum total input sequence length after\n",
            "                        tokenization. Sequences longer than this will be\n",
            "                        truncated.\n",
            "  --preprocessing_num_workers PREPROCESSING_NUM_WORKERS\n",
            "                        The number of processes to use for the preprocessing.\n",
            "  --mlm_probability MLM_PROBABILITY\n",
            "                        Ratio of tokens to mask for masked language modeling\n",
            "                        loss\n",
            "  --line_by_line [LINE_BY_LINE]\n",
            "                        Whether distinct lines of text in the dataset are to\n",
            "                        be handled as distinct sequences.\n",
            "  --pad_to_max_length [PAD_TO_MAX_LENGTH]\n",
            "                        Whether to pad all samples to `max_seq_length`. If\n",
            "                        False, will pad the samples dynamically when batching\n",
            "                        to the maximum length in the batch.\n",
            "  --max_train_samples MAX_TRAIN_SAMPLES\n",
            "                        For debugging purposes or quicker training, truncate\n",
            "                        the number of training examples to this value if set.\n",
            "  --max_eval_samples MAX_EVAL_SAMPLES\n",
            "                        For debugging purposes or quicker training, truncate\n",
            "                        the number of evaluation examples to this value if\n",
            "                        set.\n",
            "  --output_dir OUTPUT_DIR\n",
            "                        The output directory where the model predictions and\n",
            "                        checkpoints will be written.\n",
            "  --overwrite_output_dir [OVERWRITE_OUTPUT_DIR]\n",
            "                        Overwrite the content of the output directory.Use this\n",
            "                        to continue training if output_dir points to a\n",
            "                        checkpoint directory.\n",
            "  --do_train [DO_TRAIN]\n",
            "                        Whether to run training.\n",
            "  --do_eval [DO_EVAL]   Whether to run eval on the dev set.\n",
            "  --do_predict [DO_PREDICT]\n",
            "                        Whether to run predictions on the test set.\n",
            "  --evaluation_strategy {no,steps,epoch}\n",
            "                        The evaluation strategy to use.\n",
            "  --prediction_loss_only [PREDICTION_LOSS_ONLY]\n",
            "                        When performing evaluation and predictions, only\n",
            "                        returns the loss.\n",
            "  --per_device_train_batch_size PER_DEVICE_TRAIN_BATCH_SIZE\n",
            "                        Batch size per GPU/TPU core/CPU for training.\n",
            "  --per_device_eval_batch_size PER_DEVICE_EVAL_BATCH_SIZE\n",
            "                        Batch size per GPU/TPU core/CPU for evaluation.\n",
            "  --per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE\n",
            "                        Deprecated, the use of `--per_device_train_batch_size`\n",
            "                        is preferred. Batch size per GPU/TPU core/CPU for\n",
            "                        training.\n",
            "  --per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE\n",
            "                        Deprecated, the use of `--per_device_eval_batch_size`\n",
            "                        is preferred.Batch size per GPU/TPU core/CPU for\n",
            "                        evaluation.\n",
            "  --gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS\n",
            "                        Number of updates steps to accumulate before\n",
            "                        performing a backward/update pass.\n",
            "  --eval_accumulation_steps EVAL_ACCUMULATION_STEPS\n",
            "                        Number of predictions steps to accumulate before\n",
            "                        moving the tensors to the CPU.\n",
            "  --learning_rate LEARNING_RATE\n",
            "                        The initial learning rate for AdamW.\n",
            "  --weight_decay WEIGHT_DECAY\n",
            "                        Weight decay for AdamW if we apply some.\n",
            "  --adam_beta1 ADAM_BETA1\n",
            "                        Beta1 for AdamW optimizer\n",
            "  --adam_beta2 ADAM_BETA2\n",
            "                        Beta2 for AdamW optimizer\n",
            "  --adam_epsilon ADAM_EPSILON\n",
            "                        Epsilon for AdamW optimizer.\n",
            "  --max_grad_norm MAX_GRAD_NORM\n",
            "                        Max gradient norm.\n",
            "  --num_train_epochs NUM_TRAIN_EPOCHS\n",
            "                        Total number of training epochs to perform.\n",
            "  --max_steps MAX_STEPS\n",
            "                        If > 0: set total number of training steps to perform.\n",
            "                        Override num_train_epochs.\n",
            "  --lr_scheduler_type {linear,cosine,cosine_with_restarts,polynomial,constant,constant_with_warmup}\n",
            "                        The scheduler type to use.\n",
            "  --warmup_ratio WARMUP_RATIO\n",
            "                        Linear warmup over warmup_ratio fraction of total\n",
            "                        steps.\n",
            "  --warmup_steps WARMUP_STEPS\n",
            "                        Linear warmup over warmup_steps.\n",
            "  --logging_dir LOGGING_DIR\n",
            "                        Tensorboard log dir.\n",
            "  --logging_strategy {no,steps,epoch}\n",
            "                        The logging strategy to use.\n",
            "  --logging_first_step [LOGGING_FIRST_STEP]\n",
            "                        Log the first global_step\n",
            "  --logging_steps LOGGING_STEPS\n",
            "                        Log every X updates steps.\n",
            "  --save_strategy {no,steps,epoch}\n",
            "                        The checkpoint save strategy to use.\n",
            "  --save_steps SAVE_STEPS\n",
            "                        Save checkpoint every X updates steps.\n",
            "  --save_total_limit SAVE_TOTAL_LIMIT\n",
            "                        Limit the total amount of checkpoints.Deletes the\n",
            "                        older checkpoints in the output_dir. Default is\n",
            "                        unlimited checkpoints\n",
            "  --no_cuda [NO_CUDA]   Do not use CUDA even when it is available\n",
            "  --seed SEED           Random seed that will be set at the beginning of\n",
            "                        training.\n",
            "  --fp16 [FP16]         Whether to use 16-bit (mixed) precision instead of\n",
            "                        32-bit\n",
            "  --fp16_opt_level FP16_OPT_LEVEL\n",
            "                        For fp16: Apex AMP optimization level selected in\n",
            "                        ['O0', 'O1', 'O2', and 'O3'].See details at\n",
            "                        https://nvidia.github.io/apex/amp.html\n",
            "  --fp16_backend {auto,amp,apex}\n",
            "                        The backend to be used for mixed precision.\n",
            "  --fp16_full_eval [FP16_FULL_EVAL]\n",
            "                        Whether to use full 16-bit precision evaluation\n",
            "                        instead of 32-bit\n",
            "  --local_rank LOCAL_RANK\n",
            "                        For distributed training: local_rank\n",
            "  --tpu_num_cores TPU_NUM_CORES\n",
            "                        TPU: Number of TPU cores (automatically passed by\n",
            "                        launcher script)\n",
            "  --tpu_metrics_debug [TPU_METRICS_DEBUG]\n",
            "                        Deprecated, the use of `--debug tpu_metrics_debug` is\n",
            "                        preferred. TPU: Whether to print debug metrics\n",
            "  --debug DEBUG         Whether or not to enable debug mode. Current options:\n",
            "                        `underflow_overflow` (Detect underflow and overflow in\n",
            "                        activations and weights), `tpu_metrics_debug` (print\n",
            "                        debug metrics on TPU).\n",
            "  --dataloader_drop_last [DATALOADER_DROP_LAST]\n",
            "                        Drop the last incomplete batch if it is not divisible\n",
            "                        by the batch size.\n",
            "  --eval_steps EVAL_STEPS\n",
            "                        Run an evaluation every X steps.\n",
            "  --dataloader_num_workers DATALOADER_NUM_WORKERS\n",
            "                        Number of subprocesses to use for data loading\n",
            "                        (PyTorch only). 0 means that the data will be loaded\n",
            "                        in the main process.\n",
            "  --past_index PAST_INDEX\n",
            "                        If >=0, uses the corresponding part of the output as\n",
            "                        the past state for next step.\n",
            "  --run_name RUN_NAME   An optional descriptor for the run. Notably used for\n",
            "                        wandb logging.\n",
            "  --disable_tqdm DISABLE_TQDM\n",
            "                        Whether or not to disable the tqdm progress bars.\n",
            "  --no_remove_unused_columns\n",
            "                        Remove columns not required by the model when using an\n",
            "                        nlp.Dataset.\n",
            "  --remove_unused_columns [REMOVE_UNUSED_COLUMNS]\n",
            "                        Remove columns not required by the model when using an\n",
            "                        nlp.Dataset.\n",
            "  --label_names LABEL_NAMES [LABEL_NAMES ...]\n",
            "                        The list of keys in your dictionary of inputs that\n",
            "                        correspond to the labels.\n",
            "  --load_best_model_at_end [LOAD_BEST_MODEL_AT_END]\n",
            "                        Whether or not to load the best model found during\n",
            "                        training at the end of training.\n",
            "  --metric_for_best_model METRIC_FOR_BEST_MODEL\n",
            "                        The metric to use to compare two different models.\n",
            "  --greater_is_better GREATER_IS_BETTER\n",
            "                        Whether the `metric_for_best_model` should be\n",
            "                        maximized or not.\n",
            "  --ignore_data_skip [IGNORE_DATA_SKIP]\n",
            "                        When resuming training, whether or not to skip the\n",
            "                        first epochs and batches to get to the same training\n",
            "                        data.\n",
            "  --sharded_ddp SHARDED_DDP\n",
            "                        Whether or not to use sharded DDP training (in\n",
            "                        distributed training only). The base option should be\n",
            "                        `simple`, `zero_dp_2` or `zero_dp_3` and you can add\n",
            "                        CPU-offload to `zero_dp_2` or `zero_dp_3` like this:\n",
            "                        zero_dp_2 offload` or `zero_dp_3 offload`. You can add\n",
            "                        auto-wrap to `zero_dp_2` or with the same syntax:\n",
            "                        zero_dp_2 auto_wrap` or `zero_dp_3 auto_wrap`.\n",
            "  --deepspeed DEEPSPEED\n",
            "                        Enable deepspeed and pass the path to deepspeed json\n",
            "                        config file (e.g. ds_config.json) or an already loaded\n",
            "                        json file as a dict\n",
            "  --label_smoothing_factor LABEL_SMOOTHING_FACTOR\n",
            "                        The label smoothing epsilon to apply (zero means no\n",
            "                        label smoothing).\n",
            "  --adafactor [ADAFACTOR]\n",
            "                        Whether or not to replace AdamW by Adafactor.\n",
            "  --group_by_length [GROUP_BY_LENGTH]\n",
            "                        Whether or not to group samples of roughly the same\n",
            "                        length together when batching.\n",
            "  --length_column_name LENGTH_COLUMN_NAME\n",
            "                        Column name with precomputed lengths to use when\n",
            "                        grouping by length.\n",
            "  --report_to REPORT_TO [REPORT_TO ...]\n",
            "                        The list of integrations to report the results and\n",
            "                        logs to.\n",
            "  --ddp_find_unused_parameters DDP_FIND_UNUSED_PARAMETERS\n",
            "                        When using distributed training, the value of the flag\n",
            "                        `find_unused_parameters` passed to\n",
            "                        `DistributedDataParallel`.\n",
            "  --no_dataloader_pin_memory\n",
            "                        Whether or not to pin memory for DataLoader.\n",
            "  --dataloader_pin_memory [DATALOADER_PIN_MEMORY]\n",
            "                        Whether or not to pin memory for DataLoader.\n",
            "  --skip_memory_metrics [SKIP_MEMORY_METRICS]\n",
            "                        Whether or not to skip adding of memory profiler\n",
            "                        reports to metrics.\n",
            "  --use_legacy_prediction_loop [USE_LEGACY_PREDICTION_LOOP]\n",
            "                        Whether or not to use the legacy prediction_loop in\n",
            "                        the Trainer.\n",
            "  --push_to_hub [PUSH_TO_HUB]\n",
            "                        Whether or not to upload the trained model to the\n",
            "                        model hub after training.\n",
            "  --resume_from_checkpoint RESUME_FROM_CHECKPOINT\n",
            "                        The path to a folder with a valid checkpoint for your\n",
            "                        model.\n",
            "  --mp_parameters MP_PARAMETERS\n",
            "                        Used by the SageMaker launcher to send mp-specific\n",
            "                        args. Ignored in Trainer\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}