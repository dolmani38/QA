{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled10.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dolmani38/QA/blob/main/Korean_QA_on_Wiki.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6x4Nsa-fZkw"
      },
      "source": [
        "## Building a QA System with BERT on Wikipedia\n",
        "\n",
        "https://qa.fastforwardlabs.com/pytorch/hugging%20face/wikipedia/bert/transformers/2020/05/19/Getting_Started_with_QA.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SixlAqFJfbK9"
      },
      "source": [
        "위의 내용을 한국어 QA로 변경\n",
        "\n",
        "지식 base = wiki+네이버(view,kin,news)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cKYlcdVfekg",
        "outputId": "d6de265d-e666-469a-9707-45f555a4e6ae"
      },
      "source": [
        "!pip install wikipedia==1.4.0\n",
        "!pip install sentence-transformers==1.1.1\n",
        "!pip install transformers==4.6.0"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wikipedia==1.4.0 in /usr/local/lib/python3.7/dist-packages (1.4.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from wikipedia==1.4.0) (4.6.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wikipedia==1.4.0) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia==1.4.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia==1.4.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia==1.4.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia==1.4.0) (2020.12.5)\n",
            "Requirement already satisfied: sentence-transformers==1.1.1 in /usr/local/lib/python3.7/dist-packages (1.1.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==1.1.1) (0.1.95)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==1.1.1) (1.19.5)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==1.1.1) (1.8.1+cu101)\n",
            "Requirement already satisfied: transformers<5.0.0,>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==1.1.1) (4.6.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==1.1.1) (0.9.1+cu101)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==1.1.1) (3.2.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==1.1.1) (0.22.2.post1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==1.1.1) (4.41.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==1.1.1) (1.4.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers==1.1.1) (3.7.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers==1.1.1) (3.0.12)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers==1.1.1) (0.10.2)\n",
            "Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers==1.1.1) (0.0.8)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers==1.1.1) (0.0.45)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers==1.1.1) (4.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers==1.1.1) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers==1.1.1) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers==1.1.1) (20.9)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers==1.1.1) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers==1.1.1) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers==1.1.1) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=3.1.0->sentence-transformers==1.1.1) (8.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers<5.0.0,>=3.1.0->sentence-transformers==1.1.1) (3.4.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers==1.1.1) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers==1.1.1) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers==1.1.1) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers==1.1.1) (2.10)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<5.0.0,>=3.1.0->sentence-transformers==1.1.1) (2.4.7)\n",
            "Requirement already satisfied: transformers==4.6.0 in /usr/local/lib/python3.7/dist-packages (4.6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0) (20.9)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0) (4.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0) (2.23.0)\n",
            "Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0) (0.0.8)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0) (0.0.45)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0) (1.19.5)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0) (0.10.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.6.0) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.6.0) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.6.0) (3.7.4.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.0) (8.0.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.0) (1.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtsH2Rm6VSMt"
      },
      "source": [
        "##한국어 SQUAD 모델의 사용\n",
        "https://huggingface.co/monologg/koelectra-base-v3-finetuned-korquad\n",
        "\n",
        "##영어 + 한국어 STS 모델의 사용\n",
        "\n",
        "#Extending Sentence Embeddings Models to New Languages\n",
        "\n",
        "**Available Pre-trained Models**\n",
        "\n",
        "*   **distiluse-base-multilingual-cased**: Supported languages: Arabic, Chinese, Dutch, English, French, German, Italian, Korean, Polish, Portuguese, Russian, Spanish, Turkish. Model is based on DistilBERT-multi-lingual.\n",
        "*   **xlm-r-base-en-ko-nli-ststb**: Supported languages: English, Korean. Performance on Korean STSbenchmark: 81.47\n",
        "*   **xlm-r-large-en-ko-nli-ststb**: Supported languages: English, Korean. Performance on Korean STSbenchmark: 84.05 --> 이거 사용!\n",
        "\n",
        "\n",
        "참조:https://github.com/UKPLab/sentence-transformers/blob/master/docs/pretrained-models/multilingual-models.md\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwgl-wMvf13B"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "\n",
        "SQUAD_MODEL = \"monologg/koelectra-base-v3-finetuned-korquad\"\n",
        "STS_MODEL = \"xlm-r-large-en-ko-nli-ststb\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(SQUAD_MODEL)\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(SQUAD_MODEL)\n",
        "#reader = DocumentReader(SQUAD_MODEL) \n",
        "embedder = SentenceTransformer(STS_MODEL)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Omq9ivSP-oZl"
      },
      "source": [
        "# Print iterations progress\n",
        "class ProgressBar:\n",
        "\n",
        "  def __init__(self,total=20, prefix = '', suffix = '', decimals = 1, length = 20, fill = '█', printEnd = \"\\r\"):\n",
        "    self.total = total\n",
        "    self.prefix = prefix\n",
        "    self.suffix = suffix\n",
        "    self.decimals = decimals\n",
        "    self.length = length\n",
        "    self.fill = fill\n",
        "    self.printEnd = printEnd\n",
        "    self.ite = 0\n",
        "\n",
        "  def printProgress(self,iteration, text):\n",
        "      self.ite += iteration\n",
        "      percent = (\"{0:.\" + str(self.decimals) + \"f}\").format(100 * (self.ite / float(self.total)))\n",
        "\n",
        "      filledLength = int(self.length * self.ite // self.total)\n",
        "      bar = self.fill * filledLength + '-' * (self.length - filledLength)\n",
        "      print(f'\\r{self.prefix} |{bar}| {percent}% {self.suffix}  {text}', end=\"\", flush=True)\n",
        "      # Print New Line on Complete\n",
        "      if self.ite == self.total: \n",
        "          print()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msXss3LjHNCD"
      },
      "source": [
        "class AnswerVoter:\n",
        "  def __init__(self, threshold_score=3,max_rank=5):\n",
        "    self.answes = {}\n",
        "    self.threshold_score= threshold_score\n",
        "    self.max_rank = max_rank\n",
        "\n",
        "  def add_ans(self,ans,score,src,pb):\n",
        "    key = ans.replace(' ','')\n",
        "    if score > self.threshold_score:\n",
        "      #print(' --- Candidate answer:',ans,score)\n",
        "      pb.printProgress(0,'Candidate answer:' + str(ans) + ' ' + str(score))\n",
        "      if key in self.answes:\n",
        "        self.answes[key][1] += score\n",
        "        if src in self.answes[key][2]:\n",
        "          pass\n",
        "        else:\n",
        "          self.answes[key][2].append(src)\n",
        "      else:\n",
        "        self.answes[key] = [ans,score,[src]]\n",
        "\n",
        "  def get_ans(self):\n",
        "    answer = []\n",
        "    sorted_x = sorted(self.answes.items(), key=lambda kv: kv[1][1],reverse=True)\n",
        "    for i in range(min(self.max_rank,len(sorted_x))):\n",
        "      answer.append(sorted_x[i])\n",
        "\n",
        "    return answer\n",
        "\n",
        "  def print(self):\n",
        "    answer = self.get_ans()\n",
        "    for ans in answer:\n",
        "      print('Answer:',ans[1][0], ' score:',ans[1][1], ' source:',ans[1][2])\n",
        "      #print('Answer:',ans)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8Q6ylSQ073J"
      },
      "source": [
        "import wikipedia as wiki\n",
        "import pprint as pp\n",
        "from collections import OrderedDict\n",
        "import torch\n",
        "\n",
        "class DocumentReader:\n",
        "    def __init__(self, _tokenizer, _model):\n",
        "        #self.READER_PATH = pretrained_model_name_or_path\n",
        "        self.tokenizer = _tokenizer #AutoTokenizer.from_pretrained(self.READER_PATH)\n",
        "        self.model = _model #AutoModelForQuestionAnswering.from_pretrained(self.READER_PATH)\n",
        "        self.max_len = self.model.config.max_position_embeddings\n",
        "        self.chunked = False\n",
        "\n",
        "        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "        #self.tokenizer.to(self.device)\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def tokenize(self, question, context):\n",
        "        self.inputs = self.tokenizer.encode_plus(question, context, max_length=512, truncation=True, add_special_tokens=True, return_tensors=\"pt\")\n",
        "        self.inputs.to(self.device)\n",
        "        self.input_ids = self.inputs[\"input_ids\"].tolist()[0]\n",
        "\n",
        "        if len(self.input_ids) > self.max_len:\n",
        "            self.inputs = self.chunkify()\n",
        "            self.chunked = True\n",
        "            print('input_ids:',len(self.input_ids),'max_len:',self.max_len)\n",
        "\n",
        "    def chunkify(self):\n",
        "        \"\"\" \n",
        "        Break up a long article into chunks that fit within the max token\n",
        "        requirement for that Transformer model. \n",
        "\n",
        "        Calls to BERT / RoBERTa / ALBERT require the following format:\n",
        "        [CLS] question tokens [SEP] context tokens [SEP].\n",
        "        \"\"\"\n",
        "\n",
        "        # create question mask based on token_type_ids\n",
        "        # value is 0 for question tokens, 1 for context tokens\n",
        "        qmask = self.inputs['token_type_ids'].lt(1)\n",
        "        qt = torch.masked_select(self.inputs['input_ids'], qmask)\n",
        "        chunk_size = self.max_len - qt.size()[0] - 1 # the \"-1\" accounts for\n",
        "        # having to add an ending [SEP] token to the end\n",
        "\n",
        "        # create a dict of dicts; each sub-dict mimics the structure of pre-chunked model input\n",
        "        chunked_input = OrderedDict()\n",
        "        for k,v in self.inputs.items():\n",
        "            q = torch.masked_select(v, qmask)\n",
        "            c = torch.masked_select(v, ~qmask)\n",
        "            chunks = torch.split(c, chunk_size)\n",
        "            \n",
        "            for i, chunk in enumerate(chunks):\n",
        "                if i not in chunked_input:\n",
        "                    chunked_input[i] = {}\n",
        "\n",
        "                thing = torch.cat((q, chunk))\n",
        "                if i != len(chunks)-1:\n",
        "                    if k == 'input_ids':\n",
        "                        thing = torch.cat((thing, torch.tensor([102])))\n",
        "                    else:\n",
        "                        thing = torch.cat((thing, torch.tensor([1])))\n",
        "\n",
        "                chunked_input[i][k] = torch.unsqueeze(thing, dim=0)\n",
        "        return chunked_input.to(self.device)\n",
        "\n",
        "    def get_answer(self,answer,src,pb):\n",
        "        if self.chunked:\n",
        "            \n",
        "            for k, chunk in self.inputs.items():\n",
        "                #chunk.to(self.device)\n",
        "                outputs = self.model(**chunk)\n",
        "\n",
        "                answer_start = torch.argmax(outputs[0])\n",
        "                answer_end = torch.argmax(outputs[1]) + 1\n",
        "                ans = self.convert_ids_to_string(chunk['input_ids'][0][answer_start:answer_end])\n",
        "                if ans.startswith(('[CLS]','[SEP]',' ','°')) or ans=='':\n",
        "                    #raise Exception('No Answer')\n",
        "                    pass\n",
        "                else:\n",
        "                    score = float(torch.max(outputs[0]))\n",
        "                    #print(ans,score)\n",
        "                    answer.add_ans(ans,score,src,pb)\n",
        "                    #answer += ans + ', '\n",
        "                    #print(ans,torch.max(answer_start_scores),torch.max(answer_end_scores))\n",
        "                    #break\n",
        "        else:\n",
        "            outputs = self.model(**self.inputs)\n",
        "\n",
        "            #print(outputs[0])\n",
        "\n",
        "            answer_start = torch.argmax(outputs[0])  # get the most likely beginning of answer with the argmax of the score\n",
        "            answer_end = torch.argmax(outputs[1]) + 1  # get the most likely end of answer with the argmax of the score\n",
        "        \n",
        "            ans = self.convert_ids_to_string(self.inputs['input_ids'][0][\n",
        "                                              answer_start:answer_end])\n",
        "            \n",
        "            if ans in ['',' ','  ']:\n",
        "              pass\n",
        "            else:\n",
        "              score = float(torch.max(outputs[0]))\n",
        "              #print(ans,score)\n",
        "              answer.add_ans(ans,score,src,pb)\n",
        "        #if len(answer) == 0:\n",
        "        #  raise Error(\"No Answer\") \n",
        "        return answer\n",
        "        \n",
        "    def convert_ids_to_string(self, input_ids):\n",
        "        return self.tokenizer.convert_tokens_to_string(self.tokenizer.convert_ids_to_tokens(input_ids))"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tD9xyTjf_AD"
      },
      "source": [
        "import sys\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "import wikipedia as wiki\n",
        "import pprint as pp\n",
        "from collections import OrderedDict\n",
        "import scipy\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "class Korean_QA_on_Wiki:\n",
        "  def __init__(self, document_reader,sentence_embedder):\n",
        "    self.reader = document_reader\n",
        "    self.embedder = sentence_embedder\n",
        "    wiki.set_lang('ko')\n",
        "\n",
        "  def __search_from_wiki(self,question,max_rank):\n",
        "    results = wiki.search(question,results=max_rank)\n",
        "    contents = []\n",
        "    for result in results:\n",
        "      try:\n",
        "        page = wiki.page(result)\n",
        "        #print(f\"Top wiki result: {page}\")\n",
        "        text = page.content\n",
        "        contents.append((text,page))\n",
        "      except Exception as ex:\n",
        "        print(ex)\n",
        "    return contents\n",
        "\n",
        "  def __search_from_naver(self,question,max_rank):\n",
        "    contents = []\n",
        "    url = 'https://search.naver.com/search.naver'\n",
        "    for w in ['view','kin','news','kdic']:\n",
        "      params = {'query': question,'where': w,}\n",
        "      response = requests.get(url, params=params)\n",
        "      html = response.text\n",
        "      #뷰티풀소프의 인자값 지정\n",
        "      soup = BeautifulSoup(html, 'html.parser')\n",
        "      #쪼개기\n",
        "      #title_list = soup.find_all('a', href=True)\n",
        "      title_list = soup.select('.api_txt_lines')\n",
        "      #print(title_list)\n",
        "      tmp = []\n",
        "      for tag in title_list:\n",
        "        tmp.append(tag.text)\n",
        "      contents.append((''.join(tmp),url + '?where='+w))\n",
        "      tmp.clear()\n",
        "    #print(contents)      \n",
        "    return contents\n",
        "\n",
        "\n",
        "\n",
        "  def question(self, questions, max_rank = 5):\n",
        "    answers = {}\n",
        "    for question in questions:\n",
        "        print(f\"Question: {question}\")\n",
        "        \n",
        "        contents = []\n",
        "        contents.extend(self.__search_from_naver(question,max_rank))\n",
        "        contents.extend(self.__search_from_wiki(question,max_rank))\n",
        "        src_count = len(contents)\n",
        "        pb = ProgressBar(total=src_count+1,prefix='Searching answers...')\n",
        "        pb.printProgress(0,str(src_count)+' of sources')\n",
        "        #print('-- Source count : ', len(contents))\n",
        "        answer = AnswerVoter()\n",
        "        if self.embedder is None:\n",
        "            for context, src in contents:\n",
        "                #text = contents[idx][0]\n",
        "                pb.printProgress(+1,src)\n",
        "                #print('-- source :', contents[idx][1])\n",
        "                try:\n",
        "                    self.reader.tokenize(question, context)\n",
        "                    self.reader.get_answer(answer,src,pb)\n",
        "                    #answer.add_src(contents[idx][1])\n",
        "                    #t = (self.reader.get_answer(answer),contents[idx][1])\n",
        "                    #print(f\"Answer: {t[0]}\", f\" from {t[1]}\")\n",
        "                    #answer_list.append(t)\n",
        "                except Exception as ex:\n",
        "                    #pb.printProgress(0,sys.exc_info()[0])\n",
        "                    print(ex,sys.exc_info())\n",
        "                    #pass    \n",
        "        else:\n",
        "            corpus_embeddings = self.embedder.encode([a for (a,b) in contents],show_progress_bar=False) \n",
        "            query_embeddings = self.embedder.encode([question])\n",
        "            distances = scipy.spatial.distance.cdist(query_embeddings, corpus_embeddings, \"cosine\")[0]\n",
        "\n",
        "            results = zip(range(len(distances)), distances)\n",
        "            results = sorted(results, key=lambda x: x[1])\n",
        "            \n",
        "\n",
        "            for idx, distance in results:\n",
        "                context = contents[idx][0]\n",
        "                pb.printProgress(+1,contents[idx][1])\n",
        "                #print('-- source :', contents[idx][1])\n",
        "                try:\n",
        "                    self.reader.tokenize(question, context)\n",
        "                    self.reader.get_answer(answer,contents[idx][1],pb)\n",
        "                    #answer.add_src(contents[idx][1])\n",
        "                    #t = (self.reader.get_answer(answer),contents[idx][1])\n",
        "                    #print(f\"Answer: {t[0]}\", f\" from {t[1]}\")\n",
        "                    #answer_list.append(t)\n",
        "                    \n",
        "                except Exception as ex:\n",
        "                    #pb.printProgress(0,sys.exc_info()[0])\n",
        "                    print(ex,sys.exc_info())\n",
        "                    #pass    \n",
        "        answers[question] = answer.get_ans()\n",
        "        pb.printProgress(+1,\"완료\")\n",
        "        answer.print()\n",
        "        print(' ')\n",
        "    return answers\n"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5Uu_CZhgFVk"
      },
      "source": [
        "kqaw = Korean_QA_on_Wiki(DocumentReader(tokenizer,model), embedder)"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yJytmp2_E9o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ee14b7b-ae4f-4c51-f9f9-2b9ae6a62ab2"
      },
      "source": [
        "answers = kqaw.question([\"아브라함은 자식이 몇명인가?\"])"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question: 아브라함은 자식이 몇명인가?\n",
            "Searching answers... |████████████████████| 100.0%   완료\n",
            "Answer: 13명  score: 10.882498741149902  source: [<WikipediaPage '소저너 트루스'>]\n",
            "Answer: 테월데  score: 9.216580390930176  source: [<WikipediaPage '에티오피아 이름'>]\n",
            "Answer: 6명  score: 6.823949337005615  source: [<WikipediaPage '아브라함'>]\n",
            "Answer: 45명 40명  score: 6.245438098907471  source: ['https://search.naver.com/search.naver?where=view']\n",
            "Answer: 몇 명  score: 3.558769941329956  source: ['https://search.naver.com/search.naver?where=news']\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vcd67iu3v2L",
        "outputId": "e5f2d3f6-6d8d-41fb-86bf-6f4c529af68d"
      },
      "source": [
        "answers = kqaw.question([\"베트남에서 가장 인기있는 관광지는 어디인가요?\"])"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question: 베트남에서 가장 인기있는 관광지는 어디인가요?\n",
            "Searching answers... |████████████████████| 100.0%   완료\n",
            "Answer: 냐짱  score: 9.128152847290039  source: [<WikipediaPage '냐짱'>]\n",
            "Answer: 축구  score: 8.47502613067627  source: [<WikipediaPage '태국의 문화'>]\n",
            "Answer: 다낭  score: 8.257136344909668  source: ['https://search.naver.com/search.naver?where=view']\n",
            "Answer: 하롱 베이  score: 6.724521160125732  source: ['https://search.naver.com/search.naver?where=kin']\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrVzd_ciOLMt",
        "outputId": "c1714659-a31b-41db-9c4f-fb6ea7137529",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "answers = kqaw.question([\"베트남 다낭에서 맛집 한국식당 이름은?\"])"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question: 베트남 다낭에서 맛집 한국식당 이름은?\n",
            "Searching answers... |████████████████████| 100.0%   완료\n",
            "Answer: 홍대  score: 7.922461986541748  source: ['https://search.naver.com/search.naver?where=kin']\n",
            "Answer: 배베식당  score: 4.599730014801025  source: ['https://search.naver.com/search.naver?where=view']\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIyQ1Wr6QI3n",
        "outputId": "9d183f45-b648-4d32-e209-621921485269",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "answers = kqaw.question([\"코로나19는 언제 종식되는가?\"])"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question: 코로나19는 언제 종식되는가?\n",
            "Searching answers... |████████████████████| 100.0%   완료\n",
            "Answer: 2021년  score: 9.98672866821289  source: ['https://search.naver.com/search.naver?where=view']\n",
            "Answer: 6월 15일  score: 9.97450065612793  source: [<WikipediaPage '뉴질랜드의 코로나19 범유행'>]\n",
            "Answer: 2021년 9월 중순  score: 6.855931758880615  source: ['https://search.naver.com/search.naver?where=kin']\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyzCu02dQo9u",
        "outputId": "35190344-99f4-4f94-840c-beddff345422",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "answers = kqaw.question([\"아시아나IDT의 대표이사는 누구인가?\"])"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question: 아시아나IDT의 대표이사는 누구인가?\n",
            "Searching answers... |████████████████████| 100.0%   완료\n",
            "Answer: 박세창  score: 10.907037734985352  source: ['https://search.naver.com/search.naver?where=view']\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9vfQ-3OQNz7",
        "outputId": "1dcbab46-78de-491a-bbf6-9b0292652d9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "answers = kqaw.question([\"아시아나항공은 어디에 매각되는가?\"])"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question: 아시아나항공은 어디에 매각되는가?\n",
            "Searching answers... |████████████████████| 100.0%   완료\n",
            "Answer: 금호아시아나그룹  score: 8.167031288146973  source: ['https://search.naver.com/search.naver?where=kin']\n",
            "Answer: 대한항공  score: 6.548533916473389  source: ['https://search.naver.com/search.naver?where=news']\n",
            "Answer: HDC현대산업개발  score: 6.190684795379639  source: ['https://search.naver.com/search.naver?where=kdic']\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCnEX6uzgHn5",
        "outputId": "866393fd-44ba-4b87-fe85-49215a47ea89"
      },
      "source": [
        "answers = kqaw.question([\"북한에서 실질적인 권력자는 누구인가?\",\n",
        "                           \"세계에서 가장 넓은 호수는?\",\n",
        "                           \"오로라가 가장 잘 보이는 곳은?\",\n",
        "                           \"심장이 죄어오듯이 아프면 의심되는 병은 무엇인가?\",\n",
        "                           \"항문에서 피가 나는 병은 무엇인가?\",\n",
        "                           \"김재규는 박정희를 왜 죽였는가?\",\n",
        "                           \"케네디를 죽인 암살범은 누구인가?\",\n",
        "                           \"술 취하지 않는 방법은?\",\n",
        "                           \"사람을 사랑해서 생기는 병은?\",\n",
        "                           \"부모는 자식을 왜 사랑하는가?\",\n",
        "                           \"나의 와이프는 나를 사랑하는가?\",\n",
        "                           \"신은 존재 하는가?\",\n",
        "                           \"사람의 인생에서 가장 소중한 것은 무엇인가?\",\n",
        "                           \"바람난 여자는 다시 돌아올 수 있는가?\",\n",
        "                           \"위가 쓰리고 아플 때 어떤 약을 복용해야 하는가?\",\n",
        "                           \"눈알이 빠지면 어떻게 되는가?\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question: 북한에서 실질적인 권력자는 누구인가?\n",
            "Searching answers... |████████████████████| 100.0%   완료\n",
            "Answer: 김일성  score: 37.01249718666077  source: ['https://search.naver.com/search.naver?where=kdic', <WikipediaPage '김일성'>]\n",
            "Answer: 김정은  score: 11.77053427696228  source: ['https://search.naver.com/search.naver?where=news', 'https://search.naver.com/search.naver?where=view']\n",
            " \n",
            "Question: 세계에서 가장 넓은 호수는?\n",
            "Searching answers... |████████████████████| 100.0%   완료\n",
            "Answer: 카스피해  score: 29.744450569152832  source: ['https://search.naver.com/search.naver?where=kin', 'https://search.naver.com/search.naver?where=kdic', <WikipediaPage '호수'>]\n",
            "Answer: 티티카카 호  score: 11.382185935974121  source: [<WikipediaPage '남아메리카'>]\n",
            " \n",
            "Question: 오로라가 가장 잘 보이는 곳은?\n",
            "Searching answers... |████████████████████| 100.0%   완료\n",
            "Answer: 계란형 지대  score: 32.557658195495605  source: ['https://search.naver.com/search.naver?where=kin', 'https://search.naver.com/search.naver?where=kdic']\n",
            "Answer: 남극및 북극 양극지방  score: 19.048751831054688  source: ['https://search.naver.com/search.naver?where=kdic', <WikipediaPage '오로라'>]\n",
            " \n",
            "Question: 심장이 죄어오듯이 아프면 의심되는 병은 무엇인가?\n",
            "Searching answers... |████████████████████| 100.0%   완료\n",
            "Answer: 조현병  score: 12.697730541229248  source: [<WikipediaPage '조현병'>]\n",
            "Answer: 류마티스 관절염  score: 10.58558177947998  source: ['https://search.naver.com/search.naver?where=view']\n",
            " \n",
            "Question: 항문에서 피가 나는 병은 무엇인가?\n",
            "Searching answers... |████████████████████| 100.0%   완료\n",
            "Answer: 치질 , 치열 , 치루  score: 11.42421817779541  source: ['https://search.naver.com/search.naver?where=view']\n",
            "Answer: 이유항문  score: 9.650527000427246  source: ['https://search.naver.com/search.naver?where=view']\n",
            " \n",
            "Question: 김재규는 박정희를 왜 죽였는가?\n",
            "Searching answers... |████████████████████| 100.0%   완료\n",
            "Answer: 10 · 26 사건  score: 10.449921607971191  source: ['https://search.naver.com/search.naver?where=kdic']\n",
            "Answer: 미국과의 관계  score: 10.000889778137207  source: [<WikipediaPage '닉슨 독트린'>]\n",
            " \n",
            "Question: 케네디를 죽인 암살범은 누구인가?\n",
            "Searching answers... |████████████████████| 100.0%   완료\n",
            "Answer: 리 하비 오스월드  score: 45.29909944534302  source: ['https://search.naver.com/search.naver?where=view', <WikipediaPage '링컨과 케네디의 공통점'>, 'https://search.naver.com/search.naver?where=kdic']\n",
            "Answer: 오스왈드  score: 23.996009349822998  source: ['https://search.naver.com/search.naver?where=kin', 'https://search.naver.com/search.naver?where=view']\n",
            " \n",
            "Question: 술 취하지 않는 방법은?\n",
            "Searching answers... |████████████████████| 100.0%   완료\n",
            "Answer: 법을 행하는 수단  score: 10.731337547302246  source: [<WikipediaPage '법가'>]\n",
            "Answer: 모닝플러스  score: 10.689274787902832  source: ['https://search.naver.com/search.naver?where=kin']\n",
            " \n",
            "Question: 사람을 사랑해서 생기는 병은?\n",
            "Searching answers... |████████████████████| 100.0%   완료\n",
            "Answer: 말티즈 심장병  score: 8.629045486450195  source: ['https://search.naver.com/search.naver?where=kin']\n",
            "Answer: 상사병  score: 7.885115146636963  source: ['https://search.naver.com/search.naver?where=kin']\n",
            " \n",
            "Question: 부모는 자식을 왜 사랑하는가?\n",
            "Searching answers... |████████████████████| 100.0%   완료\n",
            "Answer: 자애로워야 하고  score: 6.522762775421143  source: ['https://search.naver.com/search.naver?where=kdic']\n",
            "Answer: 포세이돈  score: 6.231316089630127  source: [<WikipediaPage '포세이돈'>]\n",
            " \n",
            "Question: 나의 와이프는 나를 사랑하는가?\n",
            "Searching answers... |████████████████████| 100.0%   완료\n",
            " \n",
            "Question: 신은 존재 하는가?\n",
            "Searching answers... |████████████████████| 100.0%   완료\n",
            "Answer: 자연적 혹은 초자연적 존재  score: 4.550796031951904  source: [<WikipediaPage '신'>]\n",
            " \n",
            "Question: 사람의 인생에서 가장 소중한 것은 무엇인가?\n",
            "Searching answers... |████████████████████| 100.0%   완료\n",
            "Answer: 나 자신  score: 10.68074893951416  source: ['https://search.naver.com/search.naver?where=kin']\n",
            "Answer: 가족  score: 6.324282646179199  source: ['https://search.naver.com/search.naver?where=kin']\n",
            " \n",
            "Question: 바람난 여자는 다시 돌아올 수 있는가?\n",
            "Searching answers... |████████████████████| 100.0%   완료\n",
            "Answer: 홀포도  score: 3.7049643993377686  source: ['https://search.naver.com/search.naver?where=view']\n",
            " \n",
            "Question: 위가 쓰리고 아플 때 어떤 약을 복용해야 하는가?\n",
            "Searching answers... |████████████████████| 100.0%   완료\n",
            "Answer: 양 약  score: 10.814971923828125  source: ['https://search.naver.com/search.naver?where=kin']\n",
            "Answer: 매스틱  score: 10.228936195373535  source: ['https://search.naver.com/search.naver?where=view']\n",
            " \n",
            "Question: 눈알이 빠지면 어떻게 되는가?\n",
            "Searching answers... |████████████████████| 100.0%   완료\n",
            "Answer: 도탄  score: 6.598306179046631  source: [<WikipediaPage '렐파첸'>]\n",
            "Answer: 애꾸눈  score: 6.041463375091553  source: [<WikipediaPage '오딘'>]\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6qwf2zGp9N3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5cc9ddb-44b2-4a23-be7b-8bc043fde1a2"
      },
      "source": [
        "answers = kqaw.question([\"아시아나항공은 어디에 매각될 것인가?\",\n",
        "                         \"박세창은 어느 회사의 사장인가?\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question: 아시아나항공은 어디에 매각될 것인가?\n",
            "Searching answers... |████████████████████| 100.0%   완료\n",
            "Answer: HDC현대산업개발  score: 15.071530818939209  source: ['https://search.naver.com/search.naver?where=kdic']\n",
            "Answer: 한진상사  score: 11.714262008666992  source: ['https://search.naver.com/search.naver?where=kdic']\n",
            "Answer: 산업은행  score: 5.711127758026123  source: ['https://search.naver.com/search.naver?where=news']\n",
            "Answer: LG그룹  score: 4.931703090667725  source: [<WikipediaPage '문화방송'>]\n",
            " \n",
            "Question: 박세창은 어느 회사의 사장인가?\n",
            "Searching answers... |████████████████████| 100.0%   완료\n",
            "Answer: 아시아나IDT  score: 61.1358003616333  source: ['https://search.naver.com/search.naver?where=view', 'https://search.naver.com/search.naver?where=news']\n",
            "Answer: 금호타이어  score: 16.81703233718872  source: [<WikipediaPage '금호석유화학'>]\n",
            "Answer: 금호아시아나그룹  score: 9.963479042053223  source: ['https://search.naver.com/search.naver?where=news']\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRM7VQaRqzt0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21f85ad2-efce-40b0-945d-5c23b9ad329b"
      },
      "source": [
        "answers = kqaw.question([\"아시아나항공 사장의 이름은?\",\n",
        "                         \"금호건설의 사장은 누구인가?\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question: 아시아나항공 사장의 이름은?\n",
            "Searching answers... |████████████████████| 100.0%   완료\n",
            "Answer: 한창수  score: 20.9244384765625  source: ['https://search.naver.com/search.naver?where=view', 'https://search.naver.com/search.naver?where=news']\n",
            "Answer: 윤영두  score: 11.341591835021973  source: ['https://search.naver.com/search.naver?where=kin']\n",
            " \n",
            "Question: 금호건설의 사장은 누구인가?\n",
            "Searching answers... |████████████████████| 100.0%   완료\n",
            "Answer: 박삼구  score: 10.64574146270752  source: ['https://search.naver.com/search.naver?where=kdic']\n",
            "Answer: 이서형  score: 10.147002220153809  source: ['https://search.naver.com/search.naver?where=view']\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwq_CRCDG-4S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7caec894-1ebe-49e0-9788-a1c095e2e907"
      },
      "source": [
        "# 한진중공업 매각 우선협상대상자\n",
        "answers = kqaw.question([\"한진중공업 매각 우선협상대상자는 어디인가?\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question: 한진중공업 매각 우선협상대상자는 어디인가?\n",
            "Searching answers... |████████████████████| 100.0%   완료\n",
            "Answer: 동부건설  score: 17.863832473754883  source: ['https://search.naver.com/search.naver?where=view']\n",
            "Answer: 동부건설 컨소  score: 11.456985473632812  source: ['https://search.naver.com/search.naver?where=view']\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qIRQgHSrZo8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "311d52fc-b810-4c2e-f9af-94504c8923dc"
      },
      "source": [
        "answers = kqaw.question([\"아파트 값은 계속 오를 것인가?\",\n",
        "                         \"코로나는 언제 종식 될 것인가?\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question: 아파트 값은 계속 오를 것인가?\n",
            "Searching answers... |████████████████████| 100.0%   완료\n",
            " \n",
            "Question: 코로나는 언제 종식 될 것인가?\n",
            "Searching answers... |████████████████████| 100.0%   완료\n",
            "Answer: 2020년  score: 7.0133490562438965  source: ['https://search.naver.com/search.naver?where=kdic']\n",
            "Answer: 2021년 9월 중순  score: 4.102957248687744  source: ['https://search.naver.com/search.naver?where=kin']\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ARRfo7-sgSJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58bfc2d6-2006-466a-c411-bc18e4164d27"
      },
      "source": [
        "answers = kqaw.question([\"단백질의 화학식 구성은 어떻게 되는가?\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question: 단백질의 화학식 구성은 어떻게 되는가?\n",
            "Searching answers... |████████████████████| 100.0%   완료\n",
            "Answer: HO2CCH2NH2  score: 10.998641014099121  source: [<WikipediaPage '글라이신'>]\n",
            "Answer: 글리코실화  score: 10.18725872039795  source: [<WikipediaPage '세린'>]\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbGFACCktHam",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b25f2cb-7405-4f0d-c199-3683e59a798d"
      },
      "source": [
        "answers = kqaw.question([\"우리나라 특허의 권리보장 기간은 몇년인가?\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question: 우리나라 특허의 권리보장 기간은 몇년인가?\n",
            "Searching answers... |████████████████████| 100.0%   완료\n",
            "Answer: 20년  score: 31.303051948547363  source: ['https://search.naver.com/search.naver?where=view', 'https://search.naver.com/search.naver?where=kin', 'https://search.naver.com/search.naver?where=kdic']\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Srh6Zf5TtgcQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8492a66-8d37-4aae-bd14-718d6f9e9c1e"
      },
      "source": [
        "answers = kqaw.question([\"발열 마른기침 피로감 등의 증상을 보이면 어떤 병이 의심되는가?\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question: 발열 마른기침 피로감 등의 증상을 보이면 어떤 병이 의심되는가?\n",
            "Searching answers... |████████████████████| 100.0%   완료\n",
            "Answer: 코로나 19  score: 15.271196842193604  source: ['https://search.naver.com/search.naver?where=kin', 'https://search.naver.com/search.naver?where=view']\n",
            "Answer: 광견병  score: 11.497452735900879  source: ['https://search.naver.com/search.naver?where=kdic']\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjYnN2Ggt4ER",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "220eaf98-0493-4a0e-ec62-4fcf267f5532"
      },
      "source": [
        "answers = kqaw.question([\"흉부통증과 호흡곤란, 쉰목소리, 가끔 피가 썩인 가래도 있습니다. 어떤 병일까요?\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question: 흉부통증과 호흡곤란, 쉰목소리, 가끔 피가 썩인 가래도 있습니다. 어떤 병일까요?\n",
            "Searching answers... |████████████████████| 100.0%   완료\n",
            "Answer: 폐암  score: 12.317249298095703  source: ['https://search.naver.com/search.naver?where=kin', 'https://search.naver.com/search.naver?where=view']\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhgIS5vtubVs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58eea76e-f283-47e0-df1c-0b7ba0963d21"
      },
      "source": [
        "answers = kqaw.question([\"똥을 싸고 나면 휴지에 피가 뭍습니다. 의심되는 병은 무엇인가요?\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question: 똥을 싸고 나면 휴지에 피가 뭍습니다. 의심되는 병은 무엇인가요?\n",
            "Searching answers... |████████████████████| 100.0%   완료\n",
            "Answer: 치열  score: 16.448171615600586  source: ['https://search.naver.com/search.naver?where=kin']\n",
            "Answer: 궤양성 대장염  score: 9.296835899353027  source: ['https://search.naver.com/search.naver?where=kin']\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1clXBpGu0st",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "929f24b1-a024-4fb2-9cd0-ac4f30f1e3b8"
      },
      "source": [
        "answers = kqaw.question([\"신경망 알고리즘의 활성화 함수에는 어떤 것이 있나요?\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question: 신경망 알고리즘의 activation function에는 어떤 것이 있나요?\n",
            "Searching answers... |████████████████████| 100.0%   완료\n",
            "Answer: 활성화 함수  score: 7.86516809463501  source: ['https://search.naver.com/search.naver?where=view']\n",
            "Answer: 활성함수  score: 6.889105796813965  source: ['https://search.naver.com/search.naver?where=view']\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIBSD9HcvITx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82d1811d-d004-42ef-c0a0-dd08c904f74c"
      },
      "source": [
        "answers = kqaw.question([\"현존하는 인공지능 중 가장 성능이 우수한 것은 무엇입니까?\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question: 현존하는 인공지능 중 가장 성능이 우수한 것은 무엇입니까?\n",
            "Searching answers... |████████████████████| 100.0%   완료\n",
            "Answer: Global Hawk  score: 17.53031873703003  source: ['https://search.naver.com/search.naver?where=kdic', <WikipediaPage '무인 항공기'>]\n",
            "Answer: 슈퍼 컴퓨터  score: 10.809731483459473  source: ['https://search.naver.com/search.naver?where=kin']\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVxLa0T0vhAV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d05e2332-a512-4d1d-9ebe-40928754b929"
      },
      "source": [
        "answers = kqaw.question([\"이세돌을 이긴 것은 무엇입니까?\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question: 이세돌을 이긴 것은 무엇입니까?\n",
            "Searching answers... |████████████████████| 100.0%   완료\n",
            "Answer: 알파고  score: 60.73257637023926  source: ['https://search.naver.com/search.naver?where=view', <WikipediaPage '인공지능'>, 'https://search.naver.com/search.naver?where=kin', <WikipediaPage '알파고'>]\n",
            "Answer: 깔끔함  score: 8.735248565673828  source: [<WikipediaPage '인공지능'>]\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ay5GzjELwAG-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fed3be0c-ef2b-49d0-d8f0-ae8633459818"
      },
      "source": [
        "answers = kqaw.question([\"피타고라스는 어느 나라 사람인가?\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question: 피타고라스는 어느 나라 사람인가?\n",
            "Searching answers... |████████████████████| 100.0%   완료\n",
            "Answer: 그리스  score: 40.09191274642944  source: ['https://search.naver.com/search.naver?where=kin', 'https://search.naver.com/search.naver?where=view', 'https://search.naver.com/search.naver?where=news', 'https://search.naver.com/search.naver?where=kdic', <WikipediaPage '수학자'>]\n",
            "Answer: 이집트  score: 6.741414546966553  source: ['https://search.naver.com/search.naver?where=view']\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h43Zxml-wSWs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e21b4e09-93f3-4105-a3ae-a48bb193e25a"
      },
      "source": [
        "answers = kqaw.question([\"아이즈원 멤버 인원수는?\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question: 아이즈원 멤버 인원수는?\n",
            "Searching answers... |████████████████████| 100.0%   완료\n",
            "Answer: 12명  score: 11.964734077453613  source: ['https://search.naver.com/search.naver?where=news']\n",
            "Answer: 4명  score: 11.074978828430176  source: ['https://search.naver.com/search.naver?where=kin']\n",
            "Answer: 8명  score: 10.20055103302002  source: ['https://search.naver.com/search.naver?where=kin']\n",
            "Answer: 300명  score: 6.7700066566467285  source: ['https://search.naver.com/search.naver?where=kdic']\n",
            "Answer: 12  score: 6.522476673126221  source: ['https://search.naver.com/search.naver?where=view']\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L96Ik9MawxRd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8208db73-2f98-463a-e3b4-d6dd7dadf7c3"
      },
      "source": [
        "answers = kqaw.question([\"트와이스 중에 가장 인기 있는 사람은?\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question: 트와이스 중에 가장 인기 있는 사람은?\n",
            "Searching answers... |████████████████████| 100.0%   완료\n",
            "Answer: 지효  score: 16.386046409606934  source: ['https://search.naver.com/search.naver?where=news', <WikipediaPage '청하 (가수)'>]\n",
            "Answer: 가는 세월  score: 10.251775741577148  source: ['https://search.naver.com/search.naver?where=kdic']\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ni_mSMGTxnFi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88c276d6-24f7-4f37-a108-c33d7cd2cb82"
      },
      "source": [
        "answers = kqaw.question([\"벤츠 자동차가 처음 발명된 년도는?\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question: 벤츠 자동차가 처음 발명된 년도는?\n",
            "Searching answers... |████████████████████| 100.0%   완료\n",
            "Answer: 1886년  score: 33.59215593338013  source: ['https://search.naver.com/search.naver?where=kin', 'https://search.naver.com/search.naver?where=kdic', <WikipediaPage '자동차'>, <WikipediaPage '만하임'>]\n",
            "Answer: 1883년  score: 11.766658782958984  source: ['https://search.naver.com/search.naver?where=kin']\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfivYMWWydC6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b064db96-4222-4abc-f40f-0aa98a4a9497"
      },
      "source": [
        "answers = kqaw.question([\"교통사고 대비를 위해 들어야 하는 보험은 무엇인가?\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question: 교통사고 대비를 위해 들어야 하는 보험은 무엇인가?\n",
            "Searching answers... |████████████████████| 100.0%   완료\n",
            "Answer: 자동차보험  score: 10.861504554748535  source: ['https://search.naver.com/search.naver?where=news']\n",
            "Answer: 운전자보험  score: 10.007701873779297  source: ['https://search.naver.com/search.naver?where=view', 'https://search.naver.com/search.naver?where=kin']\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZojgWg1zQVz"
      },
      "source": [
        "answers = kqaw.question([\"너의 이름은 무엇이니?\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jRkKp0A0DIN"
      },
      "source": [
        "answers = kqaw.question([\"너는 남자니 여자니?\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDssSlN7hOSB"
      },
      "source": [
        "#KorQuad2.0 학습.\n",
        "\n",
        "https://github.com/huggingface/datasets\n",
        "\n",
        "https://huggingface.co/transformers/custom_datasets.html#qa-squad\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68_TJLJC2toq",
        "outputId": "5719b110-dffa-4f3a-d005-7b2b7fd3624a"
      },
      "source": [
        "\n",
        "if True:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzSMzhExV6A9",
        "outputId": "a5bbea09-f17a-4dfa-d8c2-29399a41e288"
      },
      "source": [
        "!pip install datasets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (1.6.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.7/dist-packages (from datasets) (2021.4.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.11.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.3)\n",
            "Requirement already satisfied: huggingface-hub<0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.0.8)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (20.9)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets) (3.10.1)\n",
            "Requirement already satisfied: pyarrow>=1.0.0<4.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.41.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 712
        },
        "id": "o2UloPBTV0UY",
        "outputId": "1530c4ac-7f38-40c9-93f5-646ccd29977f"
      },
      "source": [
        "from datasets import list_datasets, load_dataset, list_metrics, load_metric, load_from_disk\n",
        "\n",
        "# Print all the available datasets\n",
        "print(list_datasets())\n",
        "\n",
        "# Load a dataset and print the first example in the training set\n",
        "squad_dataset = load_dataset('squad_kor_v2',cache_dir ='/content/drive/MyDrive/korQuAD2.1/dataset')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['acronym_identification', 'ade_corpus_v2', 'adversarial_qa', 'aeslc', 'afrikaans_ner_corpus', 'ag_news', 'ai2_arc', 'air_dialogue', 'ajgt_twitter_ar', 'allegro_reviews', 'allocine', 'alt', 'amazon_polarity', 'amazon_reviews_multi', 'amazon_us_reviews', 'ambig_qa', 'amttl', 'anli', 'app_reviews', 'aqua_rat', 'aquamuse', 'ar_cov19', 'ar_res_reviews', 'ar_sarcasm', 'arabic_billion_words', 'arabic_pos_dialect', 'arabic_speech_corpus', 'arcd', 'arsentd_lev', 'art', 'arxiv_dataset', 'ascent_kb', 'aslg_pc12', 'asnq', 'asset', 'assin', 'assin2', 'atomic', 'autshumato', 'babi_qa', 'banking77', 'bbaw_egyptian', 'bbc_hindi_nli', 'bc2gm_corpus', 'best2009', 'bianet', 'bible_para', 'big_patent', 'billsum', 'bing_coronavirus_query_set', 'biomrc', 'blended_skill_talk', 'blimp', 'blog_authorship_corpus', 'bn_hate_speech', 'bookcorpus', 'bookcorpusopen', 'boolq', 'bprec', 'break_data', 'brwac', 'bsd_ja_en', 'bswac', 'c3', 'c4', 'cail2018', 'caner', 'capes', 'catalonia_independence', 'cawac', 'cbt', 'cc100', 'cc_news', 'ccaligned_multilingual', 'cdsc', 'cdt', 'cfq', 'chr_en', 'cifar10', 'cifar100', 'circa', 'civil_comments', 'clickbait_news_bg', 'climate_fever', 'clinc_oos', 'clue', 'cmrc2018', 'cnn_dailymail', 'coached_conv_pref', 'coarse_discourse', 'codah', 'code_search_net', 'com_qa', 'common_gen', 'common_voice', 'commonsense_qa', 'compguesswhat', 'conceptnet5', 'conll2000', 'conll2002', 'conll2003', 'conllpp', 'conv_ai', 'conv_ai_2', 'conv_ai_3', 'coqa', 'cord19', 'cornell_movie_dialog', 'cos_e', 'cosmos_qa', 'counter', 'covid_qa_castorini', 'covid_qa_deepset', 'covid_qa_ucsd', 'covid_tweets_japanese', 'covost2', 'craigslist_bargains', 'crawl_domain', 'crd3', 'crime_and_punish', 'crows_pairs', 'cryptonite', 'cs_restaurants', 'cuad', 'curiosity_dialogs', 'daily_dialog', 'dane', 'danish_political_comments', 'dart', 'datacommons_factcheck', 'dbpedia_14', 'dbrd', 'deal_or_no_dialog', 'definite_pronoun_resolution', 'dengue_filipino', 'dialog_re', 'diplomacy_detection', 'disaster_response_messages', 'discofuse', 'discovery', 'doc2dial', 'docred', 'doqa', 'dream', 'drop', 'duorc', 'dutch_social', 'dyk', 'e2e_nlg', 'e2e_nlg_cleaned', 'ecb', 'ecthr_cases', 'ehealth_kd', 'eitb_parcc', 'eli5', 'emea', 'emo', 'emotion', 'emotone_ar', 'empathetic_dialogues', 'enriched_web_nlg', 'eraser_multi_rc', 'esnli', 'eth_py150_open', 'ethos', 'eu_regulatory_ir', 'eurlex', 'euronews', 'europa_eac_tm', 'europa_ecdc_tm', 'europarl_bilingual', 'event2Mind', 'evidence_infer_treatment', 'exams', 'factckbr', 'fake_news_english', 'fake_news_filipino', 'farsi_news', 'fashion_mnist', 'fever', 'few_rel', 'financial_phrasebank', 'finer', 'flores', 'flue', 'fquad', 'freebase_qa', 'gap', 'gem', 'generated_reviews_enth', 'generics_kb', 'german_legal_entity_recognition', 'germaner', 'germeval_14', 'giga_fren', 'gigaword', 'glucose', 'glue', 'gnad10', 'go_emotions', 'gooaq', 'google_wellformed_query', 'grail_qa', 'great_code', 'guardian_authorship', 'gutenberg_time', 'hans', 'hansards', 'hard', 'harem', 'has_part', 'hate_offensive', 'hate_speech18', 'hate_speech_filipino', 'hate_speech_offensive', 'hate_speech_pl', 'hate_speech_portuguese', 'hatexplain', 'hausa_voa_ner', 'hausa_voa_topics', 'hda_nli_hindi', 'head_qa', 'health_fact', 'hebrew_projectbenyehuda', 'hebrew_sentiment', 'hebrew_this_world', 'hellaswag', 'hind_encorp', 'hindi_discourse', 'hippocorpus', 'hkcancor', 'hope_edi', 'hotpot_qa', 'hover', 'hrenwac_para', 'hrwac', 'humicroedit', 'hybrid_qa', 'hyperpartisan_news_detection', 'iapp_wiki_qa_squad', 'id_clickbait', 'id_liputan6', 'id_nergrit_corpus', 'id_newspapers_2018', 'id_panl_bppt', 'id_puisi', 'igbo_english_machine_translation', 'igbo_monolingual', 'igbo_ner', 'ilist', 'imdb', 'imdb_urdu_reviews', 'imppres', 'indic_glue', 'indonlu', 'inquisitive_qg', 'interpress_news_category_tr', 'interpress_news_category_tr_lite', 'irc_disentangle', 'isixhosa_ner_corpus', 'isizulu_ner_corpus', 'iwslt2017', 'jeopardy', 'jfleg', 'jigsaw_toxicity_pred', 'jnlpba', 'journalists_questions', 'kannada_news', 'kd_conv', 'kde4', 'kelm', 'kilt_tasks', 'kilt_wikipedia', 'kinnews_kirnews', 'kor_3i4k', 'kor_hate', 'kor_ner', 'kor_nli', 'kor_nlu', 'kor_qpair', 'kor_sae', 'kor_sarcasm', 'labr', 'lama', 'lambada', 'large_spanish_corpus', 'laroseda', 'lc_quad', 'lener_br', 'liar', 'librispeech_asr', 'librispeech_lm', 'limit', 'lince', 'linnaeus', 'liveqa', 'lj_speech', 'lm1b', 'lst20', 'm_lama', 'mac_morpho', 'makhzan', 'math_dataset', 'math_qa', 'matinf', 'mc_taco', 'md_gender_bias', 'mdd', 'med_hop', 'medal', 'medical_dialog', 'medical_questions_pairs', 'menyo20k_mt', 'meta_woz', 'metooma', 'metrec', 'miam', 'mkb', 'mkqa', 'mlqa', 'mlsum', 'mnist', 'mocha', 'moroco', 'movie_rationales', 'mrqa', 'ms_marco', 'ms_terms', 'msr_genomics_kbcomp', 'msr_sqa', 'msr_text_compression', 'msr_zhen_translation_parity', 'msra_ner', 'mt_eng_vietnamese', 'muchocine', 'multi_booked', 'multi_news', 'multi_nli', 'multi_nli_mismatch', 'multi_para_crawl', 'multi_re_qa', 'multi_woz_v22', 'multi_x_science_sum', 'mutual_friends', 'mwsc', 'myanmar_news', 'narrativeqa', 'narrativeqa_manual', 'natural_questions', 'ncbi_disease', 'nchlt', 'ncslgr', 'nell', 'neural_code_search', 'news_commentary', 'newsgroup', 'newsph', 'newsph_nli', 'newspop', 'newsqa', 'newsroom', 'nkjp-ner', 'nli_tr', 'nlu_evaluation_data', 'norec', 'norne', 'norwegian_ner', 'nq_open', 'nsmc', 'numer_sense', 'numeric_fused_head', 'oclar', 'offcombr', 'offenseval2020_tr', 'offenseval_dravidian', 'ofis_publik', 'ohsumed', 'ollie', 'omp', 'onestop_english', 'open_subtitles', 'openbookqa', 'openslr', 'openwebtext', 'opinosis', 'opus100', 'opus_books', 'opus_dgt', 'opus_dogc', 'opus_elhuyar', 'opus_euconst', 'opus_finlex', 'opus_fiskmo', 'opus_gnome', 'opus_infopankki', 'opus_memat', 'opus_montenegrinsubs', 'opus_openoffice', 'opus_paracrawl', 'opus_rf', 'opus_tedtalks', 'opus_ubuntu', 'opus_wikipedia', 'opus_xhosanavy', 'orange_sum', 'oscar', 'para_crawl', 'para_pat', 'parsinlu_reading_comprehension', 'paws', 'paws-x', 'pec', 'peer_read', 'peoples_daily_ner', 'per_sent', 'persian_ner', 'pg19', 'php', 'piaf', 'pib', 'piqa', 'pn_summary', 'poem_sentiment', 'polemo2', 'poleval2019_cyberbullying', 'poleval2019_mt', 'polsum', 'polyglot_ner', 'prachathai67k', 'pragmeval', 'proto_qa', 'psc', 'ptb_text_only', 'pubmed', 'pubmed_qa', 'py_ast', 'qa4mre', 'qa_srl', 'qa_zre', 'qangaroo', 'qanta', 'qasc', 'qed', 'qed_amara', 'quac', 'quail', 'quarel', 'quartz', 'quora', 'quoref', 'race', 're_dial', 'reasoning_bg', 'recipe_nlg', 'reclor', 'reddit', 'reddit_tifu', 'refresd', 'reuters21578', 'ro_sent', 'ro_sts', 'ro_sts_parallel', 'roman_urdu', 'ronec', 'ropes', 'rotten_tomatoes', 's2orc', 'samsum', 'sanskrit_classic', 'saudinewsnet', 'scan', 'scb_mt_enth_2020', 'schema_guided_dstc8', 'scicite', 'scielo', 'scientific_papers', 'scifact', 'sciq', 'scitail', 'scitldr', 'search_qa', 'selqa', 'sem_eval_2010_task_8', 'sem_eval_2014_task_1', 'sem_eval_2020_task_11', 'sent_comp', 'senti_lex', 'senti_ws', 'sentiment140', 'sepedi_ner', 'sesotho_ner_corpus', 'setimes', 'setswana_ner_corpus', 'sharc', 'sharc_modified', 'sick', 'silicone', 'simple_questions_v2', 'siswati_ner_corpus', 'smartdata', 'sms_spam', 'snips_built_in_intents', 'snli', 'snow_simplified_japanese_corpus', 'so_stacksample', 'social_bias_frames', 'social_i_qa', 'sofc_materials_articles', 'sogou_news', 'spanish_billion_words', 'spc', 'species_800', 'spider', 'squad', 'squad_adversarial', 'squad_es', 'squad_it', 'squad_kor_v1', 'squad_kor_v2', 'squad_v1_pt', 'squad_v2', 'squadshifts', 'srwac', 'sst', 'stereoset', 'stsb_mt_sv', 'stsb_multi_mt', 'style_change_detection', 'subjqa', 'super_glue', 'swag', 'swahili', 'swahili_news', 'swda', 'swedish_ner_corpus', 'swedish_reviews', 'tab_fact', 'tamilmixsentiment', 'tanzil', 'tapaco', 'tashkeela', 'taskmaster1', 'taskmaster2', 'taskmaster3', 'tatoeba', 'ted_hrlr', 'ted_iwlst2013', 'ted_multi', 'ted_talks_iwslt', 'telugu_books', 'telugu_news', 'tep_en_fa_para', 'thai_toxicity_tweet', 'thainer', 'thaiqa_squad', 'thaisum', 'tilde_model', 'times_of_india_news_headlines', 'timit_asr', 'tiny_shakespeare', 'tlc', 'tmu_gfm_dataset', 'totto', 'trec', 'trivia_qa', 'tsac', 'ttc4900', 'tunizi', 'tuple_ie', 'turk', 'turkish_movie_sentiment', 'turkish_ner', 'turkish_product_reviews', 'turkish_shrinked_ner', 'turku_ner_corpus', 'tweet_eval', 'tweet_qa', 'tweets_ar_en_parallel', 'tweets_hate_speech_detection', 'twi_text_c3', 'twi_wordsim353', 'tydiqa', 'ubuntu_dialogs_corpus', 'udhr', 'um005', 'un_ga', 'un_multi', 'un_pc', 'universal_dependencies', 'universal_morphologies', 'urdu_fake_news', 'urdu_sentiment_corpus', 'web_nlg', 'web_of_science', 'web_questions', 'weibo_ner', 'wi_locness', 'wiki40b', 'wiki_asp', 'wiki_atomic_edits', 'wiki_auto', 'wiki_bio', 'wiki_dpr', 'wiki_hop', 'wiki_lingua', 'wiki_movies', 'wiki_qa', 'wiki_qa_ar', 'wiki_snippets', 'wiki_source', 'wiki_split', 'wiki_summary', 'wikiann', 'wikicorpus', 'wikihow', 'wikipedia', 'wikisql', 'wikitext', 'wikitext_tl39', 'wili_2018', 'wino_bias', 'winograd_wsc', 'winogrande', 'wiqa', 'wisesight1000', 'wisesight_sentiment', 'wmt14', 'wmt15', 'wmt16', 'wmt17', 'wmt18', 'wmt19', 'wmt20_mlqe_task1', 'wmt20_mlqe_task2', 'wmt20_mlqe_task3', 'wmt_t2t', 'wnut_17', 'wongnai_reviews', 'woz_dialogue', 'wrbsc', 'x_stance', 'xcopa', 'xed_en_fi', 'xglue', 'xnli', 'xor_tydi_qa', 'xquad', 'xquad_r', 'xsum', 'xsum_factuality', 'xtreme', 'yahoo_answers_qa', 'yahoo_answers_topics', 'yelp_polarity', 'yelp_review_full', 'yoruba_bbc_topics', 'yoruba_gv_ner', 'yoruba_text_c3', 'yoruba_wordsim353', 'youtube_caption_corrections', 'zest', 'AConsApart/anime_subtitles_DialoGPT', 'Adnan/Urdu_News_Headlines', 'Annielytics/DoctorsNotes', 'Avishekavi/Avi', 'Binbin/my_dataset', 'EMBO/biolang', 'EMBO/sd-nlp', 'Eymen3455/xsum_tr', 'FRTNX/cosuju', 'Firoj/CrisisBench', 'Fraser/mnist-text-default', 'Fraser/mnist-text-no-spaces', 'Fraser/mnist-text-small', 'Fraser/news-category-dataset', 'Fraser/python-lines', 'Fraser/short-jokes', 'Halilyesilceng/autonlp-data-nameEntityRecognition', 'Harveenchadha/Gujarati-Monolingual-Data', 'Jean-Baptiste/wikiner_fr', 'KidDiabeetus/Unhinged', 'MarianaSahagun/test', 'Melinoe/TheLabTexts', 'NTUYG/RAGTest', 'NbAiLab/norec_agg', 'NbAiLab/norne', 'NbAiLab/norwegian_parliament', 'Ofrit/tmp', 'QA/abk-eng', 'SajjadAyoubi/persian_qa', 'TRoboto/masc', 'Terry0107/RiSAWOZ', 'TimTreasure4/Test', 'Trainmaster9977/957', 'Trainmaster9977/zbakuman', 'Tyler/wikimatrix_collapsed', 'Valahaar/wsdmt', 'Vishva/UniFAQ_DataSET', 'Wikidepia/IndoParaCrawl', 'Wikidepia/IndoSQuAD', 'XiangXiang/clt', 'Yves/fhnw_swiss_parliament', 'abhishek/autonlp-data-imdb_eval', 'abwicke/C-B-R', 'abwicke/koplo', 'adamlin/re_dial', 'ajmbell/test-dataset', 'alireza655/alireza655', 'allenai/c4', 'ancs21/viwiki-18042021', 'anukaver/EstQA', 'aschvin/fhnw_test', 'ashish-shrivastava/dont-know-dataset', 'astarostap/antisemitic-tweets', 'astarostap/antisemitic_tweets', 'athivvat/thai-rap-lyrics', 'ausgequetschtem/jtrddfhfgh', 'bavard/personachat_truecased', 'bemanningssitua/dplremjfjfj', 'caca/zscczs', 'canwenxu/dogwhistle', 'ccccccc/hdjw_94ejrjr', 'cdminix/mgb1', 'cemigo/taylor_vs_shakes', 'cemigo/test-data', 'clarin-pl/cst-wikinews', 'clarin-pl/nkjp-pos', 'clarin-pl/polemo2-official', 'classla/copa_hr', 'classla/hr500k', 'classla/reldi_hr', 'classla/reldi_sr', 'classla/setimes_sr', 'cnrcastroli/aaaa', 'congpt/dstc23_asr', 'dasago78/dasago78dataset', 'david-wb/zeshel', 'dfgvhxfgv/fghghj', 'dispenst/jhghdghfd', 'dispix/test-dataset', 'dynabench/dynasent', 'dynabench/qa', 'eason929/test', 'edfews/szdfcszdf', 'edsas/fgrdtgrdtdr', 'edsas/grttyi', 'ervis/aaa', 'ervis/qqq', 'formermagic/github_python_1m', 'formu/CVT', 'fulai/DuReader', 'fuliucansheng/data_for_test', 'german-nlp-group/german_common_crawl', 'godzillavskongonlinetv/ergfdg', 'godzillavskongonlinetv/godzillavskongfullmovie', 'gustavecortal/fr_covid_news', 'hartzeer/kdfjdshfje', 'hfface/poopi', 'iamshsdf/sssssssssss', 'jaimin/wav2vec2-large-xlsr-gujarati-demo', 'jdepoix/junit_test_completion', 'jimregan/clarinpl_sejmsenat', 'jimregan/clarinpl_studio', 'joelito/ler', 'joelito/sem_eval_2010_task_8', 'julien-c/dummy-dataset-from-colab', 'k-halid/ar', 'katoensp/VR-OP', 'lavis-nlp/german_legal_sentences', 'lhoestq/custom_squad', 'lhoestq/squad', 'lhoestq/test', 'lhoestq/wikipedia_bn', 'lkiouiou/o9ui7877687', 'lohanna/testedjkcxkf', 'lucien/sciencemission', 'lucien/voacantonesed', 'lucien/wsaderfffjjjhhh', 'lucio/common_voice_eval', 'majod/CleanNaturalQuestionsDataset', 'makanan/umich', 'medzaf/test', 'metalearning/kaggale-nlp-tutorial', 'mmm-da/rutracker_anime_torrent_titles', 'mohsenfayyaz/toxicity-classification-datasets', 'mulcyber/europarl-mono', 'mustafa12/db_ee', 'mustafa12/edaaaas', 'mustafa12/thors', 'nucklehead/ht-voice-dataset', 'oelkrise/CRT', 'osanseviero/llama_test', 'parivartanayurveda/Malesexproblemsayurvedictreatment', 'patrickvonplaten/librispeech_asr_dummy', 'patrickvonplaten/scientific_papers_dummy', 'pdesoyres/test', 'peixian/equity_evaluation_corpus', 'peixian/rtGender', 'persiannlp/parsinlu_entailment', 'persiannlp/parsinlu_query_paraphrasing', 'persiannlp/parsinlu_reading_comprehension', 'persiannlp/parsinlu_sentiment', 'persiannlp/parsinlu_translation_en_fa', 'persiannlp/parsinlu_translation_fa_en', 'piEsposito/br-quad-2.0', 'piEsposito/br_quad_20', 'piEsposito/squad_20_ptbr', 'princeton-nlp/datasets-for-simcse', 'projectaligned/reddit_writingprompts_full', 'rony/soccer-dialogues', 'roskoN/dstc8-reddit-corpus', 'salesken/Paraphrase_category_detection', 'scrapfast/news_french_2020', 'sdfufygvjh/fgghuviugviu', 'seamew/Weibo', 'seamew/amazon_reviews_zh', 'seamew/weibo_avg', 'sharejing/BiPaR', 'sileod/metaeval', 'sismetanin/rureviews', 'smallv0221/my-test', 'somaimanguyat/movie21', 'somaimanguyat/xiomay', 'spacemanidol/msmarco_passage_ranking', 'ssasaa/gghghgh', 'sshleifer/pseudo_bart_xsum', 'stas/wmt14-en-de-pre-processed', 'stas/wmt16-en-ro-pre-processed', 'stiel/skjdhjkasdhasjkd', 'susumu2357/squad_v2_sv', 'tals/test', 'tommy19970714/common_voice', 'tommy19970714/jsut_asr', 'tommy19970714/jsut_asr_hiragana', 'tommy19970714/jsut_asr_hiragana_small', 'tommy19970714/laborotvspeech', 'toriving/agnews', 'toriving/atis-intent', 'toriving/cola', 'toriving/cr', 'toriving/dbpedia-content', 'toriving/dbpedia', 'toriving/imdb', 'toriving/mpqa', 'toriving/mr', 'toriving/rotten-tomato', 'toriving/snips-intent', 'toriving/sst2', 'toriving/sst5', 'toriving/subj', 'toriving/trec6', 'toriving/yahoo-answers', 'toriving/yelp', 'turingbench/TuringBench', 'uasoyasser/rgfes', 'vasudevgupta/bigbird-tokenized-natural-questions', 'vasudevgupta/data', 'vasudevgupta/natural-questions-validation', 'vasudevgupta/temperature-distribution-2d-plate', 'vasudevgupta/temperature-distribution-3d-cylinder', 'versae/adobo', 'vershasaxena91/datasets', 'vershasaxena91/squad_multitask', 'w11wo/imdb-javanese', 'webek18735/ddvoacantonesed', 'webek18735/dhikhscook', 'wmt/europarl', 'wmt/news-commentary', 'wmt/uncorpus', 'wmt/wikititles', 'wmt/wmt10', 'wmt/wmt13', 'wmt/wmt14', 'wmt/wmt15', 'wmt/wmt16', 'wmt/wmt17', 'wmt/wmt18', 'wmt/wmt19', 'yluisfern/PBU']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Reusing dataset squad_kor_v2 (/content/drive/MyDrive/korQuAD2.1/dataset/squad_kor_v2/squad_kor_v2/2.1.0/8e4ee4e5757761cf13f00b2d4e4cef2e842c0ea3c57050fec9fafc8fec60e128)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-f6c12c9ec750>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Load a dataset and print the first example in the training set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0msquad_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'squad_kor_v2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcache_dir\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'/content/drive/MyDrive/korQuAD2.1/dataset'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m#squad_dataset = load_dataset('squad_kor_v2',split='train',cache_dir ='/content/drive/MyDrive/korQuAD2.1/dataset')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#squad_dataset = load_from_disk(dataset_path = '/content/drive/MyDrive/korQuAD2.1/dataset')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, script_version, use_auth_token, **config_kwargs)\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0mkeep_in_memory\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mkeep_in_memory\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mis_small_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuilder_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m     )\n\u001b[0;32m--> 758\u001b[0;31m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuilder_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_verifications\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_verifications\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeep_in_memory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msave_infos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0mbuilder_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_infos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/datasets/builder.py\u001b[0m in \u001b[0;36mas_dataset\u001b[0;34m(self, split, run_post_process, ignore_verifications, in_memory)\u001b[0m\n\u001b[1;32m    743\u001b[0m             ),\n\u001b[1;32m    744\u001b[0m             \u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 745\u001b[0;31m             \u001b[0mmap_tuple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    746\u001b[0m         )\n\u001b[1;32m    747\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/datasets/utils/py_utils.py\u001b[0m in \u001b[0;36mmap_nested\u001b[0;34m(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, types)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnum_proc\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mnum_proc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         mapped = [\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0m_single_map_nested\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisable_tqdm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         ]\n\u001b[1;32m    206\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/datasets/utils/py_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnum_proc\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mnum_proc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         mapped = [\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0m_single_map_nested\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisable_tqdm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         ]\n\u001b[1;32m    206\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/datasets/utils/py_utils.py\u001b[0m in \u001b[0;36m_single_map_nested\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;31m# Singleton first to spare some computation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_struct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_struct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_struct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;31m# Reduce logging to keep things readable in multiprocessing with tqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/datasets/builder.py\u001b[0m in \u001b[0;36m_build_single_dataset\u001b[0;34m(self, split, run_post_process, ignore_verifications, in_memory)\u001b[0m\n\u001b[1;32m    764\u001b[0m         ds = self._as_dataset(\n\u001b[1;32m    765\u001b[0m             \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 766\u001b[0;31m             \u001b[0min_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0min_memory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    767\u001b[0m         )\n\u001b[1;32m    768\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_post_process\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/datasets/builder.py\u001b[0m in \u001b[0;36m_as_dataset\u001b[0;34m(self, split, in_memory)\u001b[0m\n\u001b[1;32m    838\u001b[0m             \u001b[0min_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0min_memory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m         )\n\u001b[0;32m--> 840\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdataset_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_post_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresources_paths\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, arrow_table, info, split, indices_table, fingerprint)\u001b[0m\n\u001b[1;32m    274\u001b[0m             raise ValueError(\n\u001b[1;32m    275\u001b[0m                 \"External features info don't match the dataset:\\nGot\\n{}\\nwith type\\n{}\\n\\nbut expected something like\\n{}\\nwith type\\n{}\".format(\n\u001b[0;32m--> 276\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferred_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferred_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m                 )\n\u001b[1;32m    278\u001b[0m             )\n",
            "\u001b[0;31mValueError\u001b[0m: External features info don't match the dataset:\nGot\n{'id': Value(dtype='string', id=None), 'title': Value(dtype='string', id=None), 'context': Value(dtype='string', id=None), 'question': Value(dtype='string', id=None), 'answer': {'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None), 'html_answer_start': Value(dtype='int32', id=None)}, 'url': Value(dtype='string', id=None), 'raw_html': Value(dtype='string', id=None)}\nwith type\nstruct<answer: struct<text: string, answer_start: int32, html_answer_start: int32>, context: string, id: string, question: string, raw_html: string, title: string, url: string>\n\nbut expected something like\n{'answer': {'answer_start': Value(dtype='int32', id=None), 'html_answer_start': Value(dtype='int32', id=None), 'text': Value(dtype='string', id=None)}, 'context': Value(dtype='string', id=None), 'id': Value(dtype='string', id=None), 'question': Value(dtype='string', id=None), 'raw_html': Value(dtype='string', id=None), 'title': Value(dtype='string', id=None), 'url': Value(dtype='string', id=None)}\nwith type\nstruct<answer: struct<answer_start: int32, html_answer_start: int32, text: string>, context: string, id: string, question: string, raw_html: string, title: string, url: string>"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVv5er7QHKCz"
      },
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "def read_squad(path):\n",
        "    path = Path(path)\n",
        "    with open(path, 'rb') as f:\n",
        "        squad_dict = json.load(f)\n",
        "\n",
        "    contexts = []\n",
        "    questions = []\n",
        "    answers = []\n",
        "    for group in squad_dict['data']:\n",
        "        for passage in group['paragraphs']:\n",
        "            context = passage['context']\n",
        "            for qa in passage['qas']:\n",
        "                question = qa['question']\n",
        "                for answer in qa['answers']:\n",
        "                    contexts.append(context)\n",
        "                    questions.append(question)\n",
        "                    answers.append(answer)\n",
        "\n",
        "    return contexts, questions, answers\n",
        "\n",
        "train_contexts, train_questions, train_answers = read_squad('/content/drive/MyDrive/korQuAD2.1/dataset/squad_kor_v2/squad_kor_v2/2.1.0/8e4ee4e5757761cf13f00b2d4e4cef2e842c0ea3c57050fec9fafc8fec60e128/squad_kor_v2-train.arrow')\n",
        "val_contexts, val_questions, val_answers = read_squad('/content/drive/MyDrive/korQuAD2.1/dataset/squad_kor_v2/squad_kor_v2/2.1.0/8e4ee4e5757761cf13f00b2d4e4cef2e842c0ea3c57050fec9fafc8fec60e128/squad_kor_v2-validation.arrow')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KE7QZvOgWCVb",
        "outputId": "49496440-b1d6-48a4-a005-b6da433478f8"
      },
      "source": [
        "squad_dataset['train']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['id', 'title', 'context', 'question', 'answer', 'url', 'raw_html'],\n",
              "    num_rows: 83486\n",
              "})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBzGOlw3knqJ",
        "outputId": "bf298cff-b3e3-4db6-9849-1bc4a2948198"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 4.2MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 19.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 22.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alvcFdcakx_S"
      },
      "source": [
        "train_contexts = squad_dataset['train']['context']\n",
        "train_questions = squad_dataset['train']['question']\n",
        "train_answers = squad_dataset['train']['answer']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Co3vmVvHlLNL"
      },
      "source": [
        "val_contexts = squad_dataset['validation']['context']\n",
        "val_questions = squad_dataset['validation']['question']\n",
        "val_answers = squad_dataset['validation']['answer']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMU9is1hmvgy"
      },
      "source": [
        "def add_end_idx(answers, contexts):\n",
        "    for answer, context in zip(answers, contexts):\n",
        "        gold_text = answer['text']\n",
        "        start_idx = answer['answer_start']\n",
        "        end_idx = start_idx + len(gold_text)\n",
        "\n",
        "        # sometimes squad answers are off by a character or two – fix this\n",
        "        if context[start_idx:end_idx] == gold_text:\n",
        "            answer['answer_end'] = end_idx\n",
        "        elif context[start_idx-1:end_idx-1] == gold_text:\n",
        "            answer['answer_start'] = start_idx - 1\n",
        "            answer['answer_end'] = end_idx - 1     # When the gold label is off by one character\n",
        "        elif context[start_idx-2:end_idx-2] == gold_text:\n",
        "            answer['answer_start'] = start_idx - 2\n",
        "            answer['answer_end'] = end_idx - 2     # When the gold label is off by two characters\n",
        "\n",
        "add_end_idx(train_answers, train_contexts)\n",
        "add_end_idx(val_answers, val_contexts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_lGxhDRm15W"
      },
      "source": [
        "from transformers import DistilBertTokenizerFast\n",
        "\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('monologg/kobert')\n",
        "\n",
        "train_encodings = tokenizer(train_contexts, train_questions, truncation=True, padding=True)\n",
        "val_encodings = tokenizer(val_contexts, val_questions, truncation=True, padding=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1fyvntqnIeG"
      },
      "source": [
        "def add_token_positions(encodings, answers):\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "    for i in range(len(answers)):\n",
        "        start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\n",
        "        end_positions.append(encodings.char_to_token(i, answers[i]['answer_end'] - 1))\n",
        "\n",
        "        # if start position is None, the answer passage has been truncated\n",
        "        if start_positions[-1] is None:\n",
        "            start_positions[-1] = tokenizer.model_max_length\n",
        "        if end_positions[-1] is None:\n",
        "            end_positions[-1] = tokenizer.model_max_length\n",
        "\n",
        "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
        "\n",
        "add_token_positions(train_encodings, train_answers)\n",
        "add_token_positions(val_encodings, val_answers)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}